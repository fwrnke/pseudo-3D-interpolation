{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pseudo-3D cube generation from densely spaced subbottom profiles via POCS interpolation","text":"<p>This repository contains the source code for the generation of pseudo-3D cubes from densely spaced 2D subbottom profiles by utilizing the Projection Onto Convex Sets (POCS) method.</p> <p>The source code accompanies a journal article published in GEOPHYSICS:</p> <pre><code>Warnke et al. (2023) Pseudo-3D cube generation from densely spaced subbottom profiles via POCS interpolation\n</code></pre> <p>When using this workflow please refer to the citation information and, in case of feedback or questions, the contact information.</p>"},{"location":"#workflow-structure","title":"Workflow structure","text":"<p>This workflow was created to enable the interpolation of high-resolution subbottom profiler data (2-6 kHz) into pseudo-3D cubes. Potential applications include:</p> <ul> <li> <p>Pseudo-3D interpolation of:</p> <ul> <li>subbottom profiler data (SBP)</li> <li>single channel reflection seismic (SCS)</li> <li>multichannel reflection seismic (MCS)</li> </ul> </li> <li> <p>Upsampling of existing post-stack 3D seismic volumes (e.g., in crossline direction)</p> </li> </ul> <p>The workflow can be subdivided into two separate stages (Figure 1):</p> <ol> <li>2D (pre-)processing of individual profiles</li> <li>3D interpolation of binned, sparse cube</li> </ol> <p>The most essential pre-processing steps for a subsequent 3D interpolation aim to correct the vertical offset between the input profiles. Without a sufficient correction, such offsets, especially at profile intersections, result in low quality interpolation outputs. This issue is addressed by three processing steps in the fist stage (Figure 1, upper panel):</p> <ul> <li>Static correction (per profile)</li> <li>Tidal heave compensation (per profile)</li> <li>Mistie correction (using all profiles) </li> </ul> <p>2D stage: Input data types</p> <p>For most of the scripts of the 2D processing stage, three different input types are allowed:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>The <code>datalist</code> is a plain text file that contains the path and name of files to process relativ to the <code>datalist.txt</code> file:</p> <pre><code>./folder/file1.sgy\n./folder/file2.sgy\n</code></pre> <p>for an examplary folder structure like this:</p> <pre><code>|-- folder/\n|    |-- file1.sgy\n|    |-- file2.sgy\n|-- datalist.txt\n</code></pre> <p>In the second stage, the following processing steps are conducted (Figure 1, lower panel):</p> <ul> <li>Binning and geometry setup</li> <li>Preprocessing of sparse 3D cube</li> <li>Gain application</li> <li>Amplitude balancing</li> <li>Frequency filter (lowpass, highpass, bandpass)</li> <li>Trace resampling</li> <li>Trace envelope</li> <li>Forward FFT (time to frequency domain)</li> <li>Interpolation via Projection Onto Convex Sets (POCS) algorithm</li> <li>Inverse FFT (frequency to time domain)</li> </ul> <p>3D stage: Input data</p> <p>Most scripts of the second, 3D processing and interpolation stage require a single <code>netCDF</code> file and, most often, one or more <code>YAML</code> configuration files. An exception is the binning script, which needs multiple 2D netCDF files.</p> <p> Figure 1: Schematic diagram of pseudo-3D interpolation workflow showing two stages: 2D processing of each profile (upper panel) and 3D interpolation of sparse 3D cube (lower panel).</p> <p>Workflow applications</p> <p>The workflow has only been tested on subbottom profiler data. However, additional proof-of-concept tests will be conducted in the first half of 2023.</p>"},{"location":"#result-examples","title":"Result examples","text":""},{"location":"#vertical-offset-corrections","title":"Vertical offset corrections","text":"<p>Figure 2 shows the effect of the different vertical offset correction methods applied to an example dataset of &gt;200 profiles. The static correction mainly improves the reflection coherence of individual lines (Figure 3) with several acquisition artifacts remaining in the data (black arrows, Figure 2a, b). As the survey was conducted over multiple days, a tide compensation basically detrends the individual profiles and improves the data quality (Figure 2c, d). Remaining misties are successfully corrected using a least-squares minimization approach (Figure 2e, f).</p> <p> Figure 2: Vertical offsets (upper row) and effect on detected seafloor reflection (lower row) after static correction (a, b), tide compensation (c, d), and mistie correction (e, f), respectively.</p>"},{"location":"#2d-topas-processing","title":"2D TOPAS processing","text":"<p>The following figure illustrate the capabilities of the first workflow stage before (Figure 3, upper row) and after applying all processing steps (Figure 3, lower row). Both the full waveform and envelope of the examplary profile section are displayed for comparison. Besides compensating the vertical offsets (larger inset figures), a despiking algorithm was applied to remove abundant and random noise burst of 20 ms length (smaller inset figures).</p> <p> Figure 3: Examplary TOPAS profile displayed as unprocessed full-waveform (a) and envelope (b) as well as processed sections (c) and (d), respectively.</p>"},{"location":"#3d-interpolation-results","title":"3D interpolation results","text":"<p>The following figure illustrates (a) the sparse cube before and (b) the full pseudo-3D cube after interpolation via POCS algorithm.</p> <p> Figure 4: Sparse (a) and interpolated (b) pseudo-3D TOPAS cube overlain by multibeam bathymetry data.</p>"},{"location":"#funding","title":"Funding","text":"<p>This workflow was developed as part of my PhD research at the School of Environment, University of Auckland, New Zealand.</p> <p>The funding was provided by the Royal Society of New Zealand Marsden Fund grant Geologic champagne: What controls sudden release of CO2 at glacial terminations on the Chatham Rise? (19-UOA-339) with additional support by the University of Auckland Doctoral Scholarship. Ship time was supported by the Ministry of Business, Innovation and Employment (MBIE) and additional funding from GNS Science.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under <code>GNU GPLv3</code>. Please refer to the project license when considering using this workflow for your own research or project.</p>"},{"location":"citation/","title":"How to cite?","text":"<p>When using this workflow in your research, please cite the following journal article and, ideally, the workflow.</p> <p>Journal article:</p> <pre><code>Warnke, F. et al. (2023) _Title_. GEOPHYSICS\n</code></pre> <p>Workflow:</p> <pre><code>DOI: xxx.xx/xxx/xxxxxxxx\n</code></pre>"},{"location":"contact/","title":"Contact information","text":"<p>For questions or feedback about the Python implemention please create an issue on GitHub. You can also contact me directly for any enquiries using this workflow for your own data:</p> <p> Fynn Warnke  School of Environment | Faculty of Science | The University of Auckland fwar378@aucklanduni.ac.nz todo </p>"},{"location":"license/","title":"License information","text":"<p>The source code for the presented workflow is licensed by <code>GNU GPLv3</code> (see <code>LICENSE</code> file or text below):</p> <pre><code>                    GNU GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nthe GNU General Public License is intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.  We, the Free Software Foundation, use the\nGNU General Public License for most of our software; it applies also to\nany other work released this way by its authors.  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  To protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights.  Therefore, you have\ncertain responsibilities if you distribute copies of the software, or if\nyou modify it: responsibilities to respect the freedom of others.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received.  You must make sure that they, too, receive\nor can get the source code.  And you must show them these terms so they\nknow their rights.\n\n  Developers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\n  For the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software.  For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\n  Some devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the manufacturer\ncan do so.  This is fundamentally incompatible with the aim of\nprotecting users' freedom to change the software.  The systematic\npattern of such abuse occurs in the area of products for individuals to\nuse, which is precisely where it is most unacceptable.  Therefore, we\nhave designed this version of the GPL to prohibit the practice for those\nproducts.  If such problems arise substantially in other domains, we\nstand ready to extend this provision to those domains in future versions\nof the GPL, as needed to protect the freedom of users.\n\n  Finally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish to\navoid the special danger that patents applied to a free program could\nmake it effectively proprietary.  To prevent this, the GPL assures that\npatents cannot be used to render the program non-free.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Use with the GNU Affero General Public License.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, your program's commands\nmight be different; for a GUI interface, you would use an \"about box\".\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU GPL, see\n&lt;https://www.gnu.org/licenses/&gt;.\n\n  The GNU General Public License does not permit incorporating your program\ninto proprietary programs.  If your program is a subroutine library, you\nmay consider it more useful to permit linking proprietary applications with\nthe library.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.  But first, please read\n&lt;https://www.gnu.org/licenses/why-not-lgpl.html&gt;.\n</code></pre>"},{"location":"license/#seismic-unix","title":"Seismic Unix","text":"<p>This repository includes a custom implementation of the Seismic Unix module <code>sugain</code>.</p> <pre><code>    This file is property of the Colorado School of Mines.\n\n    Copyright \u00a9 2008, Colorado School of Mines,\n    All rights reserved.\n\n    Redistribution and use in source and binary forms, with or \n    without modification, are permitted provided that the following \n    conditions are met:\n\n        *  Redistributions of source code must retain the above copyright \n           notice, this list of conditions and the following disclaimer.\n        *  Redistributions in binary form must reproduce the above \n           copyright notice, this list of conditions and the following \n           disclaimer in the documentation and/or other materials provided \n           with the distribution.\n        *  Neither the name of the Colorado School of Mines nor the names of\n           its contributors may be used to endorse or promote products \n           derived from this software without specific prior written permission.\n\n    Warranty Disclaimer:\n    THIS SOFTWARE IS PROVIDED BY THE COLORADO SCHOOL OF MINES AND CONTRIBUTORS \n    \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT \n    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n    FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n    COLORADO SCHOOL OF MINES OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n    INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, \n    BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n    LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \n    CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, \n    STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING \n    IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n    POSSIBILITY OF SUCH DAMAGE.\n\n    Export Restriction Disclaimer:\n    We believe that CWP/SU: Seismic Un*x is a low technology product that does\n    not appear on the Department of Commerce CCL list of restricted exports.\n    Accordingly, we believe that our product meets the qualifications of\n    an ECCN (export control classification number) of EAR99 and we believe\n    it fits the qualifications of NRR (no restrictions required), and\n    is thus not subject to export restrictions of any variety.\n\n    Approved Reference Format:\n    In publications, please refer to SU as per the following example:\n    Cohen, J. K. and Stockwell, Jr. J. W., (200_), CWP/SU: Seismic Un*x \n    Release No. __: an open source software  package for seismic \n    research and processing, \n    Center for Wave Phenomena, Colorado School of Mines.\n\n    Articles about SU in peer-reviewed journals:\n    Saeki, T., (1999), A guide to Seismic Un*x (SU)(2)---examples of data processing (part 1), data input and preparation of headers, Butsuri-Tansa (Geophysical Exploration), vol. 52, no. 5, 465-477.\n    Stockwell, Jr. J. W. (1999), The CWP/SU: Seismic Un*x Package, Computers and Geosciences, May 1999.\n    Stockwell, Jr. J. W. (1997), Free Software in Education: A case study of CWP/SU: Seismic Un*x, The Leading Edge, July 1997.\n    Templeton, M. E., Gough, C.A., (1998), Web Seismic Un*x: Making seismic reflection processing more accessible, Computers and Geosciences.\n\n    Acknowledgements:\n    SU stands for CWP/SU:Seismic Un*x, a processing line developed at Colorado \n    School of Mines, partially based on Stanford Exploration Project (SEP) \n    software.\n</code></pre>"},{"location":"2D/2D_despike/","title":"2D despiking","text":"<p>Despike SEG-Y file(s) using a 2D moving window function.</p>"},{"location":"2D/2D_despike/#description","title":"Description","text":"<p>Remove (single trace) noise bursts from seismic data using an user-defined moving 2D window (time x ntraces) approach: </p> <ol> <li>Define moving 2D window (time x ntraces) with <code>overlap</code> in time domain (in percent)</li> <li>Compute background amplitude per time sample over ntraces using user-specified function<ul> <li><code>mean</code>: average amplitude</li> <li><code>rms</code>: root mean squared amplitude</li> <li><code>median</code>: median amplitude</li> </ul> </li> <li>Compare reference trace amplitudes with computed background amplitudes using user-specified <code>threshold</code> and <ul> <li><code>amplitude &gt; threshold * background amplitude</code> \u2192 spike detection</li> <li><code>amplitude &lt;= threshold * background amplitude</code> \u2192 no spike</li> </ul> </li> <li>Replace detected spike samples using user-specified method:<ul> <li><code>scaled</code>: Scale signal down to background amplitude (based on mode). Tapering applied.</li> <li><code>mode</code>: Replace with background amplitude values</li> <li><code>threshold</code>: Replace with <code>threshold * background</code> amplitude values.</li> <li><code>zeros</code>: Replace with zero values.</li> <li><code>median</code>: Replace with median values (calculated from neighboring traces).</li> </ul> </li> </ol> <p></p> Figure 1: Schematics of 2D despike algorithm."},{"location":"2D/2D_despike/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/2D_despike/#command-line-interface","title":"Command line interface","text":"<p>The script can handle three different inputs:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like:</p> <pre><code>&gt;&gt;&gt; 08_despike {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> <p>Alternatively, the script can be executed using the (more verbose) command:</p> <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.despiking_2D_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--inplace</code>: Replace input data without creating copy (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--use_delay</code>: Use delay recording time to split input data before despiking (e.g. for TOPAS, Parasound).</li> <li><code>--byte_delay {109}</code>: Byte position of input delay times in SEG-Y file(s) (default: <code>109</code>).</li> <li><code>--mode {MODE}</code>: Mode used to compute background amplitude and detect spikes in data.</li> <li><code>mean</code></li> <li><code>rms</code></li> <li><code>median</code></li> <li><code>--window_time {TIME}</code>: Moving window length in time domain (TWT [ms]).</li> <li><code>--window_traces {TRACES}</code>: Moving window length in offset domain (traces [#]).</li> <li><code>--window_overlap {PERCENT}</code>: Time overlap of moving windows (%) (default: <code>10</code>).</li> <li><code>--threshold_factor {FACTOR}</code>: Used for spike detection (threshold_factor * background amplitude).</li> <li><code>--out_amplitude {METHOD}</code>: Replace spike amplitudes using selected method</li> <li><code>scaled</code>, <code>mode</code>, <code>threshold</code>, <code>zeros</code>, <code>median</code></li> <li><code>--verbose {LEVEL}</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/2D_despike/#python-script","title":"Python script","text":"<p>Import function</p> <pre><code>import despike_2D\n</code></pre> <p>Create dummy data</p> <pre><code>import numpy as np\ndt = 0.05  # ms\nlength = 250  # ms\nntraces = 100\nshape = (int(length / dt), ntraces)\n# create dummy data\ndata = np.ones(shape, dtype='float32')\n# insert noise burst\nnoise_time = slice(int(100/dt), int(120/dt))\nnoise_trace = 50\ndata[noise_time, noise_trace] = 5\n# add some noise\ndata += np.random.standard_normal(shape)\n</code></pre> <p>Despike dummy data</p> <pre><code>window = 30  # length (ms) of window, should be larger than noise (~20 ms)\ndata_despk = despike_2D(\ndata, window, dt, overlap=10, ntraces=5, mode='mean', threshold=2, out='scaled', verbosity=0\n)\n</code></pre> <p>Compare original vs despiked data</p> <pre><code>print('original data:', data[noise_time, noise_trace], sep='\\n')\nprint('despiked data:', data_despk[noise_time, noise_trace], sep='\\n')\n</code></pre>"},{"location":"2D/convert_segy2netcdf/","title":"Convert SEG-Y files to netCDF format","text":""},{"location":"2D/convert_segy2netcdf/#description","title":"Description","text":"<p>This utility script converts (multiple) SEG-Y file(s) to netCDF format using the Python package <code>segysak</code> in parallel.</p>"},{"location":"2D/convert_segy2netcdf/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/convert_segy2netcdf/#command-line-interface","title":"Command line interface","text":"<p>The script can handle two different inputs:</p> <ol> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 09_convert_segy2netcdf {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cnv_segy2netcdf {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory.</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--nprocesses</code>: Number of parallel conversions (should be \u2264 number of available CPUs).</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/coordinate_conversion/","title":"Coordinate conversion","text":"<p>Utility script for coordinate conversion (and export) from SEG-Y file(s).</p>"},{"location":"2D/coordinate_conversion/#description","title":"Description","text":"<p>This utility script reproject (i.e. transforms) coordinates read from SEG-Y header(s) to different coordinate reference systems (CRS). The coordinate transformation is conducted using the Python package <code>pyproj</code>, an interface to the well-kown and established PROJ library. Input and output CRS can be specified as either EPSG codes (e.g., <code>EPSG:4326</code>) or PROJ.4 strings:</p> <pre><code>+proj=longlat +datum=WGS84 +no_defs\n</code></pre> <p>Suitable coordinate reference system</p> <p>A projected CRS is required for most subsequent processing steps (e.g. UTM)!</p>"},{"location":"2D/coordinate_conversion/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/coordinate_conversion/#command-line-interface","title":"Command line interface","text":"<p>The script can handle three different inputs:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 02_reproject_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.reproject_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--crs_src</code>: Input coordinate reference system. Indicate using EPSG code or PROJ.4 string (e.g. \"epsg:4326\").</li> <li><code>--crs_dst</code>: Output coordinate reference system. Indicate using EPSG code or PROJ.4 string (e.g. \"epsg:32760\").</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--inplace</code>: Replace input data without creating copy (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--scaler</code>: Output coordinate scaler (following SEG-Y specification, default: <code>-100</code>).<ul> <li>negative: division by absolute value</li> <li>positive: multiplication by absolute value</li> </ul> </li> <li><code>--src_coords</code>: Byte position of input coordinates in SEG-Y file(s).<ul> <li><code>source</code>: 73, 77</li> <li><code>group</code>: 81, 85</li> <li><code>CDP</code>: 181, 185</li> </ul> </li> <li><code>--dst_coords</code>:  Byte position of output coordinates in SEG-Y file(s).</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/correct_delrt/","title":"Correct DelayRecordingTime (delrt)","text":"<p>Utility script to fix incorrect DelayRecordingTime (delrt) in SEG-Y file(s).</p>"},{"location":"2D/correct_delrt/#description","title":"Description","text":"<p>Utility script to fix incorrect DelayRecordingTimes (delrt) header value in SEG-Y file(s) by comparing maximum amplitudes of neighboring traces in user-defined moving windows (default: 120 samples x 5 traces).</p> <ol> <li>Extract (multiple) DelayRecordingTimes from SEG-Y trace header</li> <li>Find maximum amplitude of reference (center) trace and select data subset based on <code>win_ntraces</code> and <code>win_nsamples</code></li> <li>Create boolean mask of detected delrt values using difference amplitudes relative to reference amplitude<ul> <li>e.g. [625, 625, 625, 700, 700] (in ms) \u2192 [1, 1, 1, 0, 0]</li> </ul> </li> <li>Create boolean mask of delrt values from trace header<ul> <li>e.g. [625, 625, 700, 700, 700] (in ms) \u2192 [1, 1, 0, 0, 0]</li> </ul> </li> <li>Compare both masks and update incorrect trace headers<ul> <li>[625, 625, 700, 700, 700] (in ms) \u2192 [625, 625, 625, 700, 700] (in ms) </li> </ul> </li> </ol> <p></p> Figure: Incorrect assigned delrt (red trace) in trace header."},{"location":"2D/correct_delrt/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/correct_delrt/#command-line-interface","title":"Command line interface","text":"<p>The script can handle three different inputs:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 03_correct_delrt {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.delrt_correction_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--inplace</code>: Replace input data without creating copy (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--byte_delay {109}</code>: Byte position of input delay times in SEG-Y file(s) (default: <code>109</code>).</li> <li><code>--win_nsamples {SAMPLES}</code>: Moving window length in time domain (samples [#]).</li> <li><code>--win_ntraces {TRACES}</code>: Moving window length in offset domain (traces [#]).</li> <li><code>--verbose {LEVEL}</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/merge_segys/","title":"Merge short SEG-Y files","text":"<p>Utility script to merge short SEG-Y file(s) with longer ones.</p>"},{"location":"2D/merge_segys/#description","title":"Description","text":"<p>Merge short SEG-Y files (here: &lt; 2000 kB, ~50 traces) with an appropriate file recorded before or after. For this decision, the spatial distance between subsequent traces is used to determine the SEG-Y file to merge with.</p>"},{"location":"2D/merge_segys/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/merge_segys/#command-line-interface","title":"Command line interface","text":"<p>The script can handle two different inputs:</p> <ol> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 01_merge_segys {datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.merge_segys {datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. pad, static) to filter input files. Only used if directory is specified (default: <code>None</code>)</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>merge</code>).</li> <li><code>--filesize_kB</code>: Threshold filesize (in kilobyte) to determine which SEG-Y will be merged (default: <code>2000</code> kB).</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/mistie_correction/","title":"Mistie correction","text":"<p>Compensate mistie for SEG-Y file(s) via cross-correlation of nearest traces of intersecting lines.</p>"},{"location":"2D/mistie_correction/#description","title":"Description","text":"<p>This script corrects the vertical mistie at profile intersections by calculating an individual (i.e. global) bulk shift value for each input profile using least-squars minimization.  In case of several overlapping profiles with many intersections, this closed-loop configuration (Harper, 1991) can be represented as a set of over-determined equations and solved following the approach proposed by Bishop and Nunns (1994).</p> <p> Figure 1: Mistie (in time domain) for individual TOPAS profiles (left).</p> <p>The general workflow is summarized in the following steps:</p> <ol> <li>Calclulate profile intersections </li> <li>Extract nearest trace(s) at intersections based on euclidean distance (optionally in user-defined vertical window)</li> <li>Create reference trace from nearest traces (reduce effect of noisy traces)<ul> <li>calculating trace envelope for more performant cross-correlation</li> </ul> </li> <li>Compute cross-correlation of reference traces</li> <li>Remove low-quality cross-correlation using an user-defined quality threshold (Pearson's correlation coefficient in SciPy)</li> <li>Calculate single bulk shift value for each profile using least-squares minimization</li> <li>Apply bulk shift (for individual lines)</li> </ol> <p> </p> Figure 2: Conceptual mistie correction at intersection of two examplary profiles."},{"location":"2D/mistie_correction/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/mistie_correction/#command-line-interface","title":"Command line interface","text":"<p>The script can handle two different inputs:</p> <ol> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 07_correct_mistie {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.mistie_correction_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--inplace</code>: Replace input data without creating copy (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--coords_origin</code>: Origin of (shotpoint) coordinates (i.e. navigation).<ul> <li><code>header</code> (default): Extract coordinates from SEG-Y header(s).</li> <li><code>aux</code>: Read previously extracted coordinates from auxiliary file.</li> </ul> </li> <li><code>--coords_path</code>: Path to SEG-Y directory (coords_origin=<code>header</code>) or navigation file with coordinates (coords_origin=<code>aux</code>)</li> <li><code>--win_cc</code>: Upper/lower trace window limits used for cross-correlation (in ms), e.g., for using only seafloor reflection.</li> <li><code>--quality_threshold</code>: Cut-off threshold for cross-correlation [0-1] used to remove noisy/insufficient intersections.</li> <li><code>--write_aux</code>: Write mistie offsets to auxiliary file (<code>*.mst</code>).</li> <li><code>--write_QC</code>: Write line intersections and nearest traces to GeoPackage (<code>*.gpkg</code>).</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/mistie_correction/#references","title":"References","text":"<ul> <li>Harper, Michael D. 1991. \u201cSeismic Mis\u2010tie Resolution Technique.\u201d Geophysics 56 (11): 1825\u201330. https://doi.org/10.1190/1.1442995. </li> <li>Bishop, Thomas N., and Alan G. Nunns. 1994. \u201cCorrecting Amplitude, Time, and Phase Mis\u2010ties in Seismic Data.\u201d Geophysics 59 (6): 946\u201353. https://doi.org/10.1190/1.1443654.</li> </ul>"},{"location":"2D/pad_vertical_offsets/","title":"Trace padding","text":"<p>Pad traces in SEG-Y file(s) to correct DelayRecordingTime.</p>"},{"location":"2D/pad_vertical_offsets/#description","title":"Description","text":"<p>Check for vertical offsets in SEG-Y file(s) using trace header keyword DelayRecordingTime (delrt) and pad seismic trace with zeros to compensate variable recording starting times.</p> <ol> <li>Detect changing DelayRecordingTime based on trace header values</li> <li>Split input profile into multiple segments of different derlt</li> <li>Pad start (top) and end (bottom) of traces per segment to minimum and maximum delrt (using zeros)</li> <li>Write padded data to new SEG-Y file(s)</li> </ol> <p></p> Figure: Schemetic showing trace padding effect."},{"location":"2D/pad_vertical_offsets/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/pad_vertical_offsets/#command-line-interface","title":"Command line interface","text":"<p>The script can handle three different inputs:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 04_pad_delrt {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.delrt_padding_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--byte_delay {109}</code>: Byte position of input delay times in SEG-Y file(s) (default: <code>109</code>).</li> <li><code>--verbose {LEVEL}</code>: Level of output verbosity (default: <code>0</code>).</li> </ul> <p>NOTE: Since the total number of traces per file changes, the files cannot be updated inplace!</p>"},{"location":"2D/static_correction/","title":"Static correction","text":"<p>Compensate static effect of 2D profiles.</p>"},{"location":"2D/static_correction/#description","title":"Description","text":"<p>Compensate residual static (e.g. swell) on seismo-acoustic profile(s) using either SourceWaterDepth (mode: <code>swdep</code>) or first positive amplitude peak of seafloor reflection (mode: <code>amp</code>).</p> <ol> <li>Retrieve seafloor reflection TWT values from<ul> <li><code>swdep</code>: SourceWaterDepth stored in trace header (originating from simultaneously operating multibeam echosounder)</li> <li><code>amp</code>: detected seafloor amplitude</li> </ul> </li> <li>Filter outlier from seafloor TWT values using a rolling double medium absolute deviation (MAD) filter</li> </ol> <p></p> Figure: Examplary TOPAS profile subsection"},{"location":"2D/static_correction/#seafloor-reflection-detection-first-break-picking","title":"Seafloor reflection detection (first-break picking)","text":"<p>The basis for an sufficient static correction is a precise detection of the seafloor reflection amplitudes. Here, the STA/LTA ratio method commonly used in seismology is adapted. It uses the ratio of averages between a short-term  and a long-term window, which is denoted as STA/LTA (short-term average/long-term average). This method uses 1% and 5% of the total trace length for the determination of the short-term and long-term window length, respectively. Based on a first (rought) first break pick, a narrow time window (\\(\\pm\\) 50 samples around picked time) is defined and the shallowest (i.e. smallest two-way travel time) of the five largest amplitudes flagged as the seafloor amplitude. These picks are filtered and used to calculate the residual statics in subsequent steps.</p>"},{"location":"2D/static_correction/#median-absolute-deviation-mad","title":"Median Absolute Deviation (MAD)","text":"<p>The medium absolute deviation (MAD) is a measure of variability and can be used as a robust alternative for an outlier detection algorithm based on the more common standard deviation \\(\\sigma\\) (see Wikipedia). The MAD is defined as the median of the absolute deviations from the data median:</p> \\[ MAD = c * median(|x_{i} - \\bar{x}|) \\] <p>where \\(x_{i}\\) are the data values, \\(\\bar{x}\\) the data median value and \\(c\\) is a constant, with \\(c = 1.4826\\) for normally distributed values. The lower and upper threshold values \\(T\\) for the outlier detection are computed as follows:</p> \\[ \\displaylines{     T_{lower} = \\bar{x} - k * MAD \\\\     T_{upper} = \\bar{x} + k * MAD } \\] <p>where \\(k = 3\\) corresponds to the \\(3\\sigma\\) rule commonly used in threshold-based outlier detection.</p> <p>The assumption of a normal distribution, however, is often not suitable for real world datasets. In order to account for skewed distributions when detecting outliers, a double MAD algorithm has been proposed (Rosenmai, 2013). This simple approach includes the separate computation of a lower and upper MAD for values below and above the median:</p> \\[ \\displaylines{     MAD^{lower} = c * median(|x_i^{lower} - \\bar{x}|) \\\\     MAD^{upper} = c * median(|x_i^{upper} - \\bar{x}|) } \\] <p>Consequently, the thresholds for the outlier detection change based on the corresponding MAD:</p> \\[ \\displaylines{     T_{lower} = \\bar{x} - k * MAD^{lower} \\\\     T_{upper} = \\bar{x} + k * MAD^{upper}  } \\] <p>Here, the MAD is used to identify and remove outliers from the picked seafloor horizon (red dots, Figure).</p>"},{"location":"2D/static_correction/#smoothing-filter","title":"Smoothing filter","text":"<p>The seafloor horizon is low-passed filtered using a Savitzky-Golay filter (Savitzky and Golay, 1964) to derive a smooth version closely representing the seafloor unaffected by static (solid blue line, Figure).  One advantage of this filter is its capability to efficiently smooth data without distorting the underlying signal. The efffect of residual static is calculated as the difference between observed, non-smoothed and filtered, smoothed seafloor horizon (red line, Figure).</p>"},{"location":"2D/static_correction/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/static_correction/#command-line-interface","title":"Command line interface","text":"<p>The script can handle three different inputs:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 05_correct_static {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.static_correction_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--inplace</code>: Replace input data without creating copy (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--use_delay</code>: Use delay recording time to split input data before despiking (e.g. for TOPAS, Parasound).</li> <li><code>--byte_delay {109}</code>: Byte position of input delay times in SEG-Y file(s) (default: <code>109</code>).</li> <li><code>--mode {amp}</code>: Mode used to correct residual static (default: <code>amp</code>).<ul> <li><code>amp</code>: detected seafloor amplitude </li> <li><code>swdep</code>: stored SourceWaterDepth from trace header (if available).</li> </ul> </li> <li><code>--win_mad {TRACES}</code>: Moving window length for MAD filter (traces [#]).</li> <li><code>--window_sg {7}</code>: Moving window length for Savitzky-Golay smoothing filter (traces [#]).</li> <li><code>--write_seafloor2trace</code>: If mode is <code>amp</code> write TWT of peak seafloor amplitude to SEG-Y trace header</li> <li><code>--write_aux</code>: Write trace information and computed static to auxiliary file (<code>*.sta</code>)</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/static_correction/#references","title":"References","text":"<ul> <li>Rosenmai, Peter (2013) Using the Median Absolute Deviation to Find Outliers, https://eurekastatistics.com/using-the-median-absolute-deviation-to-find-outliers/, last accessed: 27 June 2022</li> <li>Savitzky, A., &amp; Golay, M. J. (1964). Smoothing and differentiation of data by simplified least squares procedures. Analytical chemistry, 36(8), 1627-1639.</li> </ul>"},{"location":"2D/tide_compensation/","title":"Tide compensation","text":"<p>Compensate varying tidal elevations that might occur during seismo-acoustic surveys. </p>"},{"location":"2D/tide_compensation/#description","title":"Description","text":"<p>The tidal elevation (and thus deviation from the mean sea level) is predicted using the tidal constituents based on the TPXO9-atlas models provided by Gary Egbert &amp; Svetlana Erofeeva from the Oregon State University (available on request for academic purposes).</p> <p>To derive an individual tidal elevation for each shotpoint (and time) the Python package <code>tpxo-tide-prediction</code> is utilized that allows to compute the required tidal information for each shotpoint (i.e. timestamp) of a seismo-acoustic profile.</p> <p>For more information refer to the GitHub project page.</p> <p> </p> Figure 1: Vertical offset of individual TOPAS profiles caused by varying tidal elevation (left). Example of tidal compensation for single profile (right)."},{"location":"2D/tide_compensation/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"2D/tide_compensation/#command-line-interface","title":"Command line interface","text":"<p>The script can handle three different inputs:</p> <ol> <li>single SEG-Y file (e.g., <code>filename.sgy</code>)</li> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>Additionally, the directory path of the tidal constituent netCDF files (provided from OSU) is required (<code>model_dir</code>).</p> <p>There are two options to run the script. We recommend using the CLI entry point like:</p> <pre><code>&gt;&gt;&gt; 06_compensate_tide {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> <p>Alternatively, the script can be executed using the (more verbose) command:</p> <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.tide_compensation_segy {filename.sgy | datalist.txt | &lt;/directory&gt;} [optional parameters]\n</code></pre> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--output_dir {DIR}</code>: Output directory (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--inplace</code>: Replace input data without creating copy (either <code>--inplace</code> or <code>--output_dir</code> are required!).</li> <li><code>--suffix {sgy}</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</li> <li><code>--filename_suffix {SUFFIX}</code>: Filename suffix (e.g. <code>pad</code>, <code>static</code>) to filter input files. Only used if directory is specified.</li> <li><code>--txt_suffix {despk}</code>: Suffix to append to output filename (default: <code>despk</code>).</li> <li><code>--constituents</code>, <code>-c</code>: Available tidal constituents supported by TPXO9 atlas model.</li> <li>defaults: M2, S2, K1, O1, N2, P1, K2, Q1,</li> <li>2N2, K2, M4, MF, MM, MN4, MS4,</li> <li>S1 (only v5)</li> <li><code>--correct_minor</code>: Correct for minor tidal constituents.</li> <li><code>--src_coords</code>: Byte position of input coordinates in SEG-Y file(s).</li> <li><code>source</code> (default)</li> <li><code>CDP</code></li> <li><code>group</code></li> <li><code>--crs_src</code>: Source CRS of SEG-Y file(s). Indicate using EPSG code or PROJ.4 string.</li> <li>default: EPSG:32760 (WGS 84 / UTM zone 60S)</li> <li><code>--write_aux</code>: Write times and tide predictions to auxiliary file (<code>*.tid</code>).</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"2D/tide_compensation/#python-script","title":"Python script","text":"<p>Additionally, it is possible to just import the essential functions <code>read_parameter_file</code>, <code>tide_predict</code>, and <code>write_tides</code> when using these methods in a custom script:</p>"},{"location":"2D/tide_compensation/#import-package","title":"Import package","text":"<pre><code>from tpxo_tide_prediction import (\nread_parameter_file,\ntide_predict,\nwrite_tides\n)\n</code></pre>"},{"location":"2D/tide_compensation/#read-input-parameter-from-file","title":"Read input parameter from file","text":"<pre><code># (A) read inputs from parameter file\nlat, lon, times = read_parameter_file('path/to/parameter/file.txt')\n# (B) read only `time` from parameter file and provide fixed location\nlat = -36.446349\nlon = 175.166068\nlat, lon, times = read_parameter_file('path/to/parameter/file', lat, lon)\n</code></pre>"},{"location":"2D/tide_compensation/#compute-tidal-elevation","title":"Compute tidal elevation","text":"<pre><code># lat, lon:  int, float, np.ndarray\n# times:     str, np.ndarray(dtype='datetime64[s]')\n# (A) default configuration\ntide = tide_predict('path/to/TPXO9-atlas-model', lat, lon, times)\n# (B) custom configuration (less constituents, no correction of minor constituents)\ntide = tide_predict('path/to/TPXO9-atlas-model', lat, lon, times,\nconstituents=['m2','s2','n2','k2'], correct_minor=False)\n</code></pre>"},{"location":"2D/tide_compensation/#write-computed-tides-to-formatted-output-file","title":"Write computed tides to formatted output file","text":"<pre><code># (A) custom output file\n# full output (tide at every time at every location)\nwrite_tides('path/to/output/file.tide', tide, mode='full')\n# or `track` output (lat-lon-time triplets)\nwrite_tides('path/to/output/file.tide', tide, mode='track')\n# (B) create output file from parameter file\nbasepath, filename = os.path.split('path/to/parameter/file.txt')\nbasename, suffix = os.path.splitext(filename)\nwrite_tides(basepath, basename, tide, mode='full')\n</code></pre>"},{"location":"2D/tide_compensation/#references","title":"References","text":"<p>Egbert, Gary D., and Svetlana Y. Erofeeva. \"Efficient inverse modeling of barotropic ocean tides.\" Journal of Atmospheric and Oceanic Technology 19.2 (2002): 183-204.</p>"},{"location":"3D/3D_cube_interpolation/","title":"3D interpolation of sparse cube","text":"<p>Interpolating sparse 3D cube using Projection Onto Convex Sets (POCS) method with user-specified transform (e.g. <code>FFT</code>, <code>wavelet</code>, <code>shearlet</code>, or <code>curvelet</code>).</p>"},{"location":"3D/3D_cube_interpolation/#description","title":"Description","text":"<p>For a more detailed description, please refer to the journal article about this workflow.</p>"},{"location":"3D/3D_cube_interpolation/#pocs-theory","title":"POCS theory","text":"<p>The POCS is an iterative algorithm, which can be used to reconstruct seismic data (Abma and Kabir, 2006). The iterative formula of the weighted POCS algorithm at the k-th iteration is:</p> \\[ \\textbf{D}_k = \\alpha\\textbf{D}_{obs} + [\\textbf{I} - \\alpha \\textbf{M}]  A^{-1} \\textbf{T}_k A \\textbf{D}_{k-1}, \\quad k=1,2,...,N \\] <p>where \\(\\textbf{D}_k\\) is the reconstructed data at iteration \\(k\\), \\(\\alpha\\) is a weighting factor (\\(0 &lt; \\alpha \u2264 1\\)), \\(\\textbf{D}_{obs}\\) is the observed sparse data, \\(\\textbf{I}\\) is the identity matrix, \\(\\textbf{M}\\) is the sampling matrix, \\(A\\) and \\(A^{-1}\\) the forward and inverse transforms (e.g., FFT), respecitvely. \\(N\\) is the total number of iterations and \\(\\textbf{T}_k\\) the iteration-dependent threshold.</p> <p>This threshold operator is either hard (L0 norm) or soft (L1 norm) and illustrated in Figure 1a. For improved convergence, three (3) different threshold models are implemented (Figure 1b).</p> <ul> <li>linear</li> <li>exponential (adjustable via exponent \\(q\\))</li> <li>data-driven</li> </ul> <p> </p> Figure 1: (a) Threshold operators, (b) threshold models. <p>Threshold operator and model</p> <p>A decision for the hard or soft threshold operator is largely dependent on the input data. There are; however, no large difference based on our numerical testing. With regards to the threshold model, the exponential decay function offers the most flexibility while producing reliable results.</p> <p>Besides the regular POCS, our implementation supports the fast (FPOCS) and data-driven POCS.The FPOCS is based on the work by Yang et al. (2013) as well as Gan et al. (2015, 2016) and implemented as:</p> \\[ \\displaylines{     \\textbf{D}_k^{`} = \\textbf{D}_k + \\frac{v_k-1}{v_k+1}(\\textbf{D}_k - \\textbf{D}_{k-1}), \\\\     \\textbf{D}_{k+1} = \\textbf{D}_{obs} + (\\textbf{I}-\\textbf{M})\\textbf{A}^{-1}\\textbf{T}_{\\tau}(\\textbf{A}\\textbf{D}_k^{`}) } \\] <p>with \\(v_n\\) as controlling parameter with initial values \\(v_0 = v_1 = 1\\) and \\(v_n = (1 + \\sqrt{1 + 4 v_n^2})/2\\).</p> <p>Choosing the correct POCS implementation</p> <p>The FPOCS is our recommended implementation as its convergence is much faster than of the regular and data-driven POCS by using less iterations. The effectiveness of the data-driven POCS is largly dependent on the input data quality and, based on our experiences, not always suitable for field data. </p> <p>The convergence is monitored by a cost function \\(J_k\\), which is compared to the user-defined stop criterion \\(\\epsilon\\):</p> \\[ J_k = \\frac{ || \\textbf{D}_{k} -  \\textbf{D}_{k-1}||^2_2 }{ || \\textbf{D}_{k}||^2_2 } \\] <p>A flowchart of the POCS algorithm is shown in Figure 2:</p> <p></p> Figure 2: Conceptual flowchart of POCS algorithm in the frequency domain."},{"location":"3D/3D_cube_interpolation/#implementation","title":"Implementation","text":"<p>This POCS implementation designed to interpolate multiple 2D frequency (or time) slices in parallel. The workflow could be relatively easy scaled from a local workstation or laptop to a remote cluster, which is possible by utilizing the <code>dask</code> package.</p> <p>Each CPU receives a number of 2D slices (e.g., <code>batch_chunk=20</code>), which are save to individual netCDF files (Figure 3c) before all chunks are merged into a single cube (Figure 3d).</p> <p></p> Figure 3: Conceptual diagram showing the parallel interpolation of multiple frequency or time slices. <p>For more detail regarding the implementation, please refer to the API documenation.</p>"},{"location":"3D/3D_cube_interpolation/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"3D/3D_cube_interpolation/#command-line-interface","title":"Command line interface","text":"<p>The script requires a single netCDF (3D) and a configuration YAML file:</p> <p>There are two options to run the script. We recommend using the CLI entry point like:</p> <pre><code>&gt;&gt;&gt; 13_cube_interpolate_POCS /path/to/sparse_cube.nc \\\n--path_pocs_parameter /path/to/config.yml [optional parameters]\n</code></pre> <p>Alternatively, the script can be executed using the (more verbose) command:</p> <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cube_POCS_interpolation_3D /path/to/sparse_cube.nc \\\n--path_pocs_parameter /path/to/config.yml [optional parameters]\n</code></pre> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--path_pocs_parameter</code>: Path of netCDF parameter file (YAML format). Required!</li> <li><code>--path_output_dir</code>: Output directory for interpolated slices. Defaults to basedir of input cube.</li> <li><code>--verbose {LEVEL}</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"3D/3D_cube_interpolation/#configuration-file","title":"Configuration file","text":"<p>The configuration file is the essential input for the 3D interpolation. In there, all relevant input parameter for the PCOS algorithm need to be specified. An example file might look like this:</p> <pre><code># === NETCDF PARAMETER ===\n# dimension name of input cube, e.g. 'freq_twt' (frequency domain) or 'twt' (time domain)\ndim: 'freq_twt'\n# name of netCDF variable\nvar: 'freq_env'\n# === DASK PARAMETER ===\n# number of slices per worker (e.g. CPU)\nbatch_chunk: 20\n# number of dask worker (i.e. CPUs on local machine)\nn_workers: 8\n# use `processes` (**recommended!**) or `threads`\nprocesses: False\nthreads_per_worker: 2\nmemory_limit: '3.5GB'\n# === POCS PARAMETER ===\nmetadata:\n# sparse transform\ntransform_kind: 'FFT'\nniter: 50\neps: 1e-16\n# treshold operator (hard, garrote, soft)\nthresh_op: 'hard'\n# threshold model (linear, exponential, data-driven, inverse-proportional)\nthresh_model: 'exponential-1'\n# threshold decay kind (values, )\ndecay_kind: 'values'\n# max regularization percentage\np_max: 0.99\n# min regularization percentage 1e-4\np_min: 'adaptive'\n# weighting factor\nalpha: 0.75\n# apply np.sqrt to threshold decay\nsqrt_decay: False\n# POCS version (regular, fast, data-driven)\nversion: 'fast'\n# verbose output to stdout\nverbose: False\n# apply conservative smoothing filter over interpolated slices prior to merging ('gauss', 'median', False)\napply_filter: 'gauss'\n# === DEV ===\noutput_runtime_results: true\n</code></pre>"},{"location":"3D/3D_cube_interpolation/#references","title":"References","text":"<ul> <li>Abma, Ray, and Nurul Kabir. 2006. \u201c3D Interpolation of Irregular Data with a POCS Algorithm.\u201d Geophysics 71 (6): E91\u201397. https://doi.org/10.1190/1.2356088.</li> <li>Gan, Shuwei, Shoudong Wang, Yangkang Chen, and Xiaohong Chen. 2015. \u201cSeismic Data Reconstruction via Fast Projection onto Convex Sets in the Seislet Transform Domain.\u201d In SEG Technical Program Expanded Abstracts 2015, 3814\u201319. SEG Technical Program Expanded Abstracts. Society of Exploration Geophysicists. https://doi.org/10.1190/segam2015-5744782.1.</li> <li>Gan, Shuwei, Shoudong Wang, Yangkang Chen, Xiaohong Chen, Weiling Huang, and Hanming Chen. 2016. \u201cCompressive Sensing for Seismic Data Reconstruction via Fast Projection onto Convex Sets Based on Seislet Transform.\u201d Journal of Applied Geophysics 130: 194\u2013208. https://doi.org/10.1016/j.jappgeo.2016.03.033.</li> <li>Yang, Pengliang, Jinghuai Gao, and Wenchao Chen. 2013. \u201cOn Analysis-Based Two-Step Interpolation Methods for Randomly Sampled Seismic Data.\u201d Computers &amp; Geosciences 51: 449\u201361. https://doi.org/10.1016/j.cageo.2012.07.023</li> </ul>"},{"location":"3D/apply_FFT_3D/","title":"Time to frequency domain conversion","text":"<p>Apply a time to frequency domain conversion using a 1D Fast Fourier Transform (FFT) along the time axis of an input 3D cube.</p>"},{"location":"3D/apply_FFT_3D/#description","title":"Description","text":"<p>This script uses <code>xrft.xrft.fft</code> to compute the forward FFT. <code>xrft</code> is build on <code>xarray</code>, <code>dask</code> and <code>numpy</code> and preserves netCDF metadata during computations.</p>"},{"location":"3D/apply_FFT_3D/#command-line-interface","title":"Command line interface","text":"<p>The script needs a single netCDF (3D) as input (in time domain):</p> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 12_cube_apply_FFT /path/to/cube.nc --params_netcdf /path/to/config.yml [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cube_apply_FFT /path/to/cube.nc \\\n--params_netcdf /path/to/config.yml [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--params_netcdf</code>: Path of netCDF parameter file (YAML format). Required!</li> <li><code>--prefix</code>: Prefix for new netCDF variable and coordinate.</li> <li><code>--compute_real</code>: Compute FFT assuming real input and thus discarting redundant negative frequencies.</li> <li><code>--filter {lowpas | highpass | bandpass}</code>: Optional filter to apply prior to FFT computation.</li> <li><code>--filter_freqs</code>: Filter corner frequencies (in Hz).</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"3D/apply_inverse_FFT_3D/","title":"Frequency to time domain conversion","text":"<p>Apply a frequency to time domain conversion using a 1D Fast Fourier Transform (FFT) along the time axis of an input 3D cube.</p>"},{"location":"3D/apply_inverse_FFT_3D/#description","title":"Description","text":"<p>This script uses <code>xrft.xrft.ifft</code> to compute the inverse FFT. <code>xrft</code> is build on <code>xarray</code>, <code>dask</code> and <code>numpy</code> and preserves netCDF metadata during computations.</p>"},{"location":"3D/apply_inverse_FFT_3D/#command-line-interface","title":"Command line interface","text":"<p>The script needs a single netCDF (3D) as input (in frequency domain):</p> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 14_cube_apply_IFFT /path/to/cube.nc --params_netcdf /path/to/config.yml [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cube_apply_IFFT /path/to/cube.nc \\\n--params_netcdf /path/to/config.yml [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--params_netcdf</code>: Path of netCDF parameter file (YAML format). Required!</li> <li><code>--compute_real</code>: Compute FFT assuming real input and thus discarting redundant negative frequencies.</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"3D/convert_netcdf2segy_3D/","title":"netCDF to SEG-Y conversion","text":"<p>This utility script converts pseudo-3D cube from netCDF-4 to SEG-Y format.</p>"},{"location":"3D/convert_netcdf2segy_3D/#description","title":"Description","text":"<p>The format conversion is done by utilizing the <code>SEGY-SAK</code>  together with the <code>segyio</code> packages.</p>"},{"location":"3D/convert_netcdf2segy_3D/#command-line-interface","title":"Command line interface","text":"<p>The script accepts a netCDF cube (<code>*.nc</code>) as input:</p> <p>There are two options to run the script. We recommend using the CLI entry point like: <pre><code>&gt;&gt;&gt; 15_cube_cnv_netcdf2segy /path/to/cube.nc --params_netcdf /path/to/config.yml [optional parameters]\n</code></pre> Alternatively, the script can be executed using the (more verbose) command: <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cube_cnv_netcdf2segy_3D /path/to/cube.nc \\\n--params_netcdf /path/to/config.yml [optional parameters]\n</code></pre></p> <p>Optionally, the following parameters can be specified:</p> <ul> <li><code>--help</code>, <code>-h</code>: Show help.</li> <li><code>--params_netcdf</code>: Path of netCDF parameter file (YAML format). Required!</li> <li><code>--path_segy</code>: Path of output SEG-Y file. Defaults to using input netCDF filename.</li> <li><code>--scalar_coords</code>: Coordinate scalar for SEG-Y trace header [100, 10, 1, -10, -100, 'auto']. Defaults to <code>auto</code>, i.e. applying a suitable scalar derived from the coordinates stored in the netCDF.</li> <li><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</li> </ul>"},{"location":"3D/cube_binning_geometry/","title":"3D binning and geometry setup","text":"<p>Create sparse 3D cube geometry by binning traces from 2D profiles.</p>"},{"location":"3D/cube_binning_geometry/#description","title":"Description","text":"<p>This script uses the following processing steps to setup a 3D cube geometry and assign traces to each bin:</p> <ol> <li>Setup 3D geometry using<ul> <li>rotation center and angle</li> <li>output cube extent (optionally with reference to study area extent)</li> </ul> </li> <li>Transform coordinates of all 2D profiles (x,y \u2192 iline/xline) using Affine transformation</li> <li>Stack multiple traces within a single bin using <ul> <li>average amplitude</li> <li>median amplitude</li> <li>nearest trace(s) (calculated using spatial distance \\(d\\))</li> <li>Inverse Distance Weighting (IDW) algorithm </li> </ul> </li> <li>Create inline netCDF files from 2D profiles</li> <li>Merge all inlines into a single sparse 3D cube</li> <li>Transpose cube to time-major layout (il, xl, twt) \u2192 (twt, il, xl)</li> </ol> <p></p> Figure: Conceptual 3D binning process, where tr: trace, d: distance. <p>Suitable bin sizes</p> <p>The bin sizes for a given dataset are largely dependent on (a) the line spacing and (b) the trace distance along individual lines. In case of survey lines with a predominant orientation, consider using an asymmetric bin size (e.g., 15 x 5 m).</p>"},{"location":"3D/cube_binning_geometry/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"3D/cube_binning_geometry/#command-line-interface","title":"Command line interface","text":"<p>The script can handle two different inputs:</p> <ol> <li>datalist of files to process (e.g., <code>datalist.txt</code>)</li> <li>directory with input files (e.g., <code>/input_dir</code>) </li> </ol> <p>There are two options to run the script. We recommend using the CLI entry point like:</p> <pre><code>&gt;&gt;&gt; 10_cube_geometry_binning {datalist.txt | &lt;/directory&gt;} --params_netcdf /path/to/config.yml\n    --params_spatial_ref /path/to/spatial_ref.yml --params_cube_setup /path/to/cube_setup.yml\n    --path_coords {/path/to/segys/ | /path/to/auxiliary/file.nav}\n[optional parameters]\n</code></pre> <p>Alternatively, the script can be executed using the (more verbose) command:</p> <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cube_binning_3D {datalist.txt | &lt;/directory&gt;} --params_netcdf /path/to/config.yml --params_spatial_ref /path/to/spatial_ref.yml\n    --params_cube_setup /path/to/cube_setup.yml\n    --path_coords {/path/to/segys/ | /path/to/auxiliary/file.nav}\n[optional parameters]\n</code></pre> <p>Optionally, the following parameters can be specified:</p> <ul> <li> <p><code>--help</code>, <code>-h</code>: Show help.</p> </li> <li> <p><code>--params_netcdf</code>: Path of netCDF parameter file (YAML format). Required!</p> </li> <li> <p><code>--params_spatial_ref</code>: Path of spatial reference parameter file with CRS as WKT string (YAML format). Required!</p> </li> <li> <p><code>--params_cube_setup</code>: Path of config file for cube geometry setup (YAML format). Required!</p> </li> <li> <p><code>--output_dir {DIR}</code>: Output directory for edited SEG-Y file(s).</p> </li> <li> <p><code>--suffix {sgy}</code>, <code>-s</code>: File suffix (default: <code>sgy</code>). Only used if directory is specified.</p> </li> <li> <p><code>--filename_suffix {SUFFIX}</code>: Filename suffix for guided selection (e.g. <code>env</code> or <code>despk</code>). Only used when <code>input_path</code> is a directory.</p> </li> <li> <p><code>--attribute</code>, <code>-a:</code>: Seismic attribute to compute.</p> </li> <li> <p><code>--coords_origin {header | aux}</code>: Origin of (shotpoint) coordinates (default: <code>header</code>).</p> </li> <li> <p><code>--path_coords</code>: Path to SEG-Y directory (coords_origin = <code>header</code>) or auxiliary navigation file with coordinates (coords_origin = <code>aux</code>). Required!</p> </li> <li> <p><code>--coords_fsuffix</code>: File suffix of auxiliary or SEG-Y files (depending on chosen parameter for <code>coords_origin</code>.</p> </li> <li> <p><code>--bin_size</code>: Bin size(s) in inline and crossline direction(s) given in CRS units (e.g., meter). Single value or space-separated <code>inline</code> and <code>crossline</code> values.</p> </li> <li> <p><code>--twt_limits {MIN MAX}</code>: Vertical two-way travel time range of output 3D cube (in ms).</p> </li> <li> <p><code>--parallel</code>: Process files in parallel (default: False)</p> </li> <li> <p><code>--encode</code>: Use encoding to compress output file size based on JSON parameter file (<code>params_netcdf</code>).</p> </li> <li> <p><code>--stacking_method</code>: Stacking method for multiple traces within one bin (default: <code>average</code>)</p> </li> <li> <p>'average', 'median', 'nearest', or 'IDW'</p> </li> <li> <p><code>--factor_dist</code>: Distance factor controlling the impact of weighting function: \\(1/(distance^{factor})\\). Only used if stacking_method=<code>IDW</code>.</p> </li> <li> <p><code>--dtype_data</code>: Output dtype of created 3D cube (default: <code>float32</code>).</p> </li> <li> <p><code>--name</code>: Optional identifier string to add to exported files.</p> </li> <li> <p><code>--write_aux</code>: Write auxiliary files featuring key cube parameters (default: <code>False</code>).</p> </li> <li> <p><code>--verbose {LEVEL}</code>, <code>-V</code>: Level of output verbosity (default: <code>0</code>).</p> </li> </ul>"},{"location":"3D/cube_binning_geometry/#configuration-files","title":"Configuration files","text":""},{"location":"3D/cube_binning_geometry/#netcdf-parameter","title":"netCDF parameter","text":"<p>This config file specifies netCDF metadata information and is used in subsequent scripts as well.</p> <pre><code># parameter for frequency domain\nattrs_freq:\ndata:\ndescription: Spectral amplitudes of frequency spectrum\nlong_name: amplitude\nunits: '-'\nnew_dim:\ndescription: Frequencies of seismic signal\nlong_name: frequency\nunits: kHz\n# parameter for time domain\nattrs_time:\namp:\ndescription: Measure of reflection strength of property contrast between layers\nlong_name: seismic amplitude\nseismic_attribute: amplitude\nunits: '-'\ncube:\ndescription: Sparse pseudo-3D cube created from TOPAS profiles\nhistory: ''\nlong_name: central cube\nenv:\ndescription: Phase-independent representation of seismic amplitude (instantaneous\namplitude)\nlong_name: signal envelope\nseismic_attribute: envelope\nunits: '-'\nfold:\ndescription: Number of stacked traces per bin\nunits: '#'\niline:\nlong_name: iline\nunits: '#'\ntwt:\nlong_name: two-way travel time\nstandard_name: TWT\nunits: ms\nx:\nlong_name: easting\nstandard_name: projection_x_coordinate\nunits: m\nxline:\nlong_name: xline\nunits: '#'\ny:\nlong_name: northing\nstandard_name: projection_y_coordinate\nunits: m\nencodings:\namp:\n_FillValue: -32768\ndtype: int16\nscale_factor: 5.0e-05\nenv:\n_FillValue: 0\ndtype: uint16\nscale_factor: 3.0e-05\n# auxiliary netCDF variables\nvar_aux:\n- fold\n- ref_amp\n</code></pre>"},{"location":"3D/cube_binning_geometry/#spatial-reference","title":"Spatial reference","text":"<p>The coordinate reference system (CRS) is specified as a WKT string in a separated YAML file. An example for <code>WGS 84 / UTM zone 60S</code> is shown below:</p> <pre><code>PROJCS[\"WGS 84 / UTM zone 60S\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",177],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",10000000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32760\"]]\n</code></pre>"},{"location":"3D/cube_binning_geometry/#3d-cube-setup-parameter","title":"3D cube setup parameter","text":"<p>All required information for the geometry of the created 3D cube are specified in a designated YAML file. An example file might look like this:</p> <pre><code># name of pseudo-3D cube\nname: cube_center\nlong_name: central cube\n# seismic attribute, e.g amplitude (amp), envelope (env)\nattribute: amp\n# bin size(s) for il/xl directions (in CRS units, e.g. meter)\nbin_size: #5\n- 5   # iline bin size (along y-axis), equals distance between crosslines (`dxline`)\n- 5   # xline bin size (along x-axis), equals distance between inlines (`diline`)\n# min/max TWT limits of output pseudo-3D cube (in ms)\ntwt_limits: - 725\n- 925\n# Stack multiple traces within a single bin using this method ('average', 'median', 'nearest', 'IDW')\nstacking_method: IDW\nfactor_dist: 1.0    # only used if `stacking_method` == 'IDW'\n# spatial reference of extent coordinates (should be identical to SEG-Y CRS)\nspatial_ref: EPSG:32760\nrotation_center: - 297335 - 5125120\n# positive in clockwise direction (in degree)\nrotation_angle: 12\n# extent of the actual pseudo-3D cube to generate\nextent_cube:\nlower_left: - 296586.4282892714\n- 5123519.385932244\nupper_left:\n- 295994.9195288948\n- 5126302.215856332\nupper_right:\n- 298063.7017044469\n- 5126741.949082412\nlower_right:\n- 298655.2104648235\n- 5123959.119158324\n# OPTIONAL: extent of the overall study area (must be larger than `extent_cube`)\nextent_study_area:\nlower_left: - 294112.69854084\n- 5120631.97166828\nupper_left:\n- 292594.94319787\n- 5127772.44915364\nupper_right:\n- 300664.66090393\n- 5129487.72060288\nlower_right:\n- 302182.4162469\n- 5122347.24311753\n</code></pre>"},{"location":"3D/preprocessing_3D/","title":"Pre-processing (3D)","text":"<p>Recommended pre-processing steps applied to sparse 3D cube prior to POCS interpolation.</p>"},{"location":"3D/preprocessing_3D/#description","title":"Description","text":""},{"location":"3D/preprocessing_3D/#gain-application","title":"Gain application","text":"<p>Optionally, one can apply one or multiple gain function(s) following the approach of the Seismic Unix module <code>sugain</code>. For more information about the usage and input parameter of this module please refer to its manual.</p> <p>Common gain parameter to compensate the spherical divergence effect would be:</p> <pre><code>--gain tpow=2 gpow=0.5\n</code></pre> <p> Figure 1: Schematic of gain application, e.g. to compensate the spherical divergence.</p>"},{"location":"3D/preprocessing_3D/#trace-balancing","title":"Trace balancing","text":"<p>Varying environmental conditions like the sea state might influence the recorded signal strength, which can be compesated by balancing all traces to with reference to their respective root mean square (rms) \\(A(t)_b = A(t)/A(t)_{rms}\\) or maximum amplitude \\(A(t)_b = A(t)/A(t)_{max}\\).</p> <p>The rms amplitude is calculated using the amplitudes \\(a\\) and the total number of samples \\(N\\):</p> \\[ rms = \\sqrt{\\frac{\\sum{a^2}}{N}} \\] <p>Trace balancing is triggered using the following command line parameter:</p> <pre><code>--balance {rms | max}\n</code></pre> <p> Figure 2: Schematic of trace (amplitude) balancing.</p>"},{"location":"3D/preprocessing_3D/#frequency-filtering","title":"Frequency filtering","text":"<p>The data can be filtered in the frequency domain to remove low and/or high-frequency noise outside the signal bandwidth. Three different filters are supported:</p> <ul> <li>highpass: <code>--filter highpass --filter_freqs {f_cutoff | f1} {f_stopband | f2}</code></li> <li>lowpass: <code>--filter lowpass --filter_freqs {f_stopband | f3} {f_cutoff | f4}</code></li> <li>bandpass: <code>--filter bandpass --filter_freqs f1 f2 f3 f4</code></li> </ul> <p>Info</p> <p>The frequency must be specified in Hertz (Hz).</p> <p> Figure 3: Schematic of band-pass filtering in the frequency domain.</p>"},{"location":"3D/preprocessing_3D/#trace-resampling","title":"Trace resampling","text":"<p>Both up and downsampling (Figure 4) are supported using either <code>scipy.signal.resample</code> or <code>scipy.signal.resample_poly</code>. Additionally, either <code>resampling_interval</code>, <code>resampling_frequency</code>, or <code>resampling_factor</code> must be provided. A window function (<code>window_resample</code>) can be applied before resampling the traces.</p> <p>The command line input to downsample the input data by factor 2 using <code>scipy.signal.resample</code> would look like this:</p> <pre><code>--resampling_function resample --resampling_factor 2\n</code></pre> <p> Figure 4: Schematic of trace resampling, e.g. downsampling by factor 2.</p> <p>Warning</p> <p>Be aware of the Nyquist frequency when downsampling to avoid aliasing.</p>"},{"location":"3D/preprocessing_3D/#trace-envelope","title":"Trace envelope","text":"<p>In addition, this script can calculate the trace amplitude (instantaneous amplitude) for all input traces of the sparse cube. The envelope of a seismic trace \\(E(t)\\) is independent of the phase and calculated using the following equation (SEG Wiki):</p> \\[ E(t) = \\sqrt{(\\textrm{Real}\\ s(t))^2 + (\\textrm{Imag}\\ s(t))^2} \\] <p>with \\(\\textrm{Real}\\) and \\(\\textrm{Imag}\\) representing the real and imaginary parts of the analytic trace, respectively. In this case, the \\(\\textrm{Real}\\ s(t)\\) is just the original seismic signal, whereas \\(\\textrm{Imag}\\ s(t)\\) is computed using the Hilbert transform of the signal (SEG Wiki). Computing the trace envelope is a common processing step in order to display sub-bottom profiler data with hihg vertical sampling rates like TOPAS or PARASOUND (Henkart, 2006).</p> <p>The following flag triggers the envelope calculation (after any previously applied processing step):</p> <pre><code>--envelope\n</code></pre> <p> Figure 5: Schematic of seismic signal (grey) and its envelope (black).</p>"},{"location":"3D/preprocessing_3D/#usage","title":"Usage","text":"<p>This script is designed to be used from the terminal (i.e. command line).</p>"},{"location":"3D/preprocessing_3D/#command-line-interface","title":"Command line interface","text":"<p>The script needs a single netCDF (3D) as input:</p> <p>There are two options to run the script. We recommend using the CLI entry point like:</p> <pre><code>&gt;&gt;&gt; 11_cube_preprocessing /path/to/cube.nc --params_netcdf /path/to/config.yml [optional parameters]\n</code></pre> <p>Alternatively, the script can be executed using the (more verbose) command:</p> <pre><code>&gt;&gt;&gt; python -m pseudo_3D_interpolation.cube_preprocessing_3D /path/to/cube.nc \\\n--params_netcdf /path/to/config.yml [optional parameters]\n</code></pre> <p>Optionally, the following parameters can be specified:</p> <ul> <li> <p><code>--help</code>, <code>-h</code>: Show help.</p> </li> <li> <p><code>--params_netcdf</code>: Path of netCDF parameter file (YAML format). Required!</p> </li> <li> <p><code>--path_out</code>: Output path of pre-processed 3D cube.</p> </li> <li> <p><code>--gain</code>: Parameter for (time-variant) gain function(s), e.g. <code>tpow=2 gpow=0.5</code></p> </li> <li> <p><code>--use_samples</code>: Use samples instead of TWT for (time-variant) gain function(s).</p> </li> <li> <p><code>--balance</code>: Method to define reference amplitude for (time-invariant) scaling.</p> </li> <li> <p><code>rms</code></p> </li> <li> <p><code>max</code></p> </li> <li> <p><code>--store_ref_amp</code>: Store reference amplitude used for trace balancing as netCDF variable.</p> </li> <li> <p><code>--filter</code>: ptional filter to apply prior to FFT computation.</p> </li> <li> <p><code>highpass</code></p> </li> <li><code>lowpass</code></li> <li> <p><code>bandpass</code></p> </li> <li> <p><code>--filter_freqs</code>: Filter corner frequencies (in Hz), e.g. <code>1000 1200 6000 6200</code></p> </li> <li> <p><code>--resampling_function</code>: Resampling function from <code>scipy.signal</code>.</p> </li> <li> <p><code>resample</code></p> </li> <li> <p><code>resample_poly</code></p> </li> <li> <p><code>--resampling_interval</code>, <code>-dt</code>: Output sampling interval/period of signal (in ms).</p> </li> <li> <p><code>--resampling_frequency</code>, <code>-fs</code>: Output sampling frequency/rate of signal (in Hz).</p> </li> <li> <p><code>--resampling_factor</code>, <code>-f</code>: Resampling factor (&lt;1: upsampling, &gt;1: downsampling).</p> </li> <li> <p><code>--window_resample</code>: Window function for resampling (from <code>scipy.signal.windows</code>).</p> </li> <li> <p><code>--envelope</code>: Calculate trace envelope.</p> </li> <li> <p><code>--verbose {LEVEL}</code>: Level of output verbosity (default: <code>0</code>).</p> </li> </ul>"},{"location":"api/api_cnv_segy2netcdf/","title":"<code>cnv_segy2netcdf.py</code>","text":"<p>Utility script to convert SEG-Y files to netCDF format using <code>segysak</code> in parallel.</p>"},{"location":"api/api_cnv_segy2netcdf/#pseudo_3D_interpolation.cnv_segy2netcdf._converter","title":"<code>_converter(file, file_dir, **kwargs_segysak)</code>","text":"<p>Wrap function to convert SEG-Y to netCDF.</p> Source code in <code>pseudo_3D_interpolation\\cnv_segy2netcdf.py</code> <pre><code>def _converter(file, file_dir, **kwargs_segysak) -&gt; None:\n\"\"\"Wrap function to convert SEG-Y to netCDF.\"\"\"\nname, suffix = os.path.splitext(file)\nout_nc = os.path.join(file_dir, name + '.seisnc')\nif kwargs_segysak == {}:\nkwargs_segysak = dict(cdp=5, cdpx=73, cdpy=77, silent=True)\nkwargs_segysak['strict'] = False\n# convert SEG-Y --&gt; netCDF\nsegy_converter(os.path.join(file_dir, file), out_nc, **kwargs_segysak)\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/","title":"<code>cube_POCS_interpolation_3D.py</code>","text":"<p>Interpolating sparse 3D volume using iterative POCS algorithm. The following sparse transforms are available:</p> <ul> <li><code>FFT</code>      (provided by <code>numpy.fft</code>)</li> <li><code>WAVELET</code>  (provided by <code>PyWavelets</code>)</li> <li><code>SHEARLET</code> (based on custom fork of <code>PyShearlets</code> package with disabled multithreading)</li> <li><code>CURVELET</code> (Unix systems only!)</li> </ul> <p>Warning</p> <p>The <code>CURVELET</code> transform from  is only available on Unix systems as it relies on <code>FFTW</code> version 2.1.5 that is obsolete and was last released in 1999. For further instruction on how to install the Python interface <code>curvelops</code> (build on top of <code>pylops</code>) please refer to their documentation.</p>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D--references","title":"References","text":"<ol> <li> <p>NumPy (GitHub)\u00a0\u21a9</p> </li> <li> <p>PyWavelets (GitHub)\u00a0\u21a9</p> </li> <li> <p>PyShearlets (GitHub)\u00a0\u21a9</p> </li> <li> <p>PyLops (GitHub)\u00a0\u21a9</p> </li> <li> <p>curvelops (GitHub)\u00a0\u21a9</p> </li> </ol>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.split_by_chunks","title":"<code>split_by_chunks(dataset)</code>","text":"<p>Split <code>xarray.Dataset</code> into sub-datasets (for netCDF export).</p> <p>Parameters:</p> <ul> <li> dataset             (<code>xr.Dataset</code>)         \u2013 <p>Input dataset to split.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>generator (of xr.Datasets)</code>         \u2013 <p>Sub-datasets of input dataset.</p> </li> </ul>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.split_by_chunks--references","title":"References","text":"<ol> <li> <p>https://ncar.github.io/esds/posts/2020/writing-multiple-netcdf-files-in-parallel-with-xarray-and-dask/#create-a-helper-function-to-split-a-dataset-into-sub-datasets \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def split_by_chunks(dataset):\n\"\"\"\n    Split `xarray.Dataset` into sub-datasets (for netCDF export).\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Input dataset to split.\n    Yields\n    ------\n    generator (of xr.Datasets)\n        Sub-datasets of input dataset.\n    References\n    ----------\n    [^1]: [https://ncar.github.io/esds/posts/2020/writing-multiple-netcdf-files-in-parallel-with-xarray-and-dask/#create-a-helper-function-to-split-a-dataset-into-sub-datasets](https://ncar.github.io/esds/posts/2020/writing-multiple-netcdf-files-in-parallel-with-xarray-and-dask/#create-a-helper-function-to-split-a-dataset-into-sub-datasets)\n    \"\"\"\nchunk_slices = {}\nfor dim, chunks in dataset.chunks.items():\nslices = []\nstart = 0\nfor chunk in chunks:\nif start &gt;= dataset.sizes[dim]:\nbreak\nstop = start + chunk\nslices.append(slice(start, stop))\nstart = stop\nchunk_slices[dim] = slices\nfor slices in itertools.product(*chunk_slices.values()):\nselection = dict(zip(chunk_slices.keys(), slices))\nyield dataset[selection]\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.dataset_subsets","title":"<code>dataset_subsets(dataset, dim, size)</code>","text":"<p>Generate dataset views of given <code>size</code> along <code>dim</code>.</p> <p>Parameters:</p> <ul> <li> dataset             (<code>xr.Dataset</code>)         \u2013 <p>Dataset to subset.</p> </li> <li> dim             (<code>str</code>)         \u2013 <p>Dimension along which to subset.</p> </li> <li> size             (<code>int</code>)         \u2013 <p>Size of subset along <code>dim</code>.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>xr.Dataset</code>         \u2013 <p>Subset of input dataset.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def dataset_subsets(dataset, dim: str, size: int):\n\"\"\"\n    Generate dataset views of given `size` along `dim`.\n    Parameters\n    ----------\n    dataset : xr.Dataset\n        Dataset to subset.\n    dim : str\n        Dimension along which to subset.\n    size : int\n        Size of subset along `dim`.\n    Yields\n    ------\n    xr.Dataset\n        Subset of input dataset.\n    \"\"\"\nindices = list(range(0, dataset[dim].size + size, size))\nfor start, end in zip(indices[:-1], indices[1:]):\nyield dataset[{dim: slice(start, end)}]\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.create_file_path","title":"<code>create_file_path(ds, dim='twt', prefix=None, root_path='.')</code>","text":"<p>Generate a file path when given an <code>xarray.Dataset</code>.</p> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def create_file_path(ds, dim='twt', prefix=None, root_path='.'):\n\"\"\"Generate a file path when given an `xarray.Dataset`.\"\"\"\nif prefix is None:\nprefix = datetime.datetime.today().strftime('%Y-%m-%d')\ntry:\nstart = ds[dim].data[0]\nend = ds[dim].data[-1]  # noqa\nexcept IndexError:\nstart = np.atleast_1d(ds[dim].data)[0]\nend = np.atleast_1d(ds[dim].data)[-1]  # noqa\nreturn os.path.join(root_path, f'{prefix}_{start:06.3f}_{end:06.3f}.nc')\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.split_complex_variable","title":"<code>split_complex_variable(ds, var, out_dtype='float32')</code>","text":"<p>Split complex <code>xr.DataArray</code> variable (<code>var</code>) into Real and Imag parts.</p> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def split_complex_variable(ds, var, out_dtype='float32'):\n\"\"\"Split complex `xr.DataArray` variable (`var`) into _Real_ and _Imag_ parts.\"\"\"\nds[f'{var}.real'] = ds[var].real\nds[f'{var}.imag'] = ds[var].imag\nreturn ds.drop(var)\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.gaussian_filter_rescale","title":"<code>gaussian_filter_rescale(data, sigma=1)</code>","text":"<p>Filter and rescale 2D array using Gaussian function.</p> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def gaussian_filter_rescale(data: np.ndarray, sigma=1) -&gt; np.ndarray:\n\"\"\"Filter and rescale 2D array using Gaussian function.\"\"\"\nreturn rescale(gaussian_filter(data, sigma=sigma), vmin=data.min(), vmax=data.max())\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.median_filter_rescale","title":"<code>median_filter_rescale(data, size=(3, 3))</code>","text":"<p>Filter and rescale 2D array using median function.</p> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def median_filter_rescale(data: np.ndarray, size=(3, 3)) -&gt; np.ndarray:\n\"\"\"Filter and rescale 2D array using median function.\"\"\"\nreturn rescale(median_filter(data, size=size), vmin=data.min(), vmax=data.max())\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.combine_runtime_results","title":"<code>combine_runtime_results(dir_files, prefix='combined', fsuffix='out')</code>","text":"<p>Combine individual runtime result files into single file.</p> <p>Parameters:</p> <ul> <li> dir_files             (<code>str</code>)         \u2013 <p>File directory.</p> </li> <li> prefix             (<code>str, optional</code>)         \u2013 <p>Filename prefix (default: <code>combined</code>).</p> </li> <li> fsuffix             (<code>str, optional</code>)         \u2013 <p>File suffix of ouput runtime results (default: <code>out</code>).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def combine_runtime_results(dir_files: str, prefix: str = 'combined', fsuffix: str = 'out') -&gt; None:\n\"\"\"\n    Combine individual runtime result files into single file.\n    Parameters\n    ----------\n    dir_files : str\n        File directory.\n    prefix : str, optional\n        Filename prefix (default: `combined`).\n    fsuffix : str, optional\n        File suffix of ouput runtime results (default: `out`).\n    \"\"\"\nfiles = glob.glob(os.path.join(dir_files, f'*.{fsuffix}'))\nwith open(os.path.join(dir_files, f'runtimes_{prefix}.txt'), mode='w', newline='\\n') as fout:\nfor file in files:\nwith open(file, mode='r') as f:\nfout.write(f.read())\n</code></pre>"},{"location":"api/api_cube_POCS_interpolation_3D/#pseudo_3D_interpolation.cube_POCS_interpolation_3D.main","title":"<code>main(argv=sys.argv)</code>","text":"<p>Interpolate sparse 3D cube.</p> Source code in <code>pseudo_3D_interpolation\\cube_POCS_interpolation_3D.py</code> <pre><code>def main(argv=sys.argv):\n\"\"\"Interpolate sparse 3D cube.\"\"\"\nSCRIPT = os.path.basename(__file__)\nTODAY = datetime.date.today().strftime('%Y-%m-%d')\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])  # exclude filename parameter at position 0\nverbose = args.verbose\n# parameter\nxprint(\"Load POCS parameter from config file\", kind=\"info\", verbosity=verbose)\nwith open(args.path_pocs_parameter, mode=\"r\") as f:\ncfg = yaml.safe_load(f)\ncfg['metadata']['transform_kind'] = cfg['metadata']['transform_kind'].upper()\nmetadata = cfg['metadata']\n# input 3D cube\npath_cube = args.path_cube\ndir_work, file = os.path.split(path_cube)\nfilename, suffix = os.path.splitext(file)\n# output folder (for interpolated chunks)\nprefix = f\"{filename}_{metadata['transform_kind']}_{metadata['thresh_op']}_niter-{metadata['niter']}\"\nout_path = (\nargs.path_output_dir if args.path_output_dir is not None else os.path.join(dir_work, prefix)\n)\nif not os.path.isdir(out_path):\nos.mkdir(out_path)\n# open dataset\ndim = cfg['dim']  # 'freq_twt' if 'freq' in file else 'twt'\nchunks = {dim: 1, 'iline': -1, 'xline': -1}  # dim=1 --&gt; **faster** than dim=20!!\ncube = xr.open_dataset(path_cube, chunks=chunks, engine='h5netcdf')\nshape = tuple([v for k, v in cube.dims.items() if k in ['iline', 'xline']])\n# dim = [d for d in list(cube.dims) if d not in ['iline','xline']][0]\nvar = cfg.get('var', [v for v in list(cube.data_vars) if v != 'fold'][0])\n# load fold into memory\ncube['fold'].load()\n# create data mask from fold\ncube['mask'] = cube['fold'].where(\ncube['fold'] &lt;= 1, other=1\n)  # preserve coord attrs by using `xr.DataArray.where` instead of `xr.where`\n# check for complex input (frequency domain)\nCOMPLEX = np.iscomplexobj(cube[var])\n# [POCS] initialize POCS parameter\n# write parameter to disk\nwith open(os.path.join(out_path, f'parameter_{prefix}.yml'), mode='w', newline='\\n') as f:\nyaml.safe_dump(metadata, f)\n# create FFT transform\nif metadata['transform_kind'] == 'FFT':\nmetadata['transform'] = np.fft.fft2  # mkl_fft.fft2, scipy.fft.fft2\nmetadata['itransform'] = np.fft.ifft2  # mkl_fft.ifft2, partial(scipy.fft.ifft2, workers=1)\n# create WAVELET transform\nelif metadata['transform_kind'] == 'WAVELET':\nwavelet = 'coif17'  # db30\nwavelet_mode = 'smooth'\nmetadata['transform'] = partial(pywt.wavedec2, wavelet=wavelet, mode=wavelet_mode)\nmetadata['itransform'] = partial(pywt.waverec2, wavelet=wavelet, mode=wavelet_mode)\nprefix += f'_{wavelet}-{wavelet_mode}'\n# create SHEARLET transform\nelif metadata['transform_kind'] == 'SHEARLET':\nPsi = FFST.scalesShearsAndSpectra(\nshape, numOfScales=None, realCoefficients=True, fftshift_spectra=True\n)\nmetadata['transform'] = FFST.shearletTransformSpect\nmetadata['itransform'] = FFST.inverseShearletTransformSpect\n# create Curvelet transform\nelif metadata['transform_kind'] == 'CURVELET':\nnbangles_coarse = 20  # default: 16\nallcurvelets = True\nDCTOp = curvelops.FDCT2D(\nshape, nbscales=None, nbangles_coarse=nbangles_coarse, allcurvelets=allcurvelets\n)\nmetadata['transform'] = DCTOp.matvec\nmetadata['itransform'] = DCTOp.rmatvec\nprefix += f'_nbangles-{nbangles_coarse}'\nelse:\nraise ValueError(f'Transform &lt; {metadata[\"transform_kind\"]} &gt; is not supported.')\n# [DASK] Setup dask distributed cluster\ncluster_config = dict(\nn_workers=cfg['n_workers'],\nprocesses=cfg['processes'],\nthreads_per_worker=cfg['threads_per_worker'],\nmemory_limit=cfg['memory_limit']\n)\n# [POCS] Interpolation using `dask`\nwith LocalCluster(**cluster_config, silence_logs=50) as cluster, Client(cluster) as client:\n# create slices to process\nindices = list(range(0, cube[dim].size + cfg['batch_chunk'], cfg['batch_chunk']))\nslices = [slice(start, stop) for start, stop in zip(indices[:-1], indices[1:])]\n# apply POCS for each slice\naux = Psi if metadata['transform_kind'] == 'SHEARLET' else None  # only needed for SHEARLETS\ninput_core_dims = (\n[['iline', 'xline'], ['iline', 'xline'], ['iline', 'xline', 'lvl']]\nif metadata['transform_kind'] == 'SHEARLET'\nelse [['iline', 'xline'], ['iline', 'xline'], []]\n)\nds_interp = [\nxr.apply_ufunc(\nPOCS,\ncube.isel({dim: cube_slice})[var],\ncube['mask'],\naux,\ninput_core_dims=input_core_dims,  # [['iline','xline'], ['iline','xline']],\noutput_core_dims=[['iline', 'xline']],  # [['iline','xline']]\nvectorize=True,\ndask='parallelized',\noutput_dtypes=[cube[var].dtype],  # [cube[var].dtype]\nkeep_attrs=True,\nkwargs=(\ndict(\nmetadata,\npath_results=os.path.join(\nout_path, f\"slice-{cube_slice.start:04d}-{cube_slice.stop:04d}.out\"\n),\n)\nif cfg['output_runtime_results']\nelse metadata\n),\n)\n.to_dataset(name=f'{var}_interp')\n.assign(fold=cube.fold)\nfor cube_slice in slices\n]\nif COMPLEX:\nds_interp = [split_complex_variable(ds, var=f'{var}_interp') for ds in ds_interp]\n# add metadata attributes\nexclude_keys = ['transform', 'itransform', 'results_dict', 'path_results']\nfor ds in ds_interp:\nds.attrs = cube.attrs  # copy input global attributes\nds['iline'].attrs = cube['iline'].attrs  # not preserved by xr.apply_ufunc (core dims)\nds['xline'].attrs = cube['xline'].attrs  # not preserved by xr.apply_ufunc (core dims)\nattrs_domain = '(frequency domain)' if 'freq' in dim else '(time domain)'\nds.attrs.update(\n{\n'description': f'Interpolated pseudo-3D cube using {metadata[\"transform_kind\"]} '\n+ 'transform created from TOPAS profiles '\n+ attrs_domain,\n'interp_params_keys': ';'.join([k for k in metadata if k not in exclude_keys]),\n'interp_params_vals': ';'.join(\n[str(metadata[k]) for k in metadata if k not in exclude_keys]\n),\n'history': cube.attrs.get('history', '')\n+ f'{SCRIPT}:{metadata[\"transform_kind\"]} {attrs_domain};',\n'text': cube.attrs.get('text', '')\n+ f'\\n{TODAY}: {metadata[\"transform_kind\"]} {attrs_domain.upper()}'\n}\n)\n# create output file paths\npaths = [\ncreate_file_path(ds, dim=dim, prefix=prefix, root_path=out_path) for ds in ds_interp\n]\nout_batch = [\nds.to_netcdf(path, engine='h5netcdf', compute=False) for ds, path in zip(ds_interp, paths)\n]\npath_report = os.path.join(out_path, f'dask-report_{metadata[\"transform_kind\"]}.html')\nwith dask.config.set({\n'distributed.comm.timeouts.connect': '120s',\n'distributed.worker.memory.target': 'false',\n'distributed.worker.memory.spill': 'false',\n'distributed.worker.memory.pause': 0.85,\n'distributed.nanny.environ.MALLOC_TRIM_THRESHOLD_': 0,\n}), performance_report(filename=path_report):\n# trigger computation\nfutures = client.compute(out_batch)\n# show progress bar\nprogress(futures, notebook=False)\n# [DASK] finalize and close cluster\ncluster.close()\n# [RESULTS] Merge individual runtime results files into single file\nif cfg['output_runtime_results']:\ncombine_runtime_results(out_path, prefix=prefix)\n# [RESULTS] Merge individual netCDF slices into single file\nsuffix_filter = '-smooth' if cfg['apply_filter'] else ''\npath_merged = f'{out_path}{suffix_filter}.nc'\nkwargs_filter = dict(sigma=1)\nxprint('Open multiple files as a single dataset', kind='info', verbosity=verbose)\ncube_fft = xr.open_mfdataset(\nos.path.join(out_path, '*.nc'),\nchunks=chunks,\nengine='h5netcdf',\nparallel=True,\ndata_vars='minimal',\n)\nif cfg['apply_filter']:\nxprint('Apply smoothing filter', kind='info', verbosity=verbose)\ndata_vars = [v for v in list(cube_fft.data_vars) if v != 'fold']\nfor var in data_vars:\n# apply gaussian filter (sigma = 1)\ncube_fft[var] = xr.apply_ufunc(\ngaussian_filter_rescale if cfg['apply_filter'] == 'gauss' else median_filter_rescale,\ncube_fft[var],\ninput_core_dims=[['iline', 'xline']],\noutput_core_dims=[['iline', 'xline']],\nvectorize=True,\ndask='parallelized',\noutput_dtypes=[cube_fft[var].dtype],\nkwargs=kwargs_filter,\nkeep_attrs=True,\n)\ncube_fft.attrs.update({'smoothed': cfg['apply_filter']})\ncube_fft['iline'].attrs = cube['iline'].attrs  # not preserved by xr.apply_ufunc (core dims)\ncube_fft['xline'].attrs = cube['xline'].attrs  # not preserved by xr.apply_ufunc (core dims)\nwith dask.config.set(scheduler='threads'), ProgressBar():\nxprint('Write combinded netCDF file to disk', kind='info', verbosity=verbose)\n# write merged output file\ncube_fft.to_netcdf(path_merged, engine='h5netcdf')  # 1min 60.0s (226 files -&gt; 4501 slices)\n</code></pre>"},{"location":"api/api_cube_apply_FFT/","title":"<code>cube_apply_FFT.py</code>","text":"<p>Utility function to apply forward FFT along specified axis of 3D cube (netCDF).</p>"},{"location":"api/api_cube_apply_FFT/#pseudo_3D_interpolation.cube_apply_FFT.get_freq_filter_win","title":"<code>get_freq_filter_win(filter_freqs, frequencies, dim='freq_twt', filter_type='lowpass')</code>","text":"<p>Calculate filter window (<code>highpass</code>, <code>lowpass</code>, or <code>bandpass</code>) in frequency domain.</p> <p>Parameters:</p> <ul> <li> filter_freqs             (<code>list</code>)         \u2013 <p>Cut-off frequencies as [fmin, fmax] for <code>highpass</code> or <code>lowpass</code> and [f1, f2, f3, f4] for <code>bandpass</code>. Must be same units as <code>frequencies</code> (e.g., kHz).</p> </li> <li> frequencies             (<code>xr.DataArray</code>)         \u2013 <p>DataArray coordinate of frequencies. Must be same units as <code>filter_freqs</code>(e.g., kHz).</p> </li> <li> filter_type             (<code>str, optional</code>)         \u2013 <p>Filter type in frequency domain (default: <code>lowpass</code>)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xr.DataArray</code>         \u2013 <p>Filter window with values in range [0, 1] that can be used for multiplication.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_apply_FFT.py</code> <pre><code>def get_freq_filter_win(\nfilter_freqs: list,\nfrequencies: xr.DataArray,\ndim: str = 'freq_twt',\nfilter_type: str = 'lowpass',\n) -&gt; xr.DataArray:\n\"\"\"\n    Calculate filter window (`highpass`, `lowpass`, or `bandpass`) in frequency domain.\n    Parameters\n    ----------\n    filter_freqs : list\n        Cut-off frequencies as [fmin, fmax] for `highpass` or `lowpass`\n        and [f1, f2, f3, f4] for `bandpass`. Must be same units as `frequencies` (e.g., kHz).\n    frequencies : xr.DataArray\n        DataArray coordinate of frequencies. Must be same units as `filter_freqs`(e.g., kHz).\n    filter_type : str, optional\n        Filter type in frequency domain (default: `lowpass`)\n    Returns\n    -------\n    xr.DataArray\n        Filter window with values in range [0, 1] that can be used for multiplication.\n    \"\"\"\nif filter_type in ['lowpass', 'highpass']:\nfmin = min(filter_freqs)\nfmax = max(filter_freqs)\nconstant_values = _get_const_values(kind=filter_type)\n# get sample numbers for each interval\nn_lower = np.count_nonzero(frequencies &lt; fmin)\nn_stopband = np.count_nonzero((frequencies &gt;= fmin) &amp; (frequencies &lt;= fmax))\nn_higher = np.count_nonzero(frequencies &gt; fmax)\n# print(n_lower, n_stopband, n_higher)\n# create full bandpass stopband\nstopband = _get_stopband(n_stopband, kind=filter_type)\nelif filter_type == 'bandpass':\nfilter_freqs.sort()\nf1, f2, f3, f4 = filter_freqs\nconstant_values = _get_const_values(filter_type)\n# get sample numbers for each interval\nn_lower = np.count_nonzero(frequencies &lt; f1)\nn_stopband_low = np.count_nonzero((frequencies &gt;= f1) &amp; (frequencies &lt;= f2))\nn_stopband = np.count_nonzero((frequencies &gt; f2) &amp; (frequencies &lt; f3))\nn_stopband_high = np.count_nonzero((frequencies &gt;= f3) &amp; (frequencies &lt;= f4))\nn_higher = np.count_nonzero(frequencies &gt; f4)\n# print(n_lower, n_stopband_low, n_stopband, n_stopband_high, n_higher)\n# get stopband for lower freqs\nstopband_low = _get_stopband(n_stopband_low, kind='highpass')\n# print('stopband_low', stopband_low.shape)\n# get stopband for higher freqs\nstopband_high = _get_stopband(n_stopband_high, kind='lowpass')\n# print('stopband_high', stopband_high.shape)\n# create full bandpass stopband\nstopband = np.hstack((stopband_low, np.ones((n_stopband,)), stopband_high))\n# print('stopband', stopband.shape)\n# create full filter window\nfilter_window = np.pad(\nstopband, pad_width=(n_lower, n_higher), mode='constant', constant_values=(constant_values,)\n)\nreturn xr.DataArray(filter_window, dims=[dim], coords={dim: frequencies.data})\n</code></pre>"},{"location":"api/api_cube_apply_FFT/#pseudo_3D_interpolation.cube_apply_FFT.main","title":"<code>main(argv=sys.argv, return_dataset=False)</code>","text":"<p>Apply FFT along time axis wrapper function.</p> Source code in <code>pseudo_3D_interpolation\\cube_apply_FFT.py</code> <pre><code>def main(argv=sys.argv, return_dataset=False):  # noqa\n\"\"\"Apply FFT along _time_ axis wrapper function.\"\"\"\nTODAY = datetime.date.today().strftime('%Y-%m-%d')\nSCRIPT = os.path.splitext(os.path.basename(__file__))[0]\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])  # exclude filename parameter at position 0\npath_cube = args.path_cube\ndir_work, filename = os.path.split(path_cube)\npath_cube_freq = os.path.join(dir_work, filename.replace('twt', f'{args.prefix}'))\nprefix = f'{args.prefix}_'\n# load netCDF metadata\nwith open(args.params_netcdf, 'r') as f_attrs:\nkwargs_nc = yaml.safe_load(f_attrs)\n# (1) Open cube dataset\ncube_time2freq = xr.open_dataset(path_cube, chunks='auto', engine='h5netcdf')\n# get parameter names\ndim = [d for d in list(cube_time2freq.dims) if d not in ['iline', 'xline']][0]\nvar = [v for v in list(cube_time2freq.data_vars) if v not in ['fold', 'amp_ref']][0]\nvar_new = f'{prefix}{var}'  # 'freq'\ncoord_new = f'{prefix}{dim}'\n# rechunk\nchunks = {dim: -1, 'iline': 15, 'xline': -1}\ncube_time2freq = cube_time2freq.chunk(chunks)\nattrs_var = {}\n# (2) Compute FFT along frequency axis (twt, il, xl) --&gt; (freq, il, xl)\nxprint('Compute FFT along time axis', kind='info', verbosity=args.verbose)\ndim_size = cube_time2freq[dim].size\nif dim_size % 2 != 0:\nwarnings.warn(\n(\nf'Selected dim `{dim}` has odd length ({dim_size}), ',\n'which causes issues for inverse FFT. Last slice will be removed!',\n)\n)\ndim_slice = slice(0, dim_size - 1)\nelse:\ndim_slice = slice(None)\n# compute FFT along time axis\n# var_fft = f'{var}_filt' if args.filter is not None else var\ncube_freq = (\nxrft.fft(\ncube_time2freq[var][dim_slice],\ndim=dim,\nreal_dim=dim if args.compute_real else None,\nshift=False,\n# window='hamming',\ntrue_phase=True,\ntrue_amplitude=True,\nprefix=prefix,\nchunks_to_segments=False,\n)\n.astype('complex64')\n.to_dataset(name=var_new)\n)\n# add fold DataArray\ncube_freq = cube_freq.assign(fold=cube_time2freq.fold)\n# (3) apply frequency domain filter\nif args.filter is not None:\nif args.filter_freqs is None:\nraise ValueError('Filter frequencies must be specified!')\nxprint(\nf'Apply &gt; {args.filter} &lt; filter in frequency domain',\nkind='info',\nverbosity=args.verbose,\n)\n# convert filter_freqs if neccessary\nunits = cube_time2freq[dim].attrs.get('units')\ndivisor = 1000 if units == 'ms' else 1\nfilter_freqs = [f / divisor for f in args.filter_freqs]\nfilter_window = get_freq_filter_win(\nfilter_freqs, frequencies=cube_freq[coord_new], dim=coord_new, filter_type=args.filter\n)\ncube_freq[var_new] *= filter_window\n# update metadata\n_filter_freq_str = '/'.join(str(f) for f in args.filter_freqs)\nattrs_var = {'filter': args.filter, 'filter_freq_Hz': _filter_freq_str}\nhistory_filter = f'{args.filter.upper()} ({_filter_freq_str} Hz), '\n# (4) update &amp; assign attributes\ncube_freq.attrs = cube_time2freq.attrs\ncube_freq.attrs.update(\n{\n'long_name': cube_time2freq.attrs[\"long_name\"] + ' (frequency domain)',\n'description': cube_time2freq.attrs[\"description\"] + ' (frequency domain)',\n'history': cube_time2freq.attrs[\"history\"]\n+ f'{SCRIPT}: {history_filter if args.filter is not None else \"\"}FFT({var});',\n'text': cube_time2freq.attrs.get('text', '')\n+ f'\\n{TODAY}: {history_filter[:-2] if args.filter is not None else \"\"} FFT(TIME -&gt; FREQ)'\n}\n)\ncube_freq[var_new].attrs.update({'original_var': var})\nif kwargs_nc is not None:\ncube_freq[var_new].attrs.update(kwargs_nc['attrs_freq'].get('data', {}))  # data attributes\ncube_freq[var_new].attrs.update(attrs_var)  # add filter attributes\ncube_freq[coord_new].attrs.update(\nkwargs_nc['attrs_freq'].get('new_dim', {})\n)  # new dim attributes\n# (5) Write full volume to netCDF (il, xl, freq)\nwith dask.config.set(scheduler='threads'), show_progressbar(\nProgressBar(), verbose=args.verbose\n):\ncube_freq.to_netcdf(path=path_cube_freq, engine='h5netcdf', invalid_netcdf=True)\nif return_dataset:\nreturn cube_freq\n</code></pre>"},{"location":"api/api_cube_apply_IFFT/","title":"<code>cube_apply_IFFT.py</code>","text":"<p>Utility script to apply inverse FFT (IFFT) along specified axis of 3D cube (netCDF).</p>"},{"location":"api/api_cube_apply_IFFT/#pseudo_3D_interpolation.cube_apply_IFFT.main","title":"<code>main(argv=sys.argv, return_dataset=False)</code>","text":"<p>Apply inverse FFT along frequency axis wrapper function.</p> Source code in <code>pseudo_3D_interpolation\\cube_apply_IFFT.py</code> <pre><code>def main(argv=sys.argv, return_dataset=False):  # noqa\n\"\"\"Apply inverse FFT along _frequency_ axis wrapper function.\"\"\"\nxr.set_options(keep_attrs=True)\nTODAY = datetime.date.today().strftime('%Y-%m-%d')\nSCRIPT = os.path.splitext(os.path.basename(__file__))[0]\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])  # exclude filename parameter at position 0\npath_cube_freq_interp = args.path_cube\ndir_work, file = os.path.split(path_cube_freq_interp)\ncompute_real = args.compute_real\n# load netCDF metadata\nwith open(args.params_netcdf, 'r') as f_attrs:\nkwargs_nc = yaml.safe_load(f_attrs)\n# (1) Open cube dataset\ncube_freq2time = xr.open_dataset(\npath_cube_freq_interp,\nchunks='auto',\nengine='h5netcdf',\n)\n# extract dimension for IFFT\ndim = [d for d in list(cube_freq2time.dims) if d not in ['iline', 'xline']][0]\n# extract prefix\nprefix = dim.split('_')[0]\n# use ``original`` variable name if stored as metadata\nvar = [v for v in list(cube_freq2time.data_vars) if prefix in v][0]\nvar = cube_freq2time[var].attrs.get(\n'original_var', f'{\"_\".join(var.split(\".\")[0].split(\"_\")[1:])}'\n)\n# rechunk using detected dimension\nchunks = {dim: -1, 'iline': 20, 'xline': -1}\ncube_freq2time = cube_freq2time.chunk(chunks)\n# restore complex array (from merged float32 netCDF variables)\nvar_names = list(cube_freq2time.data_vars)\nvar_real = [v for v in var_names if 'real' in v]\nvar_imag = [v for v in var_names if 'imag' in v]\nif len(var_real) &gt; 0 and len(var_imag) &gt; 0:\ncube_freq2time[var] = cube_freq2time[var_real[0]] + cube_freq2time[var_imag[0]] * 1j\ncube_freq2time = cube_freq2time.drop_vars((var_real[0], var_imag[0]))\n# (2) Compute IFFT along frequency axis (il, xl, freq) --&gt; (il, xl, twt)\n# compute IFFT along time axis\ncube_time = (\nxrft.ifft(\ncube_freq2time[var],\ndim=dim,\nreal_dim=dim if compute_real else None,\nshift=True,\ntrue_phase=True,\ntrue_amplitude=True,\n)\n.astype('float32')\n.to_dataset(name=var)\n)\n# add fold DataArray\n# cube_time = cube_time.assign(fold=cube_freq2time.fold)\ncube_time['fold'] = cube_freq2time['fold']\n# fix rounding errors..\ncube_time = cube_time.assign_coords(\n{\n'twt': (\n'twt',\ncube_time.coords['twt'].data.astype('float32'),\ncube_time.coords['twt'].attrs,\n)\n}\n)\n# update &amp; assign attributes\ncube_time.attrs = cube_freq2time.attrs\ncube_time.attrs.update(\n{\n'long_name': cube_freq2time.attrs[\"long_name\"].split(' (')[0] + ' (interpolated)',\n'history': cube_freq2time.attrs[\"history\"] + f'{SCRIPT}: IFFT({var});',\n'text': cube_freq2time.attrs.get('text', '') + f'\\n{TODAY}: INVERSE FFT(FREQ -&gt; TIME)'\n}\n)\nif kwargs_nc is not None:\ncube_time[var].attrs.update(kwargs_nc['attrs_time'].get(var.split('_')[0], {}))\ncube_time['twt'].attrs.update(kwargs_nc['attrs_time'].get('twt', {}))\ncube_time['twt'].attrs['dt'] = float(f\"{cube_time['twt'].attrs['spacing']:g}\")\ncube_time['twt'].attrs.pop('spacing')\n# (3) Write full volume (il, xl, twt) to netCDF\npath_cube_time_interp = os.path.join(\ndir_work, file.replace(prefix, 'twt').split('.')[0] + '-interp-freq.nc'\n)\nxprint('Compute inverse FFT along time axis', kind='info', verbosity=args.verbose)\nwith dask.config.set(scheduler='threads'), show_progressbar(\nProgressBar(), verbose=args.verbose\n):\ncube_time.to_netcdf(path=path_cube_time_interp, engine='h5netcdf')\nif return_dataset:\nreturn cube_time\n</code></pre>"},{"location":"api/api_cube_binning_3D/","title":"<code>cube_binning_3D.py</code>","text":"<p>Create (sparse) inline/xline cube from intersecting 2D profiles. Applying a user-defined binning method ('average', 'median', 'nearest', or 'IDW').</p>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.check_sampling_interval","title":"<code>check_sampling_interval(df)</code>","text":"<p>Check and return sampling interval (dt) from datafram of all scraped SEG-Y files.</p> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def check_sampling_interval(df):\n\"\"\"Check and return sampling interval (dt) from datafram of all scraped SEG-Y files.\"\"\"\n# filter dataframe by unique lines\ndf_filt = df.groupby(\"line_id\")[\"TRACE_SAMPLE_INTERVAL\"].min()\ndt_minmax = (df_filt.min(), df_filt.max())\nif np.mean(dt_minmax) != dt_minmax[0]:\nraise ValueError(f\"SEG-Y files with different sampling intervals (dt: {dt_minmax})\")\nreturn float(dt_minmax[0])\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.distance","title":"<code>distance(p1, p2)</code>","text":"<p>Calculate euclidian distance between to points p1 and p2 (row-wise).</p> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def distance(p1, p2):\n\"\"\"Calculate euclidian distance between to points p1 and p2 (row-wise).\"\"\"\np1 = np.asarray(p1)\np2 = np.asarray(p2)\nassert p1.shape == p2.shape, \"Input points must have identical shape!\"\nif p1.ndim == 1:\nreturn np.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)\nelif p1.ndim == 2:\nreturn np.sqrt((p2[:, 0] - p1[:, 0]) ** 2 + (p2[:, 1] - p1[:, 1]) ** 2)\nelse:\nraise NotImplementedError(\"Only 1D or 2D input arrays are supported!\")\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.points_from_extent","title":"<code>points_from_extent(extent)</code>","text":"<p>Return numpy array from extent provided as <code>(w, e, s, n)</code> or <code>(xmin, xmax, ymin, ymax)</code>, respectively.</p> <p>Parameters:</p> <ul> <li> extent             (<code>tuple</code>)         \u2013 <p>Data extent provided as <code>(w, e, s, n)</code> or <code>(xmin, xmax, ymin, ymax)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Corner coordinates of given extent as <code>(lower_left, upper_left, upper_right, lower_right)</code>.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def points_from_extent(extent: tuple):\n\"\"\"\n    Return numpy array from extent provided as `(w, e, s, n)` or `(xmin, xmax, ymin, ymax)`, respectively.\n    Parameters\n    ----------\n    extent : tuple\n        Data extent provided as `(w, e, s, n)` or `(xmin, xmax, ymin, ymax)`.\n    Returns\n    -------\n    np.ndarray\n        Corner coordinates of given extent as `(lower_left, upper_left, upper_right, lower_right)`.\n    \"\"\"\nif not isinstance(extent, (tuple, list, np.ndarray)):\nraise ValueError(\"extent must be either tuple, list or np.array\")\nreturn np.array(\n[\n[extent[0], extent[2]],  # lower left\n[extent[0], extent[3]],  # upper left\n[extent[1], extent[3]],  # upper right\n[extent[1], extent[2]],  # lower right\n]\n)\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.extent_from_points","title":"<code>extent_from_points(points)</code>","text":"<p>Return bounding box extent from points (N, 2).</p> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def extent_from_points(points):\n\"\"\"Return bounding box extent from points (N, 2).\"\"\"\npoints = np.asarray(points)\nif points.shape[1] != 2:\nraise ValueError(\"input points must be array-like of shape (N,2)\")\nreturn (points[:, 0].min(), points[:, 0].max(), points[:, 1].min(), points[:, 1].max())\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_polygon_area","title":"<code>get_polygon_area(pts)</code>","text":"<p>Calculate polygon area (in cartesian coordinates).</p> <p>Parameters:</p> <ul> <li> pts             (<code>np.ndarray</code>)         \u2013 <p>2D input array of X and Y coordinates with shape (n_points, 2).</p> </li> </ul> <p>Returns:</p> <ul> <li> area(            <code>float</code> )        \u2013 <p>Polygon area (in coordinate units).</p> </li> </ul>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_polygon_area--references","title":"References","text":"<ol> <li> <p>Stackoverflow post https://stackoverflow.com/a/66801704 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def get_polygon_area(pts: np.ndarray) -&gt; float:\n\"\"\"\n    Calculate polygon area (in cartesian coordinates).\n    Parameters\n    ----------\n    pts : np.ndarray\n        2D input array of X and Y coordinates with shape (n_points, 2).\n    Returns\n    -------\n    area : float\n        Polygon area (in coordinate units).\n    References\n    ----------\n    [^1]: Stackoverflow post [https://stackoverflow.com/a/66801704](https://stackoverflow.com/a/66801704)\n    \"\"\"\npts = np.vstack((pts, pts[0, :]))\nxs = pts[:, 0]\nys = pts[:, 1]\nreturn 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(np.roll(xs, 1), ys))\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_polygon_centroid","title":"<code>get_polygon_centroid(xy)</code>","text":"<p>Calculate polygon centroid (in cartesian coordinates).</p> <p>Parameters:</p> <ul> <li> xy             (<code>np.ndarray</code>)         \u2013 <p>2D input array of X and Y coordinates with shape (n_points, 2).</p> </li> </ul> <p>Returns:</p> <ul> <li> centroid(            <code>np.ndarray</code> )        \u2013 <p>Centroid coordinate array (x, y).</p> </li> </ul>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_polygon_centroid--references","title":"References","text":"<ol> <li> <p>Stackoverflow post https://stackoverflow.com/a/66801704 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def get_polygon_centroid(xy: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Calculate polygon centroid (in cartesian coordinates).\n    Parameters\n    ----------\n    xy : np.ndarray\n        2D input array of X and Y coordinates with shape (n_points, 2).\n    Returns\n    -------\n    centroid : np.ndarray\n        Centroid coordinate array (x, y).\n    References\n    ----------\n    [^1]: Stackoverflow post [https://stackoverflow.com/a/66801704](https://stackoverflow.com/a/66801704)\n    \"\"\"\nxs = xy[:, 0]\nys = xy[:, 1]\nreturn np.dot(xy.T + np.roll(xy.T, 1, axis=1), xs * np.roll(ys, 1) - np.roll(xs, 1) * ys) / (\n6 * get_polygon_area(xy)\n)\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.affine_transform_coords_to_ilxl","title":"<code>affine_transform_coords_to_ilxl(corner_points=None, extent=None, spacing=None, base_transform=None, inverted=False, original_coords=False, verbose=False)</code>","text":"<p>Create Affine transformation object from (a) <code>corner_points</code> or (b) <code>extent</code> and <code>spacing</code>. If <code>base_transform</code> is provided, it will be used to setup final transform.</p> <p>Parameters:</p> <ul> <li> corner_points             (<code>np.ndarray</code>)         \u2013 <p>2D array of corner points with shape (4,2): Either this variable or <code>extent</code> are required!</p> </li> <li> extent             (<code>tuple</code>)         \u2013 <p>Tuple of extent (xmin, xmax, ymin, ymax). Either this variable or <code>corner_points</code> are required!</p> </li> <li> spacing             (<code>float | tuple(float, float)</code>)         \u2013 <p>Grid bin size (in CRS units). Can be single value or tuple(ysize, xsize), i.e. (iline, xline).</p> </li> <li> base_transform             (<code>Affine, optional</code>)         \u2013 <p>Base transformation used to set up the returned transformation.</p> </li> <li> inverted             (<code>bool, optional</code>)         \u2013 <p><code>base_transform</code> is inverted (default: <code>False</code>).</p> </li> <li> original_coords             (<code>TYPE, optional</code>)         \u2013 <p>Priginal, non-transformed coordinates provided (default: <code>False</code>).</p> </li> <li> verbose             (<code>bool, optional</code>)         \u2013 <p>Print verbose output (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Affine</code>         \u2013 <p>Affine transformation.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def affine_transform_coords_to_ilxl(\ncorner_points: np.ndarray = None,\nextent: tuple = None,\nspacing: float = None,\nbase_transform: Affine = None,\ninverted: bool = False,\noriginal_coords: bool = False,\nverbose: bool = False,\n):\n\"\"\"\n    Create Affine transformation object from (a) `corner_points` or (b) `extent` and `spacing`.\n    If `base_transform` is provided, it will be used to setup final transform.\n    Parameters\n    ----------\n    corner_points : np.ndarray\n        2D array of corner points with shape (4,2):\n        Either this variable or `extent` are required!\n    extent : tuple\n        Tuple of extent (xmin, xmax, ymin, ymax).\n        Either this variable or `corner_points` are required!\n    spacing : float | tuple(float, float)\n        Grid bin size (in CRS units). Can be single value or tuple(ysize, xsize), i.e. (iline, xline).\n    base_transform : Affine, optional\n        Base transformation used to set up the returned transformation.\n    inverted : bool, optional\n        `base_transform` is inverted (default: `False`).\n    original_coords : TYPE, optional\n        Priginal, non-transformed coordinates provided (default: `False`).\n    verbose : bool, optional\n        Print verbose output (default: `False`).\n    Returns\n    -------\n    Affine\n        Affine transformation.\n    \"\"\"\nif (corner_points is None) and (extent is None):\nraise ValueError(\"Either `corner_points` or `extent` must be specified\")\nif spacing is None:\nraise ValueError(\"`spacing` must be specified\")\n# get corner coordinates from corner_points\nif corner_points is not None:\nif (original_coords) and (base_transform is not None) and (not inverted):\nwarnings.warn(\"Untested option - unexpected results possible!\")\ncorner_points = base_transform.transform(corner_points)\nelif (original_coords) and (base_transform is not None) and (inverted):\nwarnings.warn(\"Untested option - unexpected results possible!\")\ncorner_points = base_transform.inverse().transform(corner_points)\nelse:\ncorner_points = corner_points\n# get corner coordinates from extent\nelif extent is not None:\nif (original_coords) and (base_transform is not None) and (not inverted):\nwarnings.warn(\"Untested option - unexpected results possible!\")\ncorner_points = base_transform.transform(points_from_extent(extent))\nelif (original_coords) and (base_transform is not None) and (inverted):\nwarnings.warn(\"Untested option - unexpected results possible!\")\ncorner_points = base_transform.inverse().transform(points_from_extent(extent))\nelse:\ncorner_points = points_from_extent(extent)\nif isinstance(spacing, (tuple, list)):\nyspacing, xspacing = spacing\nelif isinstance(spacing, (int, float)):\nxspacing = yspacing = spacing\n# calc bin center points from corner points and bin spacing(s)\nxprint(\"corner_points:\", corner_points, kind=\"debug\", verbosity=verbose)\ncenter_points = corner_points + np.array(\n[\n[xspacing / 2, yspacing / 2],  # noqa\n[xspacing / 2, -yspacing / 2],  # noqa\n[-xspacing / 2, -yspacing / 2],  # noqa\n[-xspacing / 2, yspacing / 2],  # noqa\n]\n)\nxprint(\"center_points:\", center_points, kind=\"debug\", verbosity=verbose)\n# calc length of extent in x/y directions (in CRS units)\ndist_x = distance(center_points[0], center_points[-1])\ndist_y = distance(center_points[0], center_points[1])\nxprint(f\"distance (x-axis):  {dist_x:.2f}\", kind=\"debug\", verbosity=verbose)\nxprint(f\"distance (y-axis):  {dist_y:.2f}\", kind=\"debug\", verbosity=verbose)\n# create 1D iline/xline arrays from distances\nn_ilines = int(np.around(dist_x / xspacing, 0))  # dist: ll -&gt; lr\nn_xlines = int(np.around(dist_y / yspacing, 0))  # dist: ll -&gt; ul\nxprint(f\"# ilines: {n_ilines}\", kind=\"debug\", verbosity=verbose)\nxprint(f\"# xlines: {n_xlines}\", kind=\"debug\", verbosity=verbose)\n# create affine transform from CRS coordinates to iline/xline\naffine_coords2ilxl = (\nAffine()\n.translation(-center_points[0])\n.scaling(scale=(1.0 / (np.around(dist_x)), 1.0 / (np.around(dist_y))))\n.scaling((n_ilines, n_xlines))\n.translation((1, 1))  # iline/xline always start with 1\n)\nif (base_transform is not None) and (inverted is False):\nreturn affine_coords2ilxl @ base_transform\nelif (base_transform is not None) and (inverted is True):\nreturn affine_coords2ilxl @ base_transform.inverse()\nreturn affine_coords2ilxl\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.round_ilxl_extent","title":"<code>round_ilxl_extent(points)</code>","text":"<p>Round array of cube il/xl indices to appropriate integer indice values.</p> <pre><code>- lower/left indices  --&gt; rounded up\n- upper/right indices --&gt; rounded down\n</code></pre> <p>Parameters:</p> <ul> <li> points             (<code>np.ndarray</code>)         \u2013 <p>Array of il/xl for corner points of cube.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Rounded input array.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def round_ilxl_extent(points):\n\"\"\"\n    Round array of cube il/xl indices to appropriate integer indice values.\n        - lower/left indices  --&gt; rounded up\n        - upper/right indices --&gt; rounded down\n    Parameters\n    ----------\n    points : np.ndarray\n        Array of il/xl for corner points of cube.\n    Returns\n    -------\n    np.ndarray\n        Rounded input array.\n    \"\"\"\noffset = 1e-9\noffset_array = np.array(\n[[offset, offset], [offset, -offset], [-offset, -offset], [-offset, offset]]\n)\nreturn np.around(points + offset_array, 0).astype(\"int\")\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.pad_trace","title":"<code>pad_trace(da, delrt, twt, dt)</code>","text":"<p>Pad seismic trace at top and bottom to fit global <code>twt</code> array.</p> <p>Parameters:</p> <ul> <li> da             (<code>dask.array</code>)         \u2013 <p>Input seismic trace.</p> </li> <li> delrt             (<code>int</code>)         \u2013 <p>Delay recording time of seismic trace (ms).</p> </li> <li> twt             (<code>np.ndarray</code>)         \u2013 <p>Reference array of output two-way traveltimes (TWT).</p> </li> <li> dt             (<code>float</code>)         \u2013 <p>Sampling rate (ms).</p> </li> </ul> <p>Returns:</p> <ul> <li> out(            <code>dask.array</code> )        \u2013 <p>Padded seismic trace.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def pad_trace(da, delrt: int, twt, dt: float):\n\"\"\"\n    Pad seismic trace at top and bottom to fit global `twt` array.\n    Parameters\n    ----------\n    da : dask.array\n        Input seismic trace.\n    delrt : int\n        Delay recording time of seismic trace (ms).\n    twt : np.ndarray\n        Reference array of output two-way traveltimes (TWT).\n    dt : float\n        Sampling rate (ms).\n    Returns\n    -------\n    out : dask.array\n        Padded seismic trace.\n    \"\"\"\n# calculate TWT of last sample in input trace\ntwt_tr_end = delrt + da.size * dt - dt\n# top\nif delrt &lt; twt[0]:  # clip input trace\nntop = int(round((twt[0] - delrt) / dt))\nda = da[ntop:]\nntop = 0\nelse:  # pad input trace\nntop = int((delrt - twt[0]) / dt)\n# bottom\nif twt_tr_end &gt; twt[-1]:  # clip input trace\nnbottom = int(round((twt_tr_end - twt[-1]) / dt)) * -1\nda = da[:nbottom]\nnbottom = 0\nelse:  # pad input trace\nnbottom = int(twt.size - da.size)\nout = dask.array.pad(da, ((ntop, nbottom),), mode=\"constant\", constant_values=0.0)\nassert out.size == twt.size, (ntop, nbottom, out)\nreturn out\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.adjust_extent","title":"<code>adjust_extent(extent, spacing, eps=1e-05)</code>","text":"<p>Adjust extent to fit given spacing by adding to min/max of X and Y coordinates.</p> <p>Parameters:</p> <ul> <li> extent             (<code>tuple</code>)         \u2013 <p>Extent of point data as coordinate tuple <code>(xmin, xmax, ymin, ymax)</code>.</p> </li> <li> spacing             (<code>float | tuple(float, float)</code>)         \u2013 <p>Grid bin size (in CRS units). Can be single value or tuple featuring (xsize, ysize).</p> </li> <li> eps             (<code>float, optional</code>)         \u2013 <p>Tolerance parameter used to determine floating point precision error (default: <code>1e-5</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> extent_adj(            <code>tuple</code> )        \u2013 <p>Adjusted extent of original data extent.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def adjust_extent(extent, spacing, eps=1e-5):\n\"\"\"\n    Adjust extent to fit given spacing by adding to min/max of X and Y coordinates.\n    Parameters\n    ----------\n    extent : tuple\n        Extent of point data as coordinate tuple `(xmin, xmax, ymin, ymax)`.\n    spacing : float | tuple(float, float)\n        Grid bin size (in CRS units). Can be single value or tuple featuring (xsize, ysize).\n    eps : float, optional\n        Tolerance parameter used to determine floating point precision error (default: `1e-5`).\n    Returns\n    -------\n    extent_adj : tuple\n        Adjusted extent of original data extent.\n    \"\"\"\nif isinstance(spacing, (int, float)):\n# round extent to closest coordinate values defined by spacing\nextent_adj = [float(spacing * np.around(e / spacing)) for e in extent]\nfor i, e in enumerate(extent_adj):\nif i % 2 == 0:  # min boundaries\n# only pad value if adjusted extent is excluding data coordinates\nextent_adj[i] = e - spacing if (e &gt; extent[i] and abs(e - extent[i]) &gt; eps) else e\nelif i % 2 == 1:  # max boundaries\n# only pad value if adjusted extent is excluding data coordinates\nextent_adj[i] = e + spacing if (e &lt; extent[i] and abs(e - extent[i]) &gt; eps) else e\nelif isinstance(spacing, tuple) and len(spacing) == 2:\nextent_adj = [float(spacing[0] * np.around(e / spacing[0])) for e in extent[:2]] + [\nfloat(spacing[1] * np.around(e / spacing[1])) for e in extent[2:]\n]\n# only pad value if adjusted extent is excluding data coordinates\nextent_adj[0] = (\nextent_adj[0] - spacing[0]\nif (extent_adj[0] &gt; extent[0] and abs(extent_adj[0] - extent[0]) &gt; eps)\nelse extent_adj[0]\n)\nextent_adj[1] = (\nextent_adj[1] + spacing[0]\nif (extent_adj[1] &lt; extent[1] and abs(extent_adj[1] - extent[1]) &gt; eps)\nelse extent_adj[1]\n)\nextent_adj[2] = (\nextent_adj[2] - spacing[1]\nif (extent_adj[2] &gt; extent[2] and abs(extent_adj[2] - extent[2]) &gt; eps)\nelse extent_adj[2]\n)\nextent_adj[3] = (\nextent_adj[3] + spacing[1]\nif (extent_adj[3] &lt; extent[3] and abs(extent_adj[3] - extent[3]) &gt; eps)\nelse extent_adj[3]\n)\nreturn tuple(extent_adj)\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.transform_and_adjust_extent","title":"<code>transform_and_adjust_extent(extent_pts, spacing, transform)</code>","text":"<p>Transform using Affine <code>transform</code> and adjust cube extent according to bin spacing.</p> <p>Parameters:</p> <ul> <li> extent_pts             (<code>tuple</code>)         \u2013 <p>Data extent as tuple of <code>(w, e, s, n)</code> or <code>(xmin, xmax, ymin, ymax)</code>.</p> </li> <li> spacing             (<code>tuple</code>)         \u2013 <p>Bin spacing (il, xl) or (y, x).</p> </li> <li> transform             (<code>Affine</code>)         \u2013 <p>Affine transformation to create NS-aligned geometry for input corner points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>         \u2013 <p>Transformed (e.g. rotated) and adjusted extent.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def transform_and_adjust_extent(extent_pts: tuple, spacing: tuple, transform: Affine) -&gt; tuple:\n\"\"\"\n    Transform using Affine `transform` and adjust cube extent according to bin spacing.\n    Parameters\n    ----------\n    extent_pts : tuple\n        Data extent as tuple of `(w, e, s, n)` or `(xmin, xmax, ymin, ymax)`.\n    spacing : tuple\n        Bin spacing (il, xl) or (y, x).\n    transform : Affine\n        Affine transformation to create NS-aligned geometry for input corner points.\n    Returns\n    -------\n    tuple\n        Transformed (e.g. rotated) and adjusted extent.\n    \"\"\"\nextent_pts_t = transform.transform(extent_pts)\nextent_rect = extent_from_points(extent_pts_t)\nextent_rect = adjust_extent(extent_rect, spacing=spacing)\nreturn extent_rect\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_cube_parameter","title":"<code>get_cube_parameter(transform_forward, transform_reverse, df_nav, bin_size, cube_corner_pts, region_corner_pts=None, return_geometry=False, verbose=False)</code>","text":"<p>Create cube inline/xline bin indices and associated bin center coordinates. Based on provided forward/inverse coordinate transform objects, bin size (in coordinate units) and given extent of output cube. Inline/xline indices start with 1 if <code>region_corner_pts</code> argument is not provided.</p> <p>Parameters:</p> <ul> <li> transform_forward             (<code>Affine.transform</code>)         \u2013 <p>Forward Affine transform used as base transform for coordinate transformation.</p> </li> <li> transform_reverse             (<code>Affine.transform</code>)         \u2013 <p>Inverse Affine transform.</p> </li> <li> df_nav             (<code>pd.DataFrame</code>)         \u2013 <p>Dataframe with X and Y coordinates for each seismic trace from all SEGY files.</p> </li> <li> bin_size             (<code>float | tuple(float, float)</code>)         \u2013 <p>Size of iline/xline bins (in CRS units, e.g. meter).</p> </li> <li> cube_corner_pts             (<code>np.ndarray</code>)         \u2013 <p>Corner point coordinates of cube (lower_left, upper_left, upper_right, lower_right) with shape (4, 2).</p> </li> <li> region_corner_pts             (<code>np.ndarray, optional</code>)         \u2013 <p>Optional corner point coordinates of region (lower_left, upper_left, upper_right, lower_right) with shape (4, 2). Useful if <code>cube</code> is part of a larger region and bin indices should not start with 1.</p> </li> <li> return_geometry             (<code>bool, optional</code>)         \u2013 <p>Return adjusted geometry for cube_corner_pts (and region_corner_pts if specified)</p> </li> <li> verbose             (<code>bool, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> df_bins(            <code>pd.DataFrame</code> )        \u2013 <p>Dataframe defining output cube with inline 'il' and xline 'xl' indices and bin center coordinates ('x', 'y').</p> </li> <li> df_ilxl(            <code>pd.DataFrame</code> )        \u2013 <p>Dataframe with inline and xline indices for each trace position provided in <code>df_nav</code>.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def get_cube_parameter(\ntransform_forward,\ntransform_reverse,\ndf_nav: pd.DataFrame,\nbin_size: int,\ncube_corner_pts: np.ndarray,\nregion_corner_pts: np.ndarray = None,\nreturn_geometry: bool = False,\nverbose: bool = False,\n):\n\"\"\"\n    Create cube inline/xline bin indices and associated bin center coordinates.\n    Based on provided forward/inverse coordinate transform objects, bin size (in coordinate units)\n    and given extent of output cube.\n    Inline/xline indices start with 1 if `region_corner_pts` argument is not provided.\n    Parameters\n    ----------\n    transform_forward : Affine.transform\n        Forward Affine transform used as base transform for coordinate transformation.\n    transform_reverse : Affine.transform\n        Inverse Affine transform.\n    df_nav : pd.DataFrame\n        Dataframe with X and Y coordinates for each seismic trace from all SEGY files.\n    bin_size : float | tuple(float, float)\n        Size of iline/xline bins (in CRS units, e.g. meter).\n    cube_corner_pts : np.ndarray\n        Corner point coordinates of cube (lower_left, upper_left, upper_right, lower_right)\n        with shape (4, 2).\n    region_corner_pts : np.ndarray, optional\n        Optional corner point coordinates of region (lower_left, upper_left, upper_right, lower_right)\n        with shape (4, 2).\n        Useful if `cube` is part of a larger region and bin indices should not start with 1.\n    return_geometry : bool, optional\n        Return adjusted geometry for cube_corner_pts (and region_corner_pts if specified)\n    verbose : bool, optional\n        Print optional information to console (default: `False`).\n    Returns\n    -------\n    df_bins : pd.DataFrame\n        Dataframe defining output cube with inline 'il' and xline 'xl' indices\n        and bin center coordinates ('x', 'y').\n    df_ilxl : pd.DataFrame\n        Dataframe with inline and xline indices for each trace position provided in `df_nav`.\n    \"\"\"\n# input corner points --&gt; transform &amp; adjust\nextent_cube_t = transform_and_adjust_extent(\ncube_corner_pts, spacing=bin_size, transform=transform_forward\n)\nextent_region_t = (\ntransform_and_adjust_extent(\nregion_corner_pts, spacing=bin_size, transform=transform_forward\n)\nif region_corner_pts is not None\nelse None\n)\n# cube corner points (transformed)\ncube_corner_pts_t = points_from_extent(extent_cube_t)\nregion_corner_pts_t = (\npoints_from_extent(extent_region_t) if region_corner_pts is not None else None\n)\n# affine transformation X/Y to il/xl\ntransform_xy_to_ilxl = affine_transform_coords_to_ilxl(\nextent=extent_region_t if extent_region_t is not None else extent_cube_t,\nspacing=bin_size,\nbase_transform=transform_forward,\n)\n# get max iline/xline numbers from extent\ncube_corner_pts_cnv = transform_reverse.transform(cube_corner_pts_t)\nilxl_extent = transform_xy_to_ilxl.transform(cube_corner_pts_cnv)\nxprint(\"ilxl_extent:\", ilxl_extent, sep=\"\\n\", kind=\"debug\", verbosity=verbose)\nilxl_extent = round_ilxl_extent(ilxl_extent)\nxprint(\"ilxl_extent (rounded):\", ilxl_extent, sep=\"\\n\", kind=\"debug\", verbosity=verbose)\nil_range = (ilxl_extent[0, 0], ilxl_extent[-1, 0])\nxl_range = (ilxl_extent[0, 1], ilxl_extent[1, 1])\nil_indices = np.arange(il_range[0], il_range[-1] + 1, 1)\nxl_indices = np.arange(xl_range[0], xl_range[-1] + 1, 1)\nxprint(f\"# ilines: {il_indices.size:3d}   {il_range}\", kind=\"info\", verbosity=verbose)\nxprint(f\"# xlines: {xl_indices.size:3d}   {xl_range}\", kind=\"info\", verbosity=verbose)\n# compute center coordinates of iline/xline bins\nbins_ilxl = np.meshgrid(il_indices, xl_indices)\nbins_ilxl = np.asarray(bins_ilxl).T.reshape(-1, 2)\nbins_xy = transform_xy_to_ilxl.inverse().transform(bins_ilxl)\n# assign iline/xline numbers to each shotpoint\nilxl = transform_xy_to_ilxl.transform(df_nav[[\"x\", \"y\"]].to_numpy())\ndf_ilxl = pd.DataFrame(data=np.around(ilxl, 0).astype(\"int32\"), columns=[\"il\", \"xl\"])\nwarnings.warn(\n\"\\nCoordinates at the boundary between two ilines/xlines \"\n+ \"are assigned to next SMALLER index (x.5 --&gt; x)!\"\n)\n# create temporary bin center dataframe\ncols_bin = [\"il\", \"xl\", \"x\", \"y\"]\ndtypes_bin = [\"int32\", \"int32\", \"float64\", \"float64\"]\ndf_bins = pd.DataFrame(np.hstack((bins_ilxl, bins_xy)), columns=cols_bin).astype(\ndict(zip(cols_bin, dtypes_bin))\n)\nif return_geometry:\nextent_cube = transform_reverse.transform(cube_corner_pts_t)\nextent_region = (\ntransform_reverse.transform(region_corner_pts_t)\nif region_corner_pts is not None\nelse None\n)\nreturn df_bins, df_ilxl, (extent_cube, extent_cube_t), (extent_region, extent_region_t)\nreturn df_bins, df_ilxl\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_segy_header_dataframe","title":"<code>get_segy_header_dataframe(path, dir_seismic, df_ilxl, df_bins, byte_filter=[1, 5, 9, 71, 73, 77, 109, 115, 117], suffix='sgy', parallel=False, include_files=None, verbose=False)</code>","text":"<p>Scrape SEG-Y headers and return pandas.DataFrame of selected header entries.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013 <p>Path of textfile with filenames of SEGY files to scrape.</p> </li> <li> dir_seismic             (<code>str</code>)         \u2013 <p>Directory of SEGY files.</p> </li> <li> df_ilxl             (<code>pd.DataFrame</code>)         \u2013 <p>Dataframe of iline/xline indices for each trace position (from auxiliary navigation file).</p> </li> <li> df_bins             (<code>pd.DataFrame</code>)         \u2013 <p>Dataframe of cube bins featuring iline/xline indices and bin center coordinates.</p> </li> <li> byte_filter             (<code>list, optional</code>)         \u2013 <p>List of byte indices to scrape.</p> </li> <li> suffix             (<code>str, optional</code>)         \u2013 <p>SEG-Y file suffix (default: <code>sgy</code>).</p> </li> <li> parallel             (<code>bool, optional</code>)         \u2013 <p>Perform operation in parallel using <code>dask.delayed</code> (default: <code>False</code>).</p> </li> <li> include_files             (<code>list, optional</code>)         \u2013 <p>List of SEG-Y files to scrape headers from (default: <code>None</code>). Useful if more SEG-Y files in specified folder/datalist.</p> </li> <li> verbose             (<code>int, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>pandas.DataFrame</code>         \u2013 <p>Dataframe of scraped header information with inline/xline numbers for each trace.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def get_segy_header_dataframe(\npath: str,\ndir_seismic: str,\ndf_ilxl: pd.DataFrame,\ndf_bins: pd.DataFrame,\nbyte_filter: list = [1, 5, 9, 71, 73, 77, 109, 115, 117],\nsuffix: str = \"sgy\",\nparallel: bool = False,\ninclude_files: list = None,\nverbose: int = False,\n):\n\"\"\"\n    Scrape SEG-Y headers and return pandas.DataFrame of selected header entries.\n    Parameters\n    ----------\n    path : str\n        Path of textfile with filenames of SEGY files to scrape.\n    dir_seismic : str\n        Directory of SEGY files.\n    df_ilxl : pd.DataFrame\n        Dataframe of iline/xline indices for each trace position (from auxiliary navigation file).\n    df_bins : pd.DataFrame\n        Dataframe of cube bins featuring iline/xline indices and bin center coordinates.\n    byte_filter : list, optional\n        List of byte indices to scrape.\n    suffix : str, optional\n        SEG-Y file suffix (default: `sgy`).\n    parallel : bool, optional\n        Perform operation in parallel using `dask.delayed` (default: `False`).\n    include_files : list, optional\n        List of SEG-Y files to scrape headers from (default: `None`).\n        Useful if more SEG-Y files in specified folder/datalist.\n    verbose : int, optional\n        Print optional information to console (default: `False`).\n    Returns\n    -------\n    pandas.DataFrame\n        Dataframe of scraped header information with inline/xline numbers for each trace.\n    \"\"\"\n# load list of SEG-Y files to scrape headers\nif path is None:\nfiles_segy = glob.glob(os.path.join(dir_seismic, f\"*.{suffix}\"))\nelse:\nwith open(path, \"r\") as f:\nfiles_segy = f.read().splitlines()\nfiles_segy = sorted(files_segy)\nif include_files is not None:\nfiles_segy = [\nfile for file in files_segy if any(pattern in file for pattern in include_files)\n]\n# scrape essential header information from segys\nxprint(\nf\"Scrape trace headers of &gt; {len(files_segy)} &lt; SEG-Y files\", kind=\"info\", verbosity=verbose\n)\nif parallel:\nsegy_header_scrape_parallel = dask.delayed(segy_header_scrape)\nlist_headers = [\nsegy_header_scrape_parallel(\nos.path.join(dir_seismic, segyf), bytes_filter=byte_filter, silent=True\n)\nfor segyf in files_segy\n]\nwith show_progressbar(ProgressBar(), verbose=verbose):\nlist_headers = dask.compute(*list_headers)  # single-machine scheduler\nelse:\nlist_headers = [\nsegy_header_scrape(\nos.path.join(dir_seismic, segyf), bytes_filter=byte_filter, silent=False\n)\nfor segyf in files_segy\n]\n# merge scraped headers into single Dataframe\ndf_headers = (\npd.concat(\nlist_headers,\nkeys=list(np.arange(len(list_headers))),\n)\n.reset_index()\n.drop(columns=[\"level_1\"])\n.rename(columns={\"level_0\": \"line_id\"})\n)\ndf_headers[\"TRACE_SAMPLE_INTERVAL\"] = df_headers[\"TRACE_SAMPLE_INTERVAL\"] / 1000  # to ms\ndf_headers[\"twt_max\"] = (\ndf_headers[\"DelayRecordingTime\"]\n+ df_headers[\"TRACE_SAMPLE_COUNT\"] * df_headers[\"TRACE_SAMPLE_INTERVAL\"]\n)\ndf_headers[\"x\"] = (\ndf_headers[\"SourceX\"] / df_headers[\"SourceGroupScalar\"].abs()\nif df_headers.loc[0, \"SourceGroupScalar\"] &lt; 0\nelse df_headers[\"SourceX\"] * df_headers[\"SourceGroupScalar\"]\n)\ndf_headers[\"y\"] = (\ndf_headers[\"SourceY\"] / df_headers[\"SourceGroupScalar\"].abs()\nif df_headers.loc[0, \"SourceGroupScalar\"] &lt; 0\nelse df_headers[\"SourceY\"] * df_headers[\"SourceGroupScalar\"]\n)\ndf_headers.drop(columns=[\"SourceGroupScalar\", \"SourceX\", \"SourceY\"], inplace=True)\n# join df with line names\ndf_headers = pd.merge(\ndf_headers,\npd.Series(data=files_segy, name=\"line\"),\nleft_on=\"line_id\",\nright_index=True,\nsort=False,\n)\ndf_headers[\"line\"] = df_headers[\"line\"].astype(\"category\")\n# add iline/xline columns\n# WARNING: use `include_files` when using subset of available SEG-Y files!\ndf_headers = pd.concat((df_headers, df_ilxl), axis=\"columns\", copy=False)\n# merge SEGY headers with bin center coords\n#   how='inner' --&gt; drop all traces outside defined cube extent\ndf_extent = pd.merge(\ndf_headers,\ndf_bins,\nhow=\"inner\",\non=[\"il\", \"xl\"],\n# left_on=ilxl_labels,\n# right_on=['il','xl'],\nsuffixes=[None, \"_bin\"],\nsort=False,\n)\nxprint(\nf\"&gt; {len(df_extent)} &lt; valid out of &gt; {len(df_headers)} &lt; total points) within extent ({len(df_extent)/len(df_headers)*100:.2f} %)\",\nkind='info',\nverbosity=verbose,\n)\nreturn df_extent\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.get_segy_binary_header","title":"<code>get_segy_binary_header(path=None, segy_dir=None, suffix='sgy', byte_filter=None, check_all=False, return_byte_keys=False, parallel=False, verbose=False, **segyio_kwargs)</code>","text":"<p>Scrape binary header fields of SEG-Y file(s).</p> <p>Specify either single datalist (<code>path</code>) or folder with SEG-Y files (<code>segy_dir</code>).</p> <p>Parameters:</p> <ul> <li> path             (<code>str, optional</code>)         \u2013 <p>File path of datalist with SEG-Y file(s). Either <code>path</code> or <code>segy_dir</code> must be specified.</p> </li> <li> segy_dir             (<code>str, optional</code>)         \u2013 <p>Directory of SEG-Y file(s). Either <code>path</code> or <code>segy_dir</code> must be specified.</p> </li> <li> suffix             (<code>str, optional</code>)         \u2013 <p>SEG-Y file suffix (default: <code>sgy</code>).</p> </li> <li> byte_filter             (<code>list, optional</code>)         \u2013 <p>Exclude specified byte locations from check if <code>check_all</code> is True.</p> </li> <li> check_all             (<code>bool, optional</code>)         \u2013 <p>Check binary header of all SEG-Y files (default: <code>False</code>). Raise warning if different values are detected.</p> </li> <li> return_byte_keys             (<code>bool, optional</code>)         \u2013 <p>Return byte locations as keys. The default is a descriptive label.</p> </li> <li> parallel             (<code>bool, optional</code>)         \u2013 <p>Scrape SEG-Y files in parallel using <code>dask.delayed</code> (default: <code>False</code>).</p> </li> <li> verbose             (<code>bool, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> <li> **segyio_kwargs             (<code>dict</code>)         \u2013 <p>Optional kwargs for <code>segyio.open()</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013         <p>If neither <code>path</code> nor <code>segy_dir</code> are specified.</p> </li> </ul> <p>Warns:</p> <ul> <li> <code>UserWarning</code>           \u2013         <p>If different binary headers are found in searched SEG-Y files.</p> </li> </ul> <p>Returns:</p> <ul> <li> list_headers(            <code>dict</code> )        \u2013 <p>Binary header dictionary.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def get_segy_binary_header(\npath: str = None,\nsegy_dir: str = None,\nsuffix: str = \"sgy\",\nbyte_filter: list = None,\ncheck_all: bool = False,\nreturn_byte_keys: bool = False,\nparallel: bool = False,\nverbose: bool = False,\n**segyio_kwargs,\n):\n\"\"\"\n    Scrape binary header fields of SEG-Y file(s).\n    Specify either single datalist (`path`) or folder with SEG-Y files (`segy_dir`).\n    Parameters\n    ----------\n    path : str, optional\n        File path of datalist with SEG-Y file(s). Either `path` or `segy_dir` must be specified.\n    segy_dir : str, optional\n        Directory of SEG-Y file(s). Either `path` or `segy_dir` must be specified.\n    suffix : str, optional\n        SEG-Y file suffix (default: `sgy`).\n    byte_filter : list, optional\n        Exclude specified byte locations from check if `check_all` is True.\n    check_all : bool, optional\n        Check binary header of all SEG-Y files (default: `False`).\n        Raise warning if different values are detected.\n    return_byte_keys : bool, optional\n        Return byte locations as keys. The default is a descriptive label.\n    parallel : bool, optional\n        Scrape SEG-Y files in parallel using `dask.delayed` (default: `False`).\n    verbose : bool, optional\n         Print optional information to console (default: `False`).\n    **segyio_kwargs : dict\n        Optional kwargs for `segyio.open()`.\n    Raises\n    ------\n    ValueError\n        If neither `path` nor `segy_dir` are specified.\n    Warns\n    -----\n    UserWarning\n        If different binary headers are found in searched SEG-Y files.\n    Returns\n    -------\n    list_headers : dict\n        Binary header dictionary.\n    \"\"\"\n# load list of SEG-Y files to scrape headers\nif path:\nwith open(path, \"r\") as f:\nsegy_dir = os.path.dirname(path)\nfiles_segy = f.read().splitlines()\nfiles_segy = sorted([os.path.join(segy_dir, f) for f in files_segy])\nelif segy_dir:\nfiles_segy = glob.glob(os.path.join(segy_dir, f\"*.{suffix}\"))\nelse:\nraise ValueError(\"Either `path` or `segy_dir` must be specified!\")\nsegyio_kwargs[\"strict\"] = False\nif check_all:\nif parallel:\nsegy_bin_scrape_parallel = dask.delayed(segy_bin_scrape)\nlist_headers = [\nsegy_bin_scrape_parallel(segyf, **segyio_kwargs) for segyf in files_segy\n]\nwith show_progressbar(ProgressBar(), verbose=verbose):\nlist_headers = dask.compute(*list_headers)  # single-machine scheduler\nelse:\nlist_headers = [segy_bin_scrape(segyf, **segyio_kwargs) for segyf in files_segy]\nelse:\nlist_headers = [segy_bin_scrape(files_segy[0], **segyio_kwargs)]\nif return_byte_keys:\nlist_headers = [\n{b: v for b, (k, v) in zip(segyio.binfield.keys.values(), list_headers[i].items())}\nfor i in range(len(list_headers))\n]\nif check_all:\nfrom functools import reduce\nsegyio_binfield_values = dict([(value, key) for key, value in segyio.binfield.keys.items()])\nif byte_filter is None:\nbyte_filter = [\nsegyio.BinField.Samples\nif return_byte_keys\nelse segyio_binfield_values[segyio.BinField.Samples],\n]\nelse:\nbyte_filter = [\nbyte if return_byte_keys else segyio_binfield_values[byte] for byte in byte_filter\n]\n# filter using `byte_filter`\nlist_headers = [\n{k: v for k, v in bheader.items() if k not in byte_filter} for bheader in list_headers\n]\n# searching for differences in binary headers\ndiff = [r for r in reduce(set.intersection, (set(h.items()) for h in list_headers))]\ndiff = [k for k in list_headers[0].keys() if k not in list(dict(diff).keys())]\nif len(diff) &gt; 0:\nwarnings.warn(\n(\n\"Found different values in binary headers of SEG-Y files:\"\nf\"\\n&gt; {', '.join(diff)} &lt;\"\n),\nUserWarning,\n)\nreturn list_headers[0]\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.open_seismic_netcdfs","title":"<code>open_seismic_netcdfs(dir_seismic, suffix='seisnc', datalist_path=None, include_files=None, kwargs_seisnc=None, parallel=True, verbose=False)</code>","text":"<p>Open multiple seismic netCDF files and return list of SEGY datasets.</p> <p>Parameters:</p> <ul> <li> dir_seismic             (<code>str</code>)         \u2013 <p>Directory of seismic files.</p> </li> <li> suffix             (<code>str, optional</code>)         \u2013 <p>File suffix (default: <code>seisnc</code>).</p> </li> <li> datalist_path             (<code>str, optional</code>)         \u2013 <p>Filter available seismic files based on list of SEGY files to use.</p> </li> <li> include_files             (<code>list, optional</code>)         \u2013 <p>List of SEG-Y files to scrape headers from (default: <code>None</code>). Useful if more SEG-Y files in specified folder/datalist.</p> </li> <li> kwargs_seisnc             (<code>dict, optional</code>)         \u2013 <p>Keyword arguments for segysak.open_seisnc (default: <code>None</code>).</p> </li> <li> parallel             (<code>bool, optional</code>)         \u2013 <p>Open netCDF files in parallel (default: <code>True</code>).</p> </li> <li> verbose             (<code>bool, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> datasets(            <code>list</code> )        \u2013 <p>List of SEGY datasets.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def open_seismic_netcdfs(\ndir_seismic: str,\nsuffix: str = \"seisnc\",\ndatalist_path: str = None,\ninclude_files: list = None,\nkwargs_seisnc: dict = None,\nparallel: bool = True,\nverbose: bool = False,\n):\n\"\"\"\n    Open multiple seismic netCDF files and return list of SEGY datasets.\n    Parameters\n    ----------\n    dir_seismic : str\n        Directory of seismic files.\n    suffix : str, optional\n        File suffix (default: `seisnc`).\n    datalist_path : str, optional\n        Filter available seismic files based on list of SEGY files to use.\n    include_files : list, optional\n        List of SEG-Y files to scrape headers from (default: `None`).\n        Useful if more SEG-Y files in specified folder/datalist.\n    kwargs_seisnc : dict, optional\n        Keyword arguments for segysak.open_seisnc (default: `None`).\n    parallel : bool, optional\n        Open netCDF files in parallel (default: `True`).\n    verbose : bool, optional\n        Print optional information to console (default: `False`).\n    Returns\n    -------\n    datasets : list\n        List of SEGY datasets.\n    \"\"\"\nif kwargs_seisnc is None or kwargs_seisnc == {}:\nkwargs_seisnc = dict(chunks={\"cdp\": 1})\n# list of all seismic files\nfiles_seisnc = sorted(glob.glob(dir_seismic + f\"/*.{suffix}\"))\nif len(files_seisnc) == 0:\nraise IOError(f'No input files (*.{suffix}) found in \"{dir_seismic}\"')\nif datalist_path is not None and os.path.isfile(datalist_path):\n# filter using datalist suffix\ndatalist_suffix = os.path.splitext(os.path.basename(datalist_path))[0].split(\"_\")[-1]\n# files_seisnc = [f for f in files_seisnc if datalist_suffix in f]\nfiles_seisnc = [\nf\nfor f in files_seisnc\nif datalist_suffix == os.path.splitext(os.path.basename(f))[0].split(\"_\")[-1]\n]\n# filter by lines within cube extent\nwith open(datalist_path, \"r\") as f:\nfiles_filter = f.read().splitlines()\nfiles_filter = sorted(files_filter)\nfiles_seisnc = [\nf\nfor f in files_seisnc\nif any(os.path.splitext(os.path.basename(f))[0] in fu for fu in files_filter)\n]\nxprint(\nf\"Found &gt; {len(files_seisnc)} &lt; input files (*.{suffix})\", kind=\"info\", verbosity=verbose\n)\nif include_files is not None:\nfiles_seisnc = [f for f in files_seisnc if any(x in f for x in include_files)]\nxprint(\nf\"Inclusion filter returned &gt; {len(files_seisnc)} &lt; input files\",\nkind=\"info\",\nverbosity=verbose,\n)\nif parallel:\nopen_seisnc_parallel = dask.delayed(open_seisnc)\ndatasets = [open_seisnc_parallel(f, **kwargs_seisnc) for f in files_seisnc]\nwith show_progressbar(ProgressBar(), verbose=verbose):\ndatasets = dask.compute(*datasets)  # single-machine scheduler\nelse:\ndatasets = [open_seisnc(f, **kwargs_seisnc) for f in files_seisnc]\nreturn datasets\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.inlines_from_seismic","title":"<code>inlines_from_seismic(df, df_bins, datasets, out_dir, bin_size, bin_size_str, stacking_method='average', factor_dist=1.0, twt_minmax=None, encode=False, is_envelope=None, dtype_data=None, ilxl_labels=['il', 'xl'], kwargs_nc=None, verbose=False)</code>","text":"<p>Create inline netCDF files from seismic profiles (*.seisnc). Using auxiliary information from SEGY header scrape and one of the following stacking methods:</p> <ul> <li><code>average</code></li> <li><code>median</code></li> <li><code>nearest</code></li> <li><code>IDW</code> (Inverse Distance Weighting)</li> </ul> <p>Parameters:</p> <ul> <li> df             (<code>pd.DataFrame</code>)         \u2013 <p>Dataframe of scraped SEGY header information and assigned iline/xline bin indices.</p> </li> <li> df_bins             (<code>pd.DataFrame</code>)         \u2013 <p>Dataframe of cube iline/xline indices and bin center coordinates.</p> </li> <li> datasets             (<code>iterable[tuple, list]</code>)         \u2013 <p>Iterable of xr.Datasets of opened *.seisnc files on disk.</p> </li> <li> out_dir             (<code>str</code>)         \u2013 <p>Output directory.</p> </li> <li> bin_size             (<code>float | tuple(float, float)</code>)         \u2013 <p>Bin size in inline/xline direction (in CRS units, e.g. meter).</p> </li> <li> bin_size_str             (<code>str</code>)         \u2013 <p>String representation of <code>bin_size</code> (suitable for filenames).</p> </li> <li> stacking_method             (<code>str, optional</code>)         \u2013 <p>Method to stack traces within single bin (default: <code>average</code>): ['average', 'median', 'nearest', 'IDW']</p> </li> <li> factor_dist             (<code>float, optional</code>)         \u2013 <p>User-specified factor controlling the severity of weighting function: 1/(dist**factor). Larger values (&gt;1) increase influence of closer while reducing impact of more distant traces and vis versa for smaller values (&lt;1).</p> </li> <li> twt_minmax             (<code>tuple</code>)         \u2013 <p>Minimum and maximum TWT (in ms): (min, max)</p> </li> <li> encode             (<code>bool, optional</code>)         \u2013 <p>Use encoding to compress output file size (default: <code>False</code>).</p> </li> <li> is_envelope             (<code>bool, optional</code>)         \u2013 <p>Boolean variable defining if input traces are provided as envelope (of amplitude).</p> </li> <li> dtype_data             (<code>str, optional</code>)         \u2013 <p>Output data array dtype. Use input datasets dtype as default.</p> </li> <li> ilxl_labels             (<code>list</code>)         \u2013 <p>List of iline/xline column names in <code>df</code>.</p> </li> <li> kwargs_nc             (<code>dict</code>)         \u2013 <p>Dictionary of netCDF attributes and parameters (from YAML file).</p> </li> <li> verbose             (<code>int, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def inlines_from_seismic(\ndf: pd.DataFrame,\ndf_bins: pd.DataFrame,\ndatasets: list,\nout_dir: str,\nbin_size: int,\nbin_size_str: str,\nstacking_method: str = \"average\",\nfactor_dist: float = 1.0,\ntwt_minmax: tuple = None,\nencode: bool = False,\nis_envelope: bool = None,\ndtype_data: str = None,\nilxl_labels: list = [\"il\", \"xl\"],\nkwargs_nc: dict = None,\nverbose: int = False,\n) -&gt; None:\n\"\"\"\n    Create inline netCDF files from seismic profiles (*.seisnc).\n    Using auxiliary information from SEGY header scrape and one of the following stacking methods:\n      - `average`\n      - `median`\n      - `nearest`\n      - `IDW` (_Inverse Distance Weighting_)\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe of scraped SEGY header information and assigned iline/xline bin indices.\n    df_bins : pd.DataFrame\n        Dataframe of cube iline/xline indices and bin center coordinates.\n    datasets : iterable[tuple, list]\n        Iterable of xr.Datasets of opened *.seisnc files on disk.\n    out_dir : str\n        Output directory.\n    bin_size : float | tuple(float, float)\n        Bin size in inline/xline direction (in CRS units, e.g. meter).\n    bin_size_str : str\n        String representation of `bin_size` (suitable for filenames).\n    stacking_method : str, optional\n        Method to stack traces within single bin (default: `average`):\n            ['average', 'median', 'nearest', 'IDW']\n    factor_dist : float, optional\n        User-specified factor controlling the severity of weighting function: 1/(dist**factor).\n        Larger values (&gt;1) increase influence of closer while reducing impact of more distant traces\n        and vis versa for smaller values (&lt;1).\n    twt_minmax : tuple\n        Minimum and maximum TWT (in ms): (min, max)\n    encode : bool, optional\n        Use encoding to compress output file size (default: `False`).\n    is_envelope : bool, optional\n        Boolean variable defining if input traces are provided as envelope (of amplitude).\n    dtype_data : str, optional\n        Output data array dtype. Use input datasets dtype as default.\n    ilxl_labels : list\n        List of iline/xline column names in `df`.\n    kwargs_nc : dict\n        Dictionary of netCDF attributes and parameters (from YAML file).\n    verbose : int, optional\n        Print optional information to console (default: `False`).\n    \"\"\"\ndef _norm_weights_per_bin(df_subset, factor_dist, col=\"dist_bin_center\"):\n\"\"\"Compute normalized weights from distance to bin center (per bin).\"\"\"\nweights = 1 / df_subset[col] ** factor_dist\ndf_subset[\"weights_norm\"] = weights / weights.sum()\nreturn df_subset\n# init stacking methods\nfold_cnts = None\nif stacking_method in [\"nearest\", \"IDW\"]:\n# calculate distance of traces to assigned bin center\ncol_dist = \"dist_bin_center\"\ndf[col_dist] = distance(df[[\"x\", \"y\"]].to_numpy(), df[[\"x_bin\", \"y_bin\"]].to_numpy())\nif stacking_method == \"IDW\":\ndf = df.groupby(by=ilxl_labels).apply(\n_norm_weights_per_bin, factor_dist=factor_dist, col=col_dist\n)\nelif stacking_method == \"nearest\":\nfold_cnts = df.groupby(by=ilxl_labels)[col_dist].count()\ndf = df.loc[df.groupby(by=ilxl_labels)[col_dist].idxmin()]\nelif stacking_method == \"average\":\nfunc = dask.array.mean\nelif stacking_method == \"median\":\nfunc = dask.array.median\n# create list of ilines to combine\ndf = df.sort_values(ilxl_labels[0])\nidx_split = np.nonzero(np.diff(df[ilxl_labels[0]]))[0] + 1\ninlines = {il[\"il\"].iloc[0]: il.sort_values(ilxl_labels[1]) for il in np.split(df, idx_split)}\n# sample rate\ndt = df.loc[df.index.min(), \"TRACE_SAMPLE_INTERVAL\"]\nil_indices = df_bins[\"il\"].unique()  # iline indices from dataframe\nxl_indices = df_bins[\"xl\"].unique()  # xline indices from dataframe\n# define global TWT range (i.e. number of samples/trace)\nif twt_minmax is not None:\ntwt_start, twt_end = twt_minmax\nelse:\ntwt_start = df[\"DelayRecordingTime\"].max()\ntwt_end = df[\"twt_max\"].min()\nnsamples = int((twt_end - twt_start) / dt)\nshape = (xl_indices.size, nsamples)  # (nxlines, nsamples)\nxprint(\nf\"Using data between &gt; {twt_start} &lt; and &gt; {twt_end} &lt; ms TWT ({nsamples} samples @ {dt} ms sampling rate)\",\nkind=\"info\",\nverbosity=verbose,\n)\n# create global TWT array (defines cube time axis)\ntwt = np.around(np.arange(twt_start, twt_end, dt, dtype=np.float64), 5)\n# check for amplitude envelopes\nif is_envelope is None:\nperc_ = datasets[0].attrs.get(\"percentiles\", None)\nis_envelope = perc_.min() &gt;= 0.0 if perc_ is not None else False\nif is_envelope:\nvar_name = \"env\"\nelse:\nvar_name = \"amp\"\n# get attributes\nif kwargs_nc is not None:\nATTRIBUTES = kwargs_nc[\"attrs_time\"]\nattrs_data = ATTRIBUTES.get(var_name, {})\nencoding = kwargs_nc[\"encodings\"].get(var_name, {})\nelse:\nattrs_data = {}\nencoding = {}\n# create output directory if not existent\nif not os.path.isdir(out_dir):\nos.mkdir(out_dir)\n# output data array dtype\nif dtype_data is None:\ndtype_data = datasets[0].data.dtype\nelse:\ndtype_data = np.dtype(dtype_data)\n# get textual header from first SEG-Y dataset\ntextual_header = datasets[0].attrs.get(\"text\", None)\nif textual_header is not None:\ntextual_header = \"\\n\".join(\n[\nline[4:]\nfor line in textual_header.split(\"\\n\")\nif re.search(r\"^\\d{4}-\\d{2}-\\d{2}:\", line[4:])  # noqa\n]  # search for time string\n)\nxprint(f\"Creating &gt; {len(il_indices)} &lt; inline netCDF files\", kind=\"info\", verbosity=verbose)\n# loop to create individual inlines of 3D cube\nfor id_iline in tqdm(\nil_indices,\ndesc=\"Creating inlines\",\nncols=80,\ntotal=len(il_indices),\nunit_scale=True,\nunit=\" files\",\n):\n# get inline ID\n# id_iline = int(inline[ilxl_labels[0]].iloc[0])\n# get inline df fro\ninline = inlines.get(id_iline, None)\n# initialize fold array\nfold = xr.DataArray(\nname=\"fold\",\ndata=dask.array.zeros((shape[0],), dtype=np.uint8),\ncoords={\"xline\": (\"xline\", xl_indices)},\nattrs=ATTRIBUTES.get(\"fold\", {}) if kwargs_nc is not None else {},\n)\nif inline is not None:\n# ----- TRACES -----\n# list of all traces in inline (lazy dask.arrays)\ntraces = [\ndatasets[li][\"data\"].sel(cdp=cdp).data\nfor li, cdp in inline[[\"line_id\", \"TRACE_SEQUENCE_FILE\"]].to_numpy()\n]\n# ----- PADDING -----\n# pad traces to fit number of samples (global)\ntraces_pad = [\npad_trace(t, delrt, twt, dt)\nfor t, delrt in zip(traces, inline.DelayRecordingTime.to_numpy())\n]\n# ----- STACKING -----\n# get xline indices as list\nxl_list = inline[ilxl_labels[1]].to_list()\n# get unique xline indices\nxl_uniq, xl_idx, xl_cnt = np.unique(xl_list, return_index=True, return_counts=True)\n# `stack` (sum) traces with same xline index\nif stacking_method in [\"average\", \"median\"]:\ntraces_stk = [\ntraces_pad[j]\nif xl_cnt[i] == 1\nelse func(dask.array.stack(traces_pad[j : j + xl_cnt[i]]), axis=0)\nfor i, j in enumerate(xl_idx)\n]\nelif stacking_method == \"nearest\":\ntraces_stk = [traces_pad[j] for i, j in enumerate(xl_idx)]\nelif stacking_method == \"IDW\":\ntraces_stk = [\ntraces_pad[j]\nif xl_cnt[i] == 1\nelse (\ndask.array.stack(traces_pad[j : j + xl_cnt[i]])\n* inline[\"weights_norm\"][j : j + xl_cnt[i]].values[..., np.newaxis]\n).sum(axis=0)\nfor i, j in enumerate(xl_idx)\n]\n# assign `fold` numbers\nfold.loc[xl_uniq] = xl_cnt if fold_cnts is None else fold_cnts.loc[id_iline].values\n# ----- CREATE INFILL TRACES -----\n# get indices of infill traces\nmask = np.isin(xl_indices, xl_uniq, assume_unique=True)\nxl_indices_gaps = xl_indices[~mask]\n# init infill traces\ntraces_gaps = [\ndask.array.zeros((twt.size,), chunks=(twt.size,), dtype=dtype_data)\nfor n in range(xl_indices_gaps.size)\n]\n# combine and sort traces in correct order\nsorter = np.argsort(np.concatenate([xl_uniq, xl_indices_gaps]))\ntraces_comb = traces_stk + traces_gaps\ntraces_comb = [traces_comb[i] for i in sorter]\niline_dask_stk = dask.array.stack(traces_comb).astype(dtype_data)\nelse:\niline_dask_stk = dask.array.zeros(shape, dtype=dtype_data)\n# create inline dataset\nfile_name = f\"iline_{id_iline:04d}_{bin_size_str}\"\n_stacking_method = (\nf\"{stacking_method} (distance factor={factor_dist})\"\nif stacking_method == \"IDW\"\nelse stacking_method\n)\n# mask of iline xy coordinates\n# mask_il = np.nonzero(bins_ilxl[:,0] == id_iline)[0]\ninline_bins = df_bins[df_bins[\"il\"] == id_iline]\ncrs = pyproj.CRS(kwargs_nc.get(\"spatial_ref\"))\nattrs = {\n\"long_name\": f\"Inline #{id_iline:d}\",\n\"bin_units\": \"m\",\n\"measurement_system\": 'm' if crs.is_projected else 'deg',\n\"epsg\": crs.to_epsg(),\n\"stacking_method\": _stacking_method,\n\"spatial_ref\": kwargs_nc[\"spatial_ref\"] if kwargs_nc is not None else \"None\",\n\"text\": textual_header if textual_header is not None else \"\",\n}\nif isinstance(bin_size, (int, float)):\nattrs[\"bin_size\"] = bin_size\ndil = dxl = bin_size\nelse:\n# bin size along iline == distance between xlines\nattrs[\"bin_size_iline\"] = dxl = bin_size[0]\n# bin size along xline == distance between ilines\nattrs[\"bin_size_xline\"] = dil = bin_size[1]\nds = xr.Dataset(\ndata_vars={var_name: ((\"xline\", \"twt\"), iline_dask_stk, attrs_data), \"fold\": fold},\ncoords={\n\"xline\": (\"xline\", xl_indices),\n\"twt\": (\"twt\", twt),\n\"iline\": id_iline,\n\"x\": (\"xline\", inline_bins[\"x\"].values),  # bins_xy[mask_il, 0]),\n\"y\": (\"xline\", inline_bins[\"y\"].values),  # bins_xy[mask_il, 1]),\n},\nattrs=attrs,\n)\n# add attributes to coordinates\nif kwargs_nc is not None:\n# update attributes\nATTRIBUTES.get(\"twt\", {}).update({\"dt\": dt})\nATTRIBUTES.get(\"iline\", {}).update(\n{\n\"diline\": dil,\n\"comment\": \"diline is the distance along dim `iline`\",\n}\n)\nATTRIBUTES.get(\"xline\", {}).update(\n{\n\"dxline\": dxl,\n\"comment\": \"dxline is the distance along dim `xline`\",\n}\n)\nfor coord in [\"iline\", \"xline\", \"twt\", \"x\", \"y\"]:\nds[coord].attrs.update(ATTRIBUTES.get(coord, {}))\nwith dask.config.set(scheduler=\"threads\"):\n# ~2x speedup when \"pre-compute\" array before export\nds.load().to_netcdf(\nos.path.join(out_dir, f\"{file_name}.nc\"),\nengine=\"h5netcdf\",\nencoding={var_name: encoding} if encode else None,\n)\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.merge_inlines_to_cube","title":"<code>merge_inlines_to_cube(inlines_dir, cube_path, kwargs_mfdataset=None, kwargs_nc=None, verbose=False)</code>","text":"<p>Merge individual inline netCDF files into single 3D cube netCDF.</p> <p>Parameters:</p> <ul> <li> inlines_dir             (<code>str</code>)         \u2013 <p>Directory of inline netCDF files.</p> </li> <li> cube_path             (<code>str</code>)         \u2013 <p>Path of cube file on disk.</p> </li> <li> kwargs_mfdataset             (<code>dict, optional</code>)         \u2013 <p>Keyword arguments for xarry.open_kwargs_mfdataset (default: <code>None</code>).</p> </li> <li> kwargs_nc             (<code>dict</code>)         \u2013 <p>Dictionary of netCDF attributes and parameters (from JSON file).</p> </li> <li> verbose             (<code>int, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> cube(            <code>xr.Dataset</code> )        \u2013 <p>Merged dataset from individual inline files.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def merge_inlines_to_cube(\ninlines_dir,\ncube_path,\nkwargs_mfdataset: dict = None,\nkwargs_nc: dict = None,\nverbose: int = False,\n) -&gt; None:\n\"\"\"\n    Merge individual inline netCDF files into single 3D cube netCDF.\n    Parameters\n    ----------\n    inlines_dir : str\n        Directory of inline netCDF files.\n    cube_path : str\n        Path of cube file on disk.\n    kwargs_mfdataset : dict, optional\n        Keyword arguments for xarry.open_kwargs_mfdataset (default: `None`).\n    kwargs_nc : dict\n        Dictionary of netCDF attributes and parameters (from JSON file).\n    verbose : int, optional\n        Print optional information to console (default: `False`).\n    Returns\n    -------\n    cube : xr.Dataset\n        Merged dataset from individual inline files.\n    \"\"\"\nif kwargs_mfdataset is None or kwargs_mfdataset == {}:\nkwargs_mfdataset = dict(\nchunks={\"iline\": 1, \"xline\": -1, \"twt\": 1000},\ncombine=\"nested\",\nconcat_dim=\"iline\",\nengine=\"h5netcdf\",\nparallel=True,\ncoords=\"all\",\n)\n# open all iline_*.nc netCDFs as one combined dataset\ncube = xr.open_mfdataset(inlines_dir, **kwargs_mfdataset)\n# compute fold mask\ncube[\"fold\"] = cube[\"fold\"].load()\nif verbose:\n_fold = np.count_nonzero(cube[\"fold\"].data)\n_bins = np.prod(cube[\"fold\"].shape)\nxprint(\nf\"Bin fold: &gt; {_fold/_bins*100:.2f}% &lt; (&gt;{_fold}&lt; out of &gt;{_bins}&lt; bins)\",\nkind=\"info\",\nverbosity=verbose,\n)\n# add global attributes\nif kwargs_nc is not None:\ncube.attrs.update(kwargs_nc[\"attrs_time\"].get(\"cube\", {}))\ncube[\"fold\"].attrs.update(\n{\"coverage_perc\": round(np.count_nonzero(cube[\"fold\"].values) / cube[\"fold\"].size * 100, 2)}\n)\n# export combined lines to single netCDF\nxprint(\nf\"Combine &gt; {cube.iline.size} &lt; merged inlines to single netCDF file\",\nkind=\"info\",\nverbosity=verbose,\n)\nwith show_progressbar(ProgressBar(), verbose=verbose):\ncube.to_netcdf(cube_path, engine=kwargs_mfdataset[\"engine\"])\nreturn cube\n</code></pre>"},{"location":"api/api_cube_binning_3D/#pseudo_3D_interpolation.cube_binning_3D.transpose_slice_major","title":"<code>transpose_slice_major(cube_path, cube_twt_path, kwargs_dataset=None, verbose=False)</code>","text":"<p>Transpose cube file (inline, xline, twt) to time-major layout (twt, inline, xline). Practical for faster time/frequency slice access.</p> <p>Parameters:</p> <ul> <li> cube_path             (<code>str</code>)         \u2013 <p>Path of cube file on disk.</p> </li> <li> cube_twt_path             (<code>str</code>)         \u2013 <p>Output file path of transposed cube.</p> </li> <li> kwargs_dataset             (<code>dict, optional</code>)         \u2013 <p>Keyword arguments for xarry.open_dataset (default: <code>None</code>).</p> </li> <li> verbose             (<code>int, optional</code>)         \u2013 <p>Print optional information to console (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> cube_file(            <code>xr.Dataset</code> )        \u2013 <p>Dataset of sparse 3D cube on disk.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\cube_binning_3D.py</code> <pre><code>def transpose_slice_major(\ncube_path, cube_twt_path, kwargs_dataset: dict = None, verbose: int = False\n) -&gt; None:\n\"\"\"\n    Transpose cube file (inline, xline, twt) to time-major layout (twt, inline, xline).\n    Practical for faster time/frequency slice access.\n    Parameters\n    ----------\n    cube_path : str\n        Path of cube file on disk.\n    cube_twt_path : str\n        Output file path of transposed cube.\n    kwargs_dataset : dict, optional\n        Keyword arguments for xarry.open_dataset (default: `None`).\n    verbose : int, optional\n        Print optional information to console (default: `False`).\n    Returns\n    -------\n    cube_file : xr.Dataset\n        Dataset of sparse 3D cube on disk.\n    \"\"\"\nif kwargs_dataset is None or kwargs_dataset == {}:\nkwargs_dataset = dict(chunks={\"iline\": -1, \"xline\": -1, \"twt\": 500}, engine=\"h5netcdf\")\n# open cube\ncube_file = xr.open_dataset(cube_path, **kwargs_dataset)\n# save transposed cube\nxprint(\n\"[INFO]    Transpose sparse 3D cube to time-major layout (twt, inline, xline)\",\nkind=\"info\",\nverbosity=verbose,\n)\nwith show_progressbar(ProgressBar(), verbose=verbose):\ncube_file.transpose(\"twt\", ...).to_netcdf(cube_twt_path, engine=kwargs_dataset[\"engine\"])\nreturn cube_file\n</code></pre>"},{"location":"api/api_cube_cnv_netcdf2segy_3D/","title":"<code>cube_cnv_netcdf2segy_3D.py</code>","text":"<p>Utility script to convert 3D cube from netCDF to SEG-Y format.</p>"},{"location":"api/api_cube_cnv_netcdf2segy_3D/#pseudo_3D_interpolation.cube_cnv_netcdf2segy_3D.segy_writer_custom","title":"<code>segy_writer_custom(seisnc, segyfile, trace_header_map=None, il_chunks=None, dimension=None, silent=False, use_text=False)</code>","text":"<p>Custom implementation of <code>segysak.segy.segy_writer</code> to change some hardcoded defaults.</p> <p>Parameters:</p> <ul> <li> seisnc             (<code>xr.Dataset</code>)         \u2013 <p>Input datset to write.</p> </li> <li> segyfile             (<code>str</code>)         \u2013 <p>Output SEG-Y file path.</p> </li> <li> trace_header_map             (<code>dict, optional</code>)         \u2013 <p>A dictionary of seisnc variables and byte locations. The variable will be written to the trace headers in the assigned byte location (default: None). By default: <code>CMP=23</code>, <code>cdp_x=181</code>, <code>cdp_y=185</code>, <code>iline=189</code>, <code>xline=193</code>.</p> </li> <li> il_chunks             (<code>int, optional</code>)         \u2013 <p>The size of data to work on - if you have memory limitations (default: <code>10</code>). This is primarily used for large 3D and ignored for 2D data.</p> </li> <li> dimension             (<code>str, optional</code>)         \u2013 <p>Data dimension to output, defaults to <code>twt</code> or <code>depth</code> whichever is present.</p> </li> <li> silent             (<code>bool, optional</code>)         \u2013 <p>Turn off progress reporting (defaults: <code>False</code>).</p> </li> <li> use_text             (<code>bool, optional</code>)         \u2013 <p>Use the seisnc text for the EBCIDC output. This text usally comes from the loaded SEG-Y file and may not match the segysak SEG-Y output (default: <code>False</code>).</p> </li> </ul>"},{"location":"api/api_cube_cnv_netcdf2segy_3D/#pseudo_3D_interpolation.cube_cnv_netcdf2segy_3D.segy_writer_custom--references","title":"References","text":"<ol> <li> <p>SEGY-SAK, https://segysak.readthedocs.io/en/latest/generated/segysak.segy.segy_writer.html \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\cube_cnv_netcdf2segy_3D.py</code> <pre><code>def segy_writer_custom(\nseisnc,\nsegyfile,\ntrace_header_map=None,\nil_chunks=None,\ndimension=None,\nsilent=False,\nuse_text=False,\n):  # noqa\n\"\"\"\n    Custom implementation of `segysak.segy.segy_writer` to change some hardcoded defaults.\n    Parameters\n    ----------\n    seisnc : xr.Dataset\n        Input datset to write.\n    segyfile : str\n        Output SEG-Y file path.\n    trace_header_map : dict, optional\n        A dictionary of seisnc variables and byte locations. The variable will be written\n        to the trace headers in the assigned byte location (default: None).\n        By default: `CMP=23`, `cdp_x=181`, `cdp_y=185`, `iline=189`, `xline=193`.\n    il_chunks : int, optional\n        The size of data to work on - if you have memory limitations (default: `10`).\n        This is primarily used for large 3D and ignored for 2D data.\n    dimension : str, optional\n        Data dimension to output, defaults to `twt` or `depth` whichever is present.\n    silent : bool, optional\n        Turn off progress reporting (defaults: `False`).\n    use_text : bool, optional\n        Use the seisnc text for the EBCIDC output. This text usally comes from\n        the loaded SEG-Y file and may not match the segysak SEG-Y output (default: `False`).\n    References\n    ----------\n    [^1]: SEGY-SAK, [https://segysak.readthedocs.io/en/latest/generated/segysak.segy.segy_writer.html](https://segysak.readthedocs.io/en/latest/generated/segysak.segy.segy_writer.html)\n    \"\"\"\nif trace_header_map:\ncheck_tracefield(trace_header_map.values())\ndimension = _check_dimension(seisnc, dimension)\n# ensure there is a coord_scalar\ncoord_scalar = seisnc.coord_scalar\nif coord_scalar is None:\ncoord_scalar = 0\nseisnc.attrs[\"coord_scalar\"] = coord_scalar\ncoord_scalar_mult = np.power(abs(coord_scalar), np.sign(coord_scalar) * -1)\nseisnc.attrs[\"coord_scalar_mult\"] = coord_scalar_mult\n# create empty trace header map if necessary\nif trace_header_map is None:\ntrace_header_map = dict()\nelse:  # check that values of thm in seisnc\nfor key in trace_header_map:\ntry:\n_ = seisnc[key]\nexcept KeyError:\nraise ValueError(\"keys of trace_header_map must be in seisnc\")\n# transfrom text if requested\nif use_text:\nif isinstance(seisnc.text, dict):  # allow for parsing custom dict\ntext = _clean_texthead(seisnc.text, 80)\nelse:\ntext = {i + 1: line for i, line in enumerate(seisnc.text.split(\"\\n\"))}\ntext = _clean_texthead(text, 80)\ncommon_kwargs = dict(silent=silent, dimension=dimension, text=text)\nif seisnc.seis.is_2d():\nthm = output_byte_locs(\"standard_2d\")\nthm.unfreeze()\nthm.update(trace_header_map)\n_ncdf2segy_2d(seisnc, segyfile, **common_kwargs, **thm)\nelif seisnc.seis.is_2dgath():\nthm = output_byte_locs(\"standard_2d_gath\")\nthm.unfreeze()\nthm.update(trace_header_map)\n_ncdf2segy_2d_gath(seisnc, segyfile, **common_kwargs, **thm)\nelif seisnc.seis.is_3d():\nthm = output_byte_locs(\"standard_3d\")\nthm.unfreeze()\nthm.update(trace_header_map)\n_ncdf2segy_3d(\nseisnc,\nsegyfile,\n**common_kwargs,\nil_chunks=il_chunks,\n**thm,\n)\nelif seisnc.seis.is_3dgath():\nthm = output_byte_locs(\"standard_3d_gath\")\nthm.unfreeze()\nthm.update(trace_header_map)\n_ncdf2segy_3dgath(\nseisnc,\nsegyfile,\n**common_kwargs,\nil_chunks=il_chunks,\n**thm,\n)\nelse:\n# cannot determine type of data writing a continuous traces\nraise NotImplementedError()\n</code></pre>"},{"location":"api/api_cube_cnv_netcdf2segy_3D/#pseudo_3D_interpolation.cube_cnv_netcdf2segy_3D.main","title":"<code>main(argv=sys.argv)</code>","text":"<p>Convert <code>netCDF</code>cube to <code>SEG_Y</code> format.</p> Source code in <code>pseudo_3D_interpolation\\cube_cnv_netcdf2segy_3D.py</code> <pre><code>def main(argv=sys.argv):  # noqa\n\"\"\"Convert `netCDF`cube to `SEG_Y` format.\"\"\"\nTIMESTAMP = datetime.datetime.now().isoformat(timespec=\"seconds\")\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])\npath_cube = args.path_cube\npath_segy = args.path_segy if args.path_segy is not None else path_cube.replace('.nc', '_DUMMY.sgy')\npath_segy = path_cube.replace('.nc', '_DUMMY.sgy')\nparams_netcdf = args.params_netcdf\nverbose = args.verbose\n# Load netCDF metadata\nif params_netcdf is not None:\nwith open(params_netcdf, 'r') as f_attrs:  # args.params_netcdf\nkwargs_nc = yaml.safe_load(f_attrs)\nNO_DATA_VARS = kwargs_nc.get('var_aux', ['fold', 'ref_amp'])\nelse:\nkwargs_nc = None\nNO_DATA_VARS = ['fold', 'ref_amp']\n# open netCDF file\nxprint('Open 3D cube', kind='info', verbosity=verbose)\nchunks = {'twt' : -1, 'iline' : 15, 'xline' : -1}\ncube = xr.open_dataset(path_cube, chunks=chunks, engine='h5netcdf')\nvar = [v for v in list(cube.data_vars) if v != NO_DATA_VARS][0]\n# dims = list(cube.dims.keys())\n# Load coordinates\ncube.coords['x'] = cube.coords['x'].compute()\ncube.coords['y'] = cube.coords['y'].compute()\n# Setup coordinates\ncoord_scalar, coord_scalar_mult = check_coordinate_scalar(\nargs.scalar_coords, xcoords=cube.coords['x'].values, ycoords=cube.coords['y'].values)\n# Add missing attributes\ncube.attrs['sample_rate'] = cube['twt'].attrs.get(\n'dt',\nint(float(cube['twt'].diff(dim='twt').mean()) * 1000) / 1000\n)\ncrs = pyproj.CRS(cube.attrs.get('spatial_ref'))\nif cube.attrs.get('measurement_system') is None:\ncube.attrs['measurement_system'] = 'm' if crs.is_projected else 'deg'\nif cube.attrs.get('epsg') is None:\ncube.attrs['epsg'] = crs.to_epsg() if crs is not None else None\ncube.attrs['coord_scalar'] = coord_scalar\ncube.attrs['coord_scalar_mult'] = coord_scalar_mult\ncube.attrs['source_file'] = os.path.basename(path_cube)\n# rename for segysak\ncube = cube.rename({\nvar: 'data',\n'x': 'cdp_x',\n'y': 'cdp_y',\n})\n# change dtype of fold array\nshape = cube['cdp_x'].shape\nncdps = np.prod(shape)\ncube['cdp'] = (('iline', 'xline'), np.arange(1, ncdps + 1).reshape(shape, order='C'))\ncube = cube.set_coords(['fold', 'cdp'])\n# specify output byte locations for trace header values\ntrace_header_map = {\n'cdp': segyio.TraceField.CDP,\n'fold': segyio.TraceField.NStackedTraces,\n'cdp_x': segyio.TraceField.CDP_X,\n'cdp_y': segyio.TraceField.CDP_Y,\n'iline': segyio.TraceField.INLINE_3D,\n'xline': segyio.TraceField.CROSSLINE_3D,\n}\n# Textual header\n# set trace header defaults\ntextual_header = {\n1: '3D SEG-Y CONVERTED FROM NETCDF USING SEGYSAK',\n3: f'CREATION: {TIMESTAMP}',\n4: f'EVOKER: {os.getlogin()}',\n10: '*** PROCESSING STEPS ***',\n35: '*** BYTE LOCATION OF KEY HEADERS ***',\n36: f'CDP: {trace_header_map.get(\"cdp\",\"\")}  FOLD: {trace_header_map.get(\"fold\",\"\")}',\n37: f'CDP UTM-X: {trace_header_map.get(\"cdp_x\",\"\")} CDP UTM-Y: {trace_header_map.get(\"cdp_x\",\"\")} '\n+ f'ALL COORDS SCALED BY: {coord_scalar_mult}',\n38: f'INLINE: {trace_header_map.get(\"iline\",\"\")}, XLINE: {trace_header_map.get(\"xline\",\"\")}',\n40: \"END TEXTUAL HEADER\",\n}\n# add processing steps\n# cube.attrs['text'] = cube.attrs['text'] + '\\n=== 3D PROCESSING ===' + f\"\\n{TODAY}: 3D BINNING {stacking_method} ILINE:{bin_size_iline:g} \" + f\"XLINE:{bin_size_xline} UNIT: METER\" + f'\\n{_text[:-1]}'  + f'\\n{TODAY}: LOWPASS (5000/5200 Hz) ' + 'FFT(TIME -&gt; FREQ)'  + f'\\n{TODAY}: FFT (FREQUENCY DOMAIN)' + f'\\n{TODAY}: INVERSE FFT(FREQ -&gt; TIME)'\nproc_steps = cube.attrs['text'].split('\\n')\ntextual_header.update(dict(zip(range(11, 11 + len(proc_steps)), proc_steps)))\ntextual_header_str = ''\nfor k in range(1, 41):\nline = textual_header.get(k)\nif line is None:\ntextual_header_str += f'C{k:02d} ' + ' ' * 75 + '\\n'\nelse:\ntextual_header_str += f'C{k:02d} {line[:75]:&lt;75}\\n'\ncube.attrs['text'] = textual_header_str[:-2]  # remove trailing line break\n# Write cube to SEG-Y\n# UserWarning from SEGYSAK due to incorrect sanity check in `segysak.segy._segy_text.put_segy_texthead`\n# -&gt; raise issue on GitHub\nsegy_writer_custom(\ncube,\npath_segy,\ntrace_header_map=trace_header_map,\nil_chunks=chunks.get('iline'),\ndimension='twt',\nsilent=False,\nuse_text=True,\n)\n# update binary header\ndto = cube['twt'].attrs.get('dt_original', None)\nwith segyio.open(path_segy, 'r+') as segyf:\nprint(segyf.bin)\nsegyf.bin.update(\ndto=int(dto * 1000) if dto is not None else 0,\ntsort=segyio.TraceSortingFormat.INLINE_SORTING,\n)\n</code></pre>"},{"location":"api/api_cube_preprocessing_3D/","title":"<code>cube_preprocessing_3D.py</code>","text":"<p>Utility script to apply various processing functions to 3D cube. These include:</p> <ul> <li>amplitude gain (time-variant)</li> <li>amplitude balancing (time-invariant)</li> <li>frequency filtering (bandpass, lowpass, highpass)</li> <li>resampling (up, down)</li> <li>trace envelope calculation</li> </ul>"},{"location":"api/api_cube_preprocessing_3D/#pseudo_3D_interpolation.cube_preprocessing_3D.main","title":"<code>main(argv=sys.argv, return_dataset=False)</code>","text":"<p>Preprocess 3D cube wrapper function.</p> Source code in <code>pseudo_3D_interpolation\\cube_preprocessing_3D.py</code> <pre><code>def main(argv=sys.argv, return_dataset=False):  # noqa\n\"\"\"Preprocess 3D cube wrapper function.\"\"\"\nTODAY = datetime.date.today().strftime('%Y-%m-%d')\nSCRIPT = os.path.splitext(os.path.basename(__file__))[0]\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])  # exclude filename parameter at position 0\npath_cube = args.path_cube\ndir_work, filename = os.path.split(path_cube)\nbasename, suffix = os.path.splitext(filename)\npath_cube_proc = (\nargs.path_out\nif args.path_out is not None\nelse os.path.join(dir_work, f'{basename}_preproc.nc')\n)\n# sanity checks\nif args.resampling_interval is not None:\nxprint('Using `resampling_interval` for resampling', kind='info', verbosity=args.verbose)\nresampling_interval = args.resampling_interval\nresampling_factor = None\nelif args.resampling_frequency is not None:\nxprint('Using `resampling_frequency` for resampling', kind='info', verbosity=args.verbose)\nresampling_interval = (1 / args.resampling_frequency) * 1000\nresampling_factor = None\nelif args.resampling_factor is not None:\nxprint('Using `resampling_factor` for resampling', kind='info', verbosity=args.verbose)\nresampling_interval = None\nresampling_factor = args.resampling_factor\n# (0) Open cube dataset\ncube = xr.open_dataset(path_cube, chunks='auto', engine='h5netcdf')\n# get parameter names\ndim = [d for d in list(cube.dims) if d not in ['iline', 'xline']][0]\nvar = [v for v in list(cube.data_vars) if v != 'fold'][0]\nvar_ref = f'{var}_ref'\n# rechunk\nchunks = {dim: -1, 'iline': 15, 'xline': -1}\ncube = cube.chunk(chunks)\ntwt_s = (\ncube[var].twt.values / 1000\nif cube.twt.attrs.get('units', 'ms') == 'ms'\nelse cube[var].twt.values\n)\ndt = cube[dim].attrs.get('dt', np.median(np.diff(cube[dim].data)))  # sampling rate (ms)\nn_samples = cube[dim].size\n# init output cube\ncube_proc = cube.copy()\n# load netCDF metadata\nif args.params_netcdf is not None:\nwith open(args.params_netcdf, 'r') as f_attrs:\nkwargs_nc = yaml.safe_load(f_attrs)\nelse:\nkwargs_nc = None\nvar_new = 'env' if args.envelope else var\nattrs_var = kwargs_nc['attrs_time'][var_new] if kwargs_nc is not None else cube[var].attrs\n_history = f'{SCRIPT}:'\n_text = f'{TODAY}: '\n# (1) balance traces (time-invariant)\nif args.balance is not None:\nbalance = args.balance\nxprint(f'Balance traces using &lt; {balance} &gt; amplitude for scaling (time-invariant)',\nkind='info', verbosity=args.verbose)\n# compute reference amplitude per trace\nkwargs_func = dict(scale=balance)\nref_amplitudes = cube[var].reduce(\ncalc_reference_amplitude, dim='twt', keep_attrs=True, **kwargs_func\n)\ncube_proc[var] = (cube[var] / ref_amplitudes)\n# update metadata\nattrs_var.update({'balanced': f'{balance} amplitude'})\n_history += f' amplitude balancing ({balance}),'\n_text += 'BALANCE.'\nif args.store_ref_amp:\ncube_proc[var_ref] = ref_amplitudes\ncube_proc[var_ref].attrs = {\n'description': 'Reference amplitudes used to scale traces',\n'method': f'{balance} scaling',\n'units': cube[var].attrs.get('units', '-'),\n}\n# (2) apply time-variant gain function\nif args.gain is not None:\nxprint(f'Apply time-variant gain function ({\",\".join(args.gain)})',\nkind='info', verbosity=args.verbose)\nkwargs_gain = args.gain\nkwargs_gain = dict([(p.split('=')[0], float(p.split('=')[1])) for p in kwargs_gain])\nkwargs_gain['twt'] = np.arange(twt_s.size) if args.use_samples else twt_s\n# apply gain function(s)\ncube_proc[var] = xr.apply_ufunc(\ngain,\ncube_proc[var],\ninput_core_dims=(['twt', 'xline'], ),\noutput_core_dims=(['twt', 'xline'], ),\nkeep_attrs=True,\nkwargs=kwargs_gain,\ndask='parallelized',\noutput_dtypes=cube_proc[var].dtype,\n)\n# update metadata\nkwargs_gain_str = ' '.join([\nf'{key}={val}' for key, val in kwargs_gain.items() if key != 'twt'\n])\nkwargs_gain_str += ' (samples-based)' if args.use_samples else ' (TWT-based)'\nattrs_var.update({'gain': kwargs_gain_str})\n_history += f' amplitude gain ({kwargs_gain_str}),'\n_text += 'GAIN.'\n# (3) FILTER (FREQUENCY)\nif args.filter is not None:\nif args.filter_freqs is None:\nraise ValueError('Filter frequencies must be specified!')\nelse:\nfilter_freqs_str = [str(f) for f in args.filter_freqs]\nkwargs_filter = dict(\nfreqs=args.filter_freqs,  # cutoff frequencies (f_low, f_high)\nfs=1 / (dt / 1000),       # sampling frequency (Hz)\naxis=-1,                  # time axis\n)\nxprint(f'Apply &lt; {args.filter} &gt; filter ({\"/\".join(filter_freqs_str)} Hz)',\nkind='info', verbosity=args.verbose)\ncube_proc[var] = xr.apply_ufunc(\nFILTER.get(args.filter),\ncube_proc[var],\ninput_core_dims=(['xline', dim], ),  # time axis has to be last\noutput_core_dims=(['xline', dim], ),\nkeep_attrs=True,\nkwargs=kwargs_filter,\ndask='parallelized',\noutput_dtypes=cube_proc[var].dtype,\n)\n# update metadata\n_filter_freq_str = '/'.join(str(f) for f in args.filter_freqs)\nattrs_var.update({'filter': args.filter,\n'filter_freq_Hz': _filter_freq_str})\n_history += f' {args.filter} ({_filter_freq_str} Hz),'\n_text += f'{args.filter.upper()} ({_filter_freq_str} Hz).'\n# (4) RESAMPLING\nif any([a is not None for a in [\nargs.resampling_interval, args.resampling_frequency, args.resampling_factor\n]]):\n# setup resampling parameter\nif resampling_interval is not None:\nresampling_factor = resampling_interval / dt\nelif resampling_factor is not None:\nresampling_interval = resampling_factor * dt\nn_resamples = int(np.ceil(n_samples / resampling_factor))\n# setup resampling function\nif args.resampling_function == 'resample':\nresample_func = resample\nkwargs_resample = dict(num=n_resamples, t=None, window=args.window_resample, axis=-1)\nelif args.resampling_function == 'resample_poly':\nup = 1 / resampling_factor if resampling_factor &lt; 1 else 1\ndown = resampling_factor if resampling_factor &gt; 1 else 1\nresample_func = resample_poly\nkwargs_resample = dict(up=up, down=down, window=args.window_resample, axis=-1)\nxprint(f'Resample &lt; {dim} &gt; from &lt; {dt} &gt; to &lt; {resampling_interval} &gt; ms',\nkind='info', verbosity=args.verbose)\n# init resampled cube\ncube_resampled = cube_proc.drop_vars(var)\ncube_resampled = cube_resampled.assign_coords(\n{dim: get_resampled_twt(cube_proc[dim].data, n_resamples, n_samples)}\n)  # assign resampled dimension\ncube_resampled[dim].attrs = cube_proc[dim].attrs  # copy metadata\ncube_resampled[dim].attrs.update(dt=resampling_interval)\n# resample traces\ncube_resampled[var] = xr.apply_ufunc(\nresample_func,\ncube_proc[var],\ninput_core_dims=[['xline', dim], ],  # time axis has to be last\noutput_core_dims=[['xline', dim], ],\nexclude_dims=set((dim,)),            # allow dimension to change size\nkeep_attrs=True,\nkwargs=kwargs_resample,\ndask='parallelized',\ndask_gufunc_kwargs={\n'output_sizes': {dim: n_resamples, 'xline': cube_proc['xline'].size}\n},\noutput_dtypes=cube_proc[var].dtype,\n)\ncube_resampled[dim] = np.around(cube_resampled[dim].astype('float64'), 3)\n# update metadata\ncube_resampled[dim].attrs.update({'resampled': 'True',\n'dt_original': dt})\n_history += f' resampling (factor: {resampling_factor}),'\n_text += 'RESAMPLE.'\n# update output filename\npath_cube_proc = re.sub(\nr'\\d{1,2}\\+\\d{0,3}(ms)',  # noqa\nffloat(resampling_interval).replace('.', '+') + 'ms',\npath_cube_proc\n)\ncube_out = cube_resampled\nelse:\ncube_out = cube_proc\n# (5) calculate trace envelope\nif args.envelope:\nxprint('Calculate trace envelope (instantaneous amplitude)', kind='info', verbosity=args.verbose)\ncube_out[var] = xr.apply_ufunc(\nenvelope,\ncube_out[var],\ninput_core_dims=[['xline', dim], ],\noutput_core_dims=[['xline', dim], ],\nkeep_attrs=True,\ndask='parallelized',\noutput_dtypes=cube_out[var].dtype,\n)\ncube_out = cube_out.rename_vars({var: var_new})  # update variable name in Dataset\n# update history\n_history += ' trace envelope,'\n_text += 'ENV.'\n# update output filename\npath_cube_proc = path_cube_proc.replace(var, var_new)\n# (6) add/update metadata\nxprint('Update netCDF metadata attributes', kind='info', verbosity=args.verbose)\ncube_out.attrs.update({\n'history': cube_out.attrs[\"history\"] + f'{_history[:-1]};',  # remove trailing comma\n'text': cube_out.attrs.get(\"text\") + f'\\n{_text[:-1]}',  # remove trailing period\n})\ncube_out[var_new].attrs = attrs_var\n# (6) write balanced cube to disk\nxprint('Write output data to file', kind='info', verbosity=args.verbose)\nwith dask.config.set(scheduler='threads'), show_progressbar(ProgressBar(), verbose=args.verbose):\ncube_out.transpose(dim, 'iline', 'xline').to_netcdf(\npath=path_cube_proc,\nengine='h5netcdf',\n)\nif return_dataset:\nreturn cube_out.transpose(dim, 'iline', 'xline'), cube\n</code></pre>"},{"location":"api/api_delrt_correction_segy/","title":"<code>delrt_correction_segy.py</code>","text":"<p>Utility script to fix incorrect DelayRecordingTime in SEG-Y file(s). It compares maximum amplitudes of neighboring traces in user-defined windows (e.g., 3 ms x 5 traces).</p>"},{"location":"api/api_delrt_correction_segy/#pseudo_3D_interpolation.delrt_correction_segy.check_varying_DelayRecordingTimes","title":"<code>check_varying_DelayRecordingTimes(path, byte_delay=109)</code>","text":"<p>Check SEG-Y file for varying DelayRecordingTimes and return boolean type.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013 <p>File path of SEG-Y file to check.</p> </li> <li> byte_delay             (<code>int, optional</code>)         \u2013 <p>Byte position of DelayRecordingTime (default: 109).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\delrt_correction_segy.py</code> <pre><code>def check_varying_DelayRecordingTimes(path, byte_delay=109):\n\"\"\"\n    Check SEG-Y file for varying DelayRecordingTimes and return boolean type.\n    Parameters\n    ----------\n    path : str\n        File path of SEG-Y file to check.\n    byte_delay : int, optional\n        Byte position of DelayRecordingTime (default: 109).\n    \"\"\"\nwith segyio.open(path, 'r', strict=False, ignore_geometry=True) as file:\n# get \"DelayRecordingTime\" for each trace\ndelrt = file.attributes(byte_delay)[:]\n# find unique DelayRecordingTimes and corresponding trace index\ndelrt_uniq, delrt_idx = np.unique(delrt, return_index=True)\nif len(delrt_uniq) &gt; 1:\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"api/api_delrt_correction_segy/#pseudo_3D_interpolation.delrt_correction_segy.correct_single_trace_DelayRecordingTime","title":"<code>correct_single_trace_DelayRecordingTime(idx, data, delrt, fldr, n_traces=5, n_samples=120, verbosity=0)</code>","text":"<p>Correct false trace DelayRecordingTime (delrt). Comparing data maximum amplitude to the max amplitude within a <code>n_samples</code> sample window of <code>n_traces</code> neighboring traces to each side.</p> <p>Parameters:</p> <ul> <li> idx             (<code>int</code>)         \u2013 <p>Index of change in DelayRecordingTimes.</p> </li> <li> data             (<code>numpy.ndarray</code>)         \u2013 <p>2D data array subset (samples x traces).</p> </li> <li> delrt             (<code>numpy.ndarray</code>)         \u2013 <p>1D array subset of DelayRecordingTimes.</p> </li> <li> fldr             (<code>numpy.ndarray</code>)         \u2013 <p>1D array subset of FieldRecord numbers.</p> </li> <li> n_traces             (<code>int, optional</code>)         \u2013 <p>Number of neighboring traces of reference trace (default: 5).</p> </li> <li> n_samples             (<code>int, optional</code>)         \u2013 <p>Number of samples per trace for comparision of max. amplitude (default: 120).</p> </li> </ul> <p>Returns:</p> <ul> <li> ref_tr_delrt_corrected(            <code>int</code> )        \u2013 <p>Correct DelayRecordingTime of reference trace.</p> </li> <li> idx_n_traces(            <code>int</code> )        \u2013 <p>Index of (updated) reference trace (only for visualization plots).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\delrt_correction_segy.py</code> <pre><code>def correct_single_trace_DelayRecordingTime(\nidx, data, delrt, fldr, n_traces=5, n_samples=120, verbosity=0\n):\n\"\"\"\n    Correct false trace DelayRecordingTime (delrt).\n    Comparing data maximum amplitude to the max amplitude\n    within a `n_samples` sample window of `n_traces` neighboring traces to each side.\n    Parameters\n    ----------\n    idx : int\n        Index of change in DelayRecordingTimes.\n    data : numpy.ndarray\n        2D data array subset (samples x traces).\n    delrt : numpy.ndarray\n        1D array subset of DelayRecordingTimes.\n    fldr : numpy.ndarray\n        1D array subset of FieldRecord numbers.\n    n_traces : int, optional\n        Number of neighboring traces of reference trace (default: 5).\n    n_samples : int, optional\n        Number of samples per trace for comparision of max. amplitude (default: 120).\n    Returns\n    -------\n    ref_tr_delrt_corrected : int\n        Correct DelayRecordingTime of reference trace.\n    idx_n_traces : int\n        Index of (updated) reference trace (only for visualization plots).\n    \"\"\"\n# select only reference trace\nref_tr = data[:, n_traces]\n# find max amplitude value and correspondig array index\nref_tr_peak_val = ref_tr.max()\nref_tr_peak_idx = ref_tr.argmax()\nxprint(\nf'ref_tr_peak_idx:  {ref_tr_peak_idx} --&gt; {ref_tr_peak_val}',\nkind='debug',\nverbosity=verbosity,\n)\n# get max amplitudes from data subset (per trace)\nref_tr_peak_idx_min = (\nref_tr_peak_idx - n_samples // 2 if ref_tr_peak_idx - n_samples // 2 &gt;= 0 else 0\n)  # account for negative indices\nref_tr_peak_idx_max = ref_tr_peak_idx + n_samples // 2 + 1\ntr_amp_maxima = data[ref_tr_peak_idx_min:ref_tr_peak_idx_max].max(axis=0)\ntr_amp_max_idx_trace = tr_amp_maxima[n_traces]\ntr_amp_maxima[\ntr_amp_maxima &gt; tr_amp_max_idx_trace\n] = tr_amp_max_idx_trace  # set all values &gt; max amplitude of reference trace\nxprint(\nf'tr_amp_maxima:          {np.around(tr_amp_maxima,4)}', kind='debug', verbosity=verbosity\n)\n# difference amplitude to reference trace (absolut)\ntr_amp_max_diff_abs = np.abs(tr_amp_maxima - ref_tr_peak_val)\nxprint(\nf'tr_amp_max_diff_abs:    {np.around(tr_amp_max_diff_abs,4)}',\nkind='debug',\nverbosity=verbosity,\n)\n# difference amplitude to reference trace (relativ)\ntr_amp_max_diff_rel = tr_amp_max_diff_abs / ref_tr_peak_val\nxprint(\nf'tr_amp_max_diff_rel:    {np.around(tr_amp_max_diff_rel,4)}',\nkind='debug',\nverbosity=verbosity,\n)\n# boolean indices indicating similar trace amplitudes (based on max amplitude in n_samples)\n# tr_amp_similarity = np.around(tr_amp_max_diff_rel, 0).astype('int')           # 1st approach\ntr_amp_similarity = np.where(tr_amp_max_diff_rel &gt; 0.8, 1, 0).astype(\n'int'\n)  # 2nd approach: quite sufficient (+ replace amp &gt; ref with ref)\n# tr_amp_similarity = rescale_linear(tr_amp_maxima)                               # 3rd approach: linear scaling\n# tr_amp_similarity = np.around(tr_amp_similarity, 0).astype('int')\nxprint(f'tr_amp_similarity:    {tr_amp_similarity}', kind='debug', verbosity=verbosity)\n# boolean indices indicating similar delrt for traces in data subset\ndelrt_similarity = np.where(delrt == delrt.max(), 1, 0)\ndelrt_similarity_inv = np.abs(delrt_similarity - 1)\nxprint(f'delrt_similarity:        {delrt_similarity}', kind='debug', verbosity=verbosity)\nxprint(f'delrt_similarity_inv:    {delrt_similarity_inv}', kind='debug', verbosity=verbosity)\n# sanity check: max amplitudes and DelayRecordingTimes are matching\n# e.g. [1, 1, 1, 0, 0, 0, 0] or [0, 0, 0, 1, 1, 1, 1]\nr = tr_amp_similarity[n_traces]\nif (\nnp.all(tr_amp_similarity[:n_traces] == r) and np.all(tr_amp_similarity[n_traces + 1 :] != r)\n) or (\nnp.all(tr_amp_similarity[:n_traces] != r) and np.all(tr_amp_similarity[n_traces + 1 :] == r)\n):\n# [1]: no correction needed\nif np.array_equal(tr_amp_similarity, delrt_similarity) or np.array_equal(\ntr_amp_similarity, delrt_similarity_inv\n):\nxprint('&lt;&lt;&lt; No correction needed &gt;&gt;&gt;', kind='debug', verbosity=verbosity)\nreturn None, None\n# [2]: incorrect DelayRecordingTime\nelse:\nxprint('*** Incorrect DelayRecordingTime! ***', kind='warning', verbosity=verbosity)\n# get unique DelayRecordingTime and corresponding indices\ndelrt_uniq, delrt_idx = np.unique(delrt, return_index=True)\n# get other DelayRecordingTime(s)\nref_tr_delrt_corrected = delrt_uniq[delrt_uniq != delrt[n_traces]]\nidx_n_traces = n_traces\n# check if incorrect DelayRecordingTime trace is offset\nelif [np.sum(tr_amp_similarity[:n_traces]), np.sum(tr_amp_similarity[n_traces + 1 :])] in [\n[n_traces, 1],\n[1, n_traces],\n]:\n# [np.sum(tr_amp_similarity[:n_traces]), np.sum(tr_amp_similarity[n_traces+1:])] in [[n_traces-1,0], [0,n_traces-1]] #3rd approach\nxprint('Eligible for adjusting offset trace', kind='debug', verbosity=verbosity)\n# convert for comprehension\ntr_amp_similarity = list(tr_amp_similarity)\n# check if first two and last two traces of subset are equal (boundary condition)\nif all(x in [tr_amp_similarity[:2], tr_amp_similarity[-2:]] for x in [[1, 1], [0, 0]]):\nxprint(\n'*** [OFFSET TRACE] Incorrect DelayRecordingTime! ***',\nkind='warning',\nverbosity=verbosity,\n)\nidx_peak_amp_changes = np.where(np.roll(tr_amp_similarity, 1) != tr_amp_similarity)[0]\n# print('idx_peak_amp_changes:', idx_peak_amp_changes)\n# fishy trace after before of DelayRecordingTime\nif len(idx_peak_amp_changes[idx_peak_amp_changes &gt; n_traces]) &lt; len(\nidx_peak_amp_changes[idx_peak_amp_changes &lt; n_traces]\n):\nidx_ = idx_peak_amp_changes[1]\n# fishy trace after change of DelayRecordingTime\nelif len(idx_peak_amp_changes[idx_peak_amp_changes &gt; n_traces]) &gt; len(\nidx_peak_amp_changes[idx_peak_amp_changes &lt; n_traces]\n):\nidx_ = idx_peak_amp_changes[-2]\n# two isolated traces with false DelayRecordingTime!\nelse:\nsys.exit('[ERROR]    Something is really messed up here. Check your data!')\n# get unique DelayRecordingTime and corresponding indices\ndelrt_uniq, delrt_idx = np.unique(delrt, return_index=True)\n# get other DelayRecordingTime(s)\nref_tr_delrt_corrected = delrt_uniq[delrt_uniq != delrt[idx_]]\n# print('ref_tr_delrt_corrected:', ref_tr_delrt_corrected)\n# set correct index for print message\nidx_n_traces = idx_\nelse:\nreturn None, None\nelse:\nreturn None, None\nif len(ref_tr_delrt_corrected) &gt; 1:\nxprint(\n'ref_tr_delrt_corrected: ', ref_tr_delrt_corrected, kind='debug', verbosity=verbosity\n)\nxprint('idx_n_traces:           ', idx_n_traces, kind='debug', verbosity=verbosity)\nxprint(\n'Found more than one DelayRecordingTime to choose from. No changes applied.',\nkind='error',\nverbosity=verbosity,\n)\nreturn None, None\nelse:\nreturn ref_tr_delrt_corrected[0], idx_n_traces\n</code></pre>"},{"location":"api/api_delrt_correction_segy/#pseudo_3D_interpolation.delrt_correction_segy.check_DelayRecordingTime_changes","title":"<code>check_DelayRecordingTime_changes(file, tracecount, byte_delay=109, n_traces=5, n_samples=120, update_segy=False, plot_org=False, plot_corr=False, filename='', verbosity=0)</code>","text":"<p>Check SEG-Y file (e.g. PARASOUND or TOPAS data) for potential incorrect DelayRecordingTimes. In case of a incorrectly assigned DelayRecordingTime the corresponding values will be updates inplace!</p> <p>Parameters:</p> <ul> <li> file             (<code>segyio.SegyFile</code>)         \u2013 <p>An open segyio file handle.</p> </li> <li> tracecount             (<code>int</code>)         \u2013 <p>Number of traces in SEG-Y file.</p> </li> <li> byte_delay             (<code>int</code>)         \u2013 <p>Byte position of DelayRecordingTime in SEG-Y trace header.</p> </li> <li> n_traces             (<code>int, optional</code>)         \u2013 <p>Number of neighboring traces of reference trace (default: 5).</p> </li> <li> n_samples             (<code>int, optional</code>)         \u2013 <p>Number of samples per trace for comparision of max. amplitude (default: 7).</p> </li> <li> update_segy             (<code>bool, optional</code>)         \u2013 <p>Enable inplace updating of DelayRecordingTime values in SEG-Y trace header (default: False).</p> </li> <li> plot_org             (<code>bool, optional</code>)         \u2013 <p>Create plot of original traces with auxiliary information (default: False).</p> </li> <li> plot_corr             (<code>bool, optional</code>)         \u2013 <p>Create plot of corrected traces with auxiliary information (default: False).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\delrt_correction_segy.py</code> <pre><code>def check_DelayRecordingTime_changes(\nfile,\ntracecount,\nbyte_delay=109,\nn_traces=5,\nn_samples=120,\nupdate_segy=False,\nplot_org=False,\nplot_corr=False,\nfilename='',\nverbosity=0,\n):\n\"\"\"\n    Check SEG-Y file (e.g. PARASOUND or TOPAS data) for potential incorrect DelayRecordingTimes.\n    In case of a incorrectly assigned DelayRecordingTime the corresponding values will be updates inplace!\n    Parameters\n    ----------\n    file : segyio.SegyFile\n        An open segyio file handle.\n    tracecount : int\n        Number of traces in SEG-Y file.\n    byte_delay : int\n        Byte position of DelayRecordingTime in SEG-Y trace header.\n    n_traces : int, optional\n        Number of neighboring traces of reference trace (default: 5).\n    n_samples : int, optional\n        Number of samples per trace for comparision of max. amplitude (default: 7).\n    update_segy : bool, optional\n        Enable inplace updating of DelayRecordingTime values in SEG-Y trace header (default: False).\n    plot_org : bool, optional\n        Create plot of original traces with auxiliary information (default: False).\n    plot_corr : bool, optional\n        Create plot of corrected traces with auxiliary information (default: False).\n    \"\"\"\n# two way travel time (TWTT) [ms]\ntwt = file.samples\n# trace sequence number within reel (starting at 1)\ntracr = file.attributes(segyio.TraceField.TRACE_SEQUENCE_FILE)[:]\n# get \"FieldRecordNumber\" for each trace\nfldr = file.attributes(segyio.TraceField.FieldRecord)[:]\n# get \"DelayRecordingTime\" for each trace\ndelrt = file.attributes(byte_delay)[:]\n# find indices where DelayRecordingTime changes\ndelrt_idx = np.where(np.roll(delrt, 1) != delrt)[0]\n# sanity check: in case first and last trace of file have same DelayRecordingTime!\nif delrt_idx[0] != 0:\ndelrt_idx = np.insert(delrt_idx, 0, 0, axis=0)\n# check for multiple DelayRecordingTimes in trace headers\nif len(delrt_idx) &gt; 1:\nxprint(\nf'Found &lt; {len(delrt_idx)-1} &gt; different DelayRecordingTimes: {dict(zip(delrt_idx, delrt[delrt_idx]))}',\nkind='info',\nverbosity=verbosity,\n)\n# loop over all DelayRecordingTime change indices\nfor i, idx in enumerate(delrt_idx[1:]):  # skip index: 0\nxprint(\nf'[{i+1}] === FRN: {fldr[idx]} - trace idx: {idx} - Delay: {delrt[idx]} ms ===',\nkind='debug',\nverbosity=verbosity,\n)\n# generate indices for subsetting of input data\nidx_subset_min = idx - n_traces\nidx_subset_max = idx + n_traces + 1\n# sanity check: make sure there are at least `n_traces` neighboring traces in data range\nif (idx_subset_min &lt; 0) or (idx_subset_max &gt; tracecount + 1):\nxprint(\nf'Not enough neighboring traces for idx: {idx} [{idx_subset_min}:{idx_subset_max}] with &gt;{tracecount}&lt; total traces. Skipped data subset.',\nkind='warning',\nverbosity=verbosity,\n)\ncontinue\n# get subset of DelayRecordingTimes\ndelrt_subset = delrt[idx_subset_min:idx_subset_max]\nif len(np.unique(delrt_subset)) &gt; 2:\nxprint(\nf'Too many different `delrt` for idx: {idx} [{idx_subset_min}:{idx_subset_max}]. Skipped data subset.',\nkind='warning',\nverbosity=verbosity,\n)\ncontinue\n# get subset of FieldRecord numbers\nfldr_subset = fldr[idx_subset_min:idx_subset_max]\n# get subset of trace sequence numbers\ntracr_subset = tracr[idx_subset_min:idx_subset_max]\n# get seismic data [amplitude]; transpose to fit numpy data structure\ndata_subset = file.trace.raw[\nidx_subset_min:idx_subset_max\n].T  # eager version (completely read into memory)\n# plot trace data (with delrt)\nif plot_org and verbosity == 2:\nplot_seismic_wiggle(\ndata_subset,\ntwt=twt,\ntraces=np.arange(idx_subset_min, idx_subset_max),\ntitle=f'{filename} - original -',\nadd_kind=delrt_subset,\ngain=1.0,\nplot_kwargs=None,\n)\n# get correct DelayRecordingTime for reference trace\ndelrt_corrected, idx_trace_subset_win = correct_single_trace_DelayRecordingTime(\nidx,\ndata_subset,\ndelrt_subset,\nfldr_subset,\nn_traces,\nn_samples,\nverbosity=verbosity,\n)\n# perform update only if correct DelayRecordingTime was calculated\nif delrt_corrected is not None:\n# update DelayRecordingTime for input trace\nmsg = f'Changing DelayRecordingTime for FRN #{fldr_subset[idx_trace_subset_win]} '\nmsg += (\nf'(idx:{tracr_subset[idx_trace_subset_win]-1}) [i:{idx_trace_subset_win}] from '\n)\nmsg += f'&gt; {delrt_subset[idx_trace_subset_win]} &lt; to &gt; {delrt_corrected} &lt;'\nxprint(msg, kind='info', verbosity=verbosity)\n# plot trace data (with corrected delrt)\nif plot_corr and verbosity == 2:\n# copy delrt\ndelrt_subset_corr = delrt_subset\n# replace incorrect delrt\ndelrt_subset_corr[idx_trace_subset_win] = delrt_corrected\nplot_seismic_wiggle(\ndata_subset,\ntwt=twt,\ntraces=np.arange(idx_subset_min, idx_subset_max),\ntitle=f'{filename} - corrected -',\nadd_kind=delrt_subset_corr,\ngain=1.0,\nplot_kwargs=None,\n)\n# update trace header DelayRecordingTime inplace\nif update_segy:\nxprint(\n'Updating trace header value of DelayRecordingTime inplace!',\nkind='warning',\nverbosity=verbosity,\n)\nheader = file.header[idx]\nxprint(\nf'file.header[{idx}] (old): {file.header[idx]}',\nkind='debug',\nverbosity=verbosity,\n)\nheader[byte_delay] = delrt_corrected\nxprint(\nf'file.header[{idx}] (new): {file.header[idx]}',\nkind='debug',\nverbosity=verbosity,\n)\n# break\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"api/api_delrt_correction_segy/#pseudo_3D_interpolation.delrt_correction_segy.wrapper_delrt_correction_segy","title":"<code>wrapper_delrt_correction_segy(in_path, out_dir, inplace, txt_suffix, byte_delay, win_ntraces, win_nsamples, verbosity)</code>","text":"<p>Correct DelayRecordingTime (<code>delrt</code>) for single SEG-Y file.</p> Source code in <code>pseudo_3D_interpolation\\delrt_correction_segy.py</code> <pre><code>def wrapper_delrt_correction_segy(\nin_path, out_dir, inplace, txt_suffix, byte_delay, win_ntraces, win_nsamples, verbosity\n):  # noqa\n\"\"\"Correct DelayRecordingTime (`delrt`) for single SEG-Y file.\"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=verbosity)\nto_process = check_varying_DelayRecordingTimes(in_path, byte_delay)\nif not to_process:\nreturn False\nif (out_dir is not None and os.path.isdir(out_dir)) and inplace is False:\nxprint('Creating copy of file in output directory', kind='info', verbosity=verbosity)\nif txt_suffix is not None:\nout_path = os.path.join(out_dir, f'{basename}_{txt_suffix}{suffix}')\nelse:\nout_path = os.path.join(out_dir, f'{basename}_delrt{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=verbosity,\n)\nos.remove(out_path)\ncopy2(in_path, out_path)\npath = out_path\nelif out_dir is None and inplace is True:\nxprint('Updating trace header inplace!', kind='warning', verbosity=verbosity)\npath = in_path\n#!!! DEBUG\n# inplace = False\n# read SEGY file\nwith segyio.open(path, 'r+', strict=False, ignore_geometry=True) as src:\nn_traces = src.tracecount  # total number of traces\n# check each change in DelayRecordingTimes\nc = check_DelayRecordingTime_changes(\nsrc,\nn_traces,\nbyte_delay,\nwin_ntraces,\nwin_nsamples,\nupdate_segy=inplace,\nplot_org=False,\nplot_corr=True,\nfilename=filename,\nverbosity=verbosity,\n)\n# update textual header\ntext = get_textual_header(path)\ninfo = f'DELRT FIX (byte:{byte_delay})'\ntext_updated = add_processing_info_header(text, info, prefix='_TODAY_')\nwrite_textual_header(path, text_updated)\n# return boolean for sanity check\nreturn c\n</code></pre>"},{"location":"api/api_delrt_padding_segy/","title":"<code>delrt_padding_segy.py</code>","text":"<p>Check for vertical offsets in SEG-Y file(s) and apply zero padding. For this, the script uses the trace header keyword DelayRecordingTime and pads seismic traces with zeros to compensate variable recording starting times.</p>"},{"location":"api/api_delrt_padding_segy/#pseudo_3D_interpolation.delrt_padding_segy.pad_trace_data","title":"<code>pad_trace_data(data, recording_delays, n_traces, dt, twt, verbosity=0)</code>","text":"<p>Pad seismic trace data recorded in window mode with a fixed length and variable <code>DelayRecordingTimes</code>. This function reads the DelayRecordingTime from the trace headers and pads the data at top and bottom with zeros.</p> <p>Parameters:</p> <ul> <li> data             (<code>numpy.ndarray</code>)         \u2013 <p>Amplitude array with samples (rows) and traces (columns). Transposed array read by segyio for numpy compatibility.</p> </li> <li> recording_delays             (<code>numpy.ndarray</code>)         \u2013 <p>Array of DelayRecordingTimes for each trace in SEG-Y file [ms].</p> </li> <li> n_traces             (<code>int</code>)         \u2013 <p>Number of traces in SEG-Y file.</p> </li> <li> dt             (<code>float</code>)         \u2013 <p>Sampling interval [ms].</p> </li> <li> twt             (<code>numpy.ndarray</code>)         \u2013 <p>Two way travel times (TWTT) of every sample in a trace.</p> </li> </ul> <p>Returns:</p> <ul> <li> data_padded(            <code>numpy.ndarray</code> )        \u2013 <p>Padded input data array in time dimension.</p> </li> <li> twt_padded(            <code>numpy.ndarray</code> )        \u2013 <p>Updated TWTT that fit the expanded time dimension.</p> </li> <li> n_samples_padded(            <code>int</code> )        \u2013 <p>New number of traces in SEG-Y file.</p> </li> <li> idx_delay(            <code>numpy.ndarray</code> )        \u2013 <p>Trace indices where DelayRecordingTime changes.</p> </li> <li> min_delay(            <code>int</code> )        \u2013 <p>Minimum DelayRecordingTime in SEG-Y file [ms].</p> </li> <li> max_delay(            <code>int</code> )        \u2013 <p>Maximum DelayRecordingTime in SEG-Y file [ms].</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\delrt_padding_segy.py</code> <pre><code>def pad_trace_data(data, recording_delays, n_traces, dt, twt, verbosity=0):\n\"\"\"\n    Pad seismic trace data recorded in window mode with a fixed length and variable `DelayRecordingTimes`.\n    This function reads the DelayRecordingTime from the trace headers and pads the data at top and bottom with zeros.\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Amplitude array with samples (rows) and traces (columns).\n        Transposed array read by segyio for numpy compatibility.\n    recording_delays : numpy.ndarray\n        Array of DelayRecordingTimes for each trace in SEG-Y file [ms].\n    n_traces : int\n        Number of traces in SEG-Y file.\n    dt : float\n        Sampling interval [ms].\n    twt : numpy.ndarray\n        Two way travel times (TWTT) of every sample in a trace.\n    Returns\n    -------\n    data_padded : numpy.ndarray\n        Padded input data array in time dimension.\n    twt_padded : numpy.ndarray\n        Updated TWTT that fit the expanded time dimension.\n    n_samples_padded : int\n        New number of traces in SEG-Y file.\n    idx_delay : numpy.ndarray\n        Trace indices where DelayRecordingTime changes.\n    min_delay : int\n        Minimum DelayRecordingTime in SEG-Y file [ms].\n    max_delay : int\n        Maximum DelayRecordingTime in SEG-Y file [ms].\n    \"\"\"\n# find indices where DelayRecordingTime changes\nidx_delay = np.where(np.roll(recording_delays, 1) != recording_delays)[0]\n# sanity check: in case first and last trace of file have same DelayRecordingTime!\nif idx_delay[0] != 0:\nidx_delay = np.insert(idx_delay, 0, 0, axis=0)\n# minimum and maximum DelayRecordingTime in SEG-Y\nmin_delay = recording_delays.min()\nmax_delay = recording_delays.max()\n# pad twt array from minimum delay to maximum delay + data window size\ntwt_padded = np.arange(min_delay, max_delay + (twt[-1] - twt[0]) + dt, dt)\nn_samples_padded = len(twt_padded)\n# initialize padded data array\ndata_padded = np.ndarray((len(twt_padded), n_traces))\n# make sure array is contiguous in memory (C order)\ndata_padded = np.ascontiguousarray(data_padded, dtype=data.dtype)\nfor i, delay in enumerate(idx_delay):\nxprint(f'{i}: {recording_delays[delay]} ms', kind='debug', verbosity=verbosity)\n# ---------- first data slices ----------\nif idx_delay[i] != idx_delay[-1]:\n# ----- get data slice -----\ndata_tmp = data[:, idx_delay[i] : idx_delay[i + 1]]\ndata_len = data_tmp.shape[0]\nxprint(f'data_tmp.shape: {data_tmp.shape}', kind='debug', verbosity=verbosity)\n# create array with samples to pad at the data top\npad_top = np.zeros(\n[\nnp.arange(min_delay, recording_delays[delay], dt).size,\nidx_delay[i + 1] - idx_delay[i],\n]\n)\nxprint(f'pad_top:    {pad_top.shape}', kind='debug', verbosity=verbosity)\ndata_tmp = np.insert(data_tmp, 0, pad_top, axis=0)\n# create array with samples to pad at the data bottom\npad_bottom = np.zeros(\n[twt_padded.size - data_len - pad_top.shape[0], idx_delay[i + 1] - idx_delay[i]]\n)\nxprint(f'pad_bottom:    {pad_bottom.shape}', kind='debug', verbosity=verbosity)\ndata_tmp = np.insert(data_tmp, data_tmp.shape[0], pad_bottom, axis=0)\n# ----- insert padded data into new ndarray -----\ndata_padded[:, idx_delay[i] : idx_delay[i + 1]] = data_tmp\n# ---------- last data slice ----------\nelif idx_delay[i] == idx_delay[-1]:\nxprint('*** last slice ***', kind='debug', verbosity=verbosity)\n# ----- get data slice -----\ndata_tmp = data[:, idx_delay[i] : n_traces]\nxprint(f'data_tmp.shape: {data_tmp.shape}', kind='debug', verbosity=verbosity)\n# create array with samples to pad at the data top\npad_top = np.zeros(\n[np.arange(min_delay, recording_delays[delay], dt).size, n_traces - idx_delay[i]]\n)\nxprint(f'pad_top:    {pad_top.shape}', kind='debug', verbosity=verbosity)\ndata_tmp = np.insert(data_tmp, 0, pad_top, axis=0)\n# create array with samples to pad at the data bottom\npad_bottom = np.zeros(\n[twt_padded.size - twt.size - pad_top.shape[0], n_traces - idx_delay[i]]\n)\nxprint(f'pad_bottom:    {pad_bottom.shape}', kind='debug', verbosity=verbosity)\ndata_tmp = np.insert(data_tmp, data_tmp.shape[0], pad_bottom, axis=0)\n# ----- insert padded data into new ndarray -----\ndata_padded[:, idx_delay[i] : n_traces] = data_tmp\nreturn data_padded, twt_padded, n_samples_padded, (idx_delay, min_delay, max_delay)\n</code></pre>"},{"location":"api/api_delrt_padding_segy/#pseudo_3D_interpolation.delrt_padding_segy.wrapper_delrt_padding_segy","title":"<code>wrapper_delrt_padding_segy(in_path, args)</code>","text":"<p>Pad DelayRecordingTime <code>delrt</code> for single SEG-Y file.</p> Source code in <code>pseudo_3D_interpolation\\delrt_padding_segy.py</code> <pre><code>def wrapper_delrt_padding_segy(in_path, args):  # noqa\n\"\"\"Pad DelayRecordingTime `delrt` for single SEG-Y file.\"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=args.verbose)\nif args.output_dir is None:\nxprint('Creating copy of file in output directory', kind='info', verbosity=args.verbose)\nout_dir = basepath\nelse:\nxprint(\n'Creating copy of file in specified output directory',\nkind='info',\nverbosity=args.verbose,\n)\nif args.txt_suffix is not None:\nout_path = os.path.join(out_dir, f'{basename}_{args.txt_suffix}{suffix}')\nelse:\nout_path = os.path.join(out_dir, f'{basename}_pad{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=args.verbose,\n)\n# os.remove(out_path)\n# read SEGY file\nwith segyio.open(in_path, 'r', strict=False, ignore_geometry=True) as src:\nn_traces = src.tracecount  # total number of traces\ndt = segyio.tools.dt(src) / 1000  # sample rate [ms]\n# n_samples = src.samples.size        # total number of samples\ntwt = src.samples  # two way travel time (TWTT) [ms]\n# get \"DelayRecordingTime\" value for each trace\nrecording_delays = src.attributes(args.byte_delay)[:]\n# find unique DelayRecordingTimes and corresponding trace index\nrecording_delays_uniq, recording_delays_idx = np.unique(\nrecording_delays[:], return_index=True\n)\n# check if padding is needed\nif len(recording_delays_uniq) &gt; 1:\nxprint(\nf'Found &lt; {len(recording_delays_uniq)} &gt; different \"DelayRecordingTimes\" for file &lt; {filename} &gt;',\nkind='info',\nverbosity=args.verbose,\n)\nelse:\nreturn False\n# get seismic data [amplitude]; transpose to fit numpy data structure\ndata = src.trace.raw[:].T  # eager version (completely read into memory)\n# pad source data to generate continuous array without vertical offsets\ndata_padded, twt_padded, n_samples_padded, delay_attrs = pad_trace_data(\ndata, recording_delays, n_traces, dt, twt, args.verbose\n)\n(idx_delay, min_delay, max_delay) = delay_attrs\n# get source metadata\nspec = segyio.tools.metadata(src)\nspec.samples = twt_padded  # set padded samples\n# create new output SEG-Y file with updated header information\nxprint(f'Writing padded output file &lt; {filename} &gt;', kind='info', verbosity=args.verbose)\nwith segyio.create(out_path, spec) as dst:\ndst.text[0] = src.text[0]  # copy textual header\ndst.bin = src.bin  # copy binary header\ndst.header = src.header  # copy trace headers\ndst.trace = np.ascontiguousarray(\ndata_padded.T, dtype=data.dtype\n)  # set padded trace data\n# update binary header with padded sample count\ndst.bin[segyio.BinField.Samples] = n_samples_padded\n# update trace headers with padded sample count &amp; new DelayRecordingTime\nfor h in dst.header[:]:\nh.update(\n{\nsegyio.TraceField.TRACE_SAMPLE_COUNT: n_samples_padded,\nsegyio.TraceField.DelayRecordingTime: min_delay,\n}\n)\n# update textual header\ntext = get_textual_header(out_path)\ninfo = f'PAD DELRT (byte:{segyio.TraceField.DelayRecordingTime})'\ntext_updated = add_processing_info_header(text, info, prefix='_TODAY_')\nwrite_textual_header(out_path, text_updated)\n</code></pre>"},{"location":"api/api_despiking_2D_segy/","title":"<code>despiking_2D_segy.py</code>","text":"<p>Despike SEG-Y file(s) using 2D moving window function.</p> <p>Remove noise bursts (only single trace!) from seismic data based on the amplitude of the data within a time window. This amplitude is compared to the background amplitude which is computed from a user defined number of adjacent traces. If the amplitude in this window exceeds the threshold x the background amplitude then the samples within the selected window may be scaled, replaced by threshold x background amplitude or set to zero. Tapering is applied if scaling.</p>"},{"location":"api/api_despiking_2D_segy/#pseudo_3D_interpolation.despiking_2D_segy.despike_2D","title":"<code>despike_2D(array, window, dt, overlap=10, ntraces=5, mode='mean', threshold=2, out='scaled', verbosity=0)</code>","text":"<p>Remove single-trace noise bursts from seismic data. The algorithm is based on the amplitude of the data within a time window. This amplitude is compared to the background amplitude which is computed from a user-defined number of adjacent traces. If the amplitude in this window exceeds the threshold x the background amplitude then the samples within the selected window may be scaled, replaced by threshold x background amplitude or set to zero. Tapering is applied to any replacement operation.</p> <p>Parameters:</p> <ul> <li> array             (<code>np.ndarray</code>)         \u2013 <p>Seismic data (samples x traces).</p> </li> <li> window             (<code>int</code>)         \u2013 <p>Time window (in ms).</p> </li> <li> dt             (<code>float</code>)         \u2013 <p>Sampling interval in milliseconds [ms].</p> </li> <li> overlap             (<code>int, optional</code>)         \u2013 <p>Window overlap in percentage (%).</p> </li> <li> ntraces             (<code>int, optional</code>)         \u2013 <p>Number of adjacent traces (default: <code>5</code>).</p> </li> <li> mode             (<code>str, optional</code>)         \u2013 <p>Algorithm to compute amplitude in window [mean, rms, median] (default: <code>mean</code>).</p> </li> <li> threshold             (<code>float, optional</code>)         \u2013 <p>Amplitude threshold for spike detection (default: <code>2</code>).</p> </li> <li> out             (<code>str, optional</code>)         \u2013 <p>Amplitude values replacing spike values (default: <code>scaled</code>).</p> <ul> <li><code>scaled</code>: Scale signal down to background amplitude (based on mode). Tapering applied.</li> <li><code>mode</code>: Replace with background amplitude values</li> <li><code>threshold</code>: Replace with threshold * background amplitude values.</li> <li><code>zeros</code>: Replace with zero values.</li> <li><code>median</code>: Replace with median values (calculated from neighboring traces).</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> despiked(            <code>np.ndarray</code> )        \u2013 <p>Despiked input data.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\despiking_2D_segy.py</code> <pre><code>def despike_2D(\narray, window, dt, overlap=10, ntraces=5, mode='mean', threshold=2, out='scaled', verbosity=0\n):\n\"\"\"\n    Remove single-trace noise bursts from seismic data.\n    The algorithm is based on the amplitude of the data within a time window.\n    This amplitude is compared to the background amplitude which is computed from a user-defined number of adjacent traces.\n    If the amplitude in this window exceeds the threshold x the background amplitude then the samples within\n    the selected window may be scaled, replaced by threshold x background amplitude or set to zero.\n    Tapering is applied to any replacement operation.\n    Parameters\n    ----------\n    array : np.ndarray\n        Seismic data (samples x traces).\n    window : int\n        Time window (in ms).\n    dt : float\n        Sampling interval in milliseconds [ms].\n    overlap : int, optional\n        Window overlap in percentage (%).\n    ntraces : int, optional\n        Number of adjacent traces (default: `5`).\n    mode : str, optional\n        Algorithm to compute amplitude in window [mean, rms, median] (default: `mean`).\n    threshold : float, optional\n        Amplitude threshold for spike detection (default: `2`).\n    out : str, optional\n        Amplitude values replacing spike values (default: `scaled`).\n        - `scaled`: Scale signal down to background amplitude (based on mode). Tapering applied.\n        - `mode`: Replace with background amplitude values\n        - `threshold`: Replace with threshold * background amplitude values.\n        - `zeros`: Replace with zero values.\n        - `median`: Replace with median values (calculated from neighboring traces).\n    Returns\n    -------\n    despiked : np.ndarray\n        Despiked input data.\n    \"\"\"\nfunctions = {'mean': np.mean, 'median': np.median, 'rms': rms}\n# checks\nif overlap &lt; 0 or overlap &gt; 100:\nraise ValueError('Overlap must be integer between 0 and 100 [%].')\nif threshold &lt; 0:\nraise ValueError('Theshold must be positive.')\nif ntraces % 2 == 0:\nraise ValueError('Number of traces must be odd integer.')\nif mode not in ['mean', 'rms', 'median']:\nraise ValueError(\"Amplitude mode must be one of ['mean', 'rms', 'median'].\")\nelse:\nfunc = functions.get(mode)\nreplace_amp_mode = ['scaled', 'mode', 'threshold', 'zeros', 'median']\nif out not in replace_amp_mode:\nraise ValueError(f\"Output amplitude option must be one of {replace_amp_mode}.\")\n# shape of moving window\nwin = (int(window / dt), ntraces)\nxprint(f'win: {win}', kind='debug', verbosity=verbosity)\n# compute overlap in samples (from time [ms])\noverlap = np.around(overlap / 100 * win[0], 0)\noverlap = overlap if overlap &gt;= 1 else 1\n# trace step size\ndx = 1\n# time step size\ndy = int(win[0] - overlap)\nxprint(f'dy (twt): {dy},  dx (traces): {dx}', kind='debug', verbosity=verbosity)\n# ---------------------------------------------------------------------\n#                       initial selection\n# ---------------------------------------------------------------------\n# get view of moving windows\nv = moving_window_2D(array, win, dx, dy)\nxprint('Creating moving window views', kind='debug', verbosity=verbosity)\nxprint('v.shape:  ', v.shape, kind='debug', verbosity=verbosity)\nxprint('v.strides:', v.strides, kind='debug', verbosity=verbosity)\n# one mode value per row (time sample) of search window (adjacent traces)\nv_mode = func(np.abs(v), axis=(-1))  # (-1): one value per row, (-2,-1): single mode\nxprint('v_mode.shape:  ', v_mode.shape, kind='debug', verbosity=verbosity)\n# 4D indices of samples representing potential spike\nv_idx = np.where(\nnp.abs(v) &gt; threshold * v_mode.reshape(v_mode.shape + (1,))\n)  # (1,): one value per row, (1,1): single mode\n# convert 4D view indices to 2D data array indices &amp; filter for unique ones\nidx_uniq = np.unique(np.vstack((v_idx[0] * dy + v_idx[2], v_idx[1] * dx + v_idx[3])).T, axis=0)\n# filter indices based on total number of spike samples per trace\n# -&gt; discard traces where total sample number &lt;= 10% of input time window\n# (assumes reasonable user input for spike length)\ntr_idx_uniq, tr_idx_cnt = np.unique(\nidx_uniq[:, 1], return_counts=True\n)  # get trace indices &amp; counts\ntr_idx_uniq_to_keep = np.where(tr_idx_cnt &gt; win[0] * 0.1)  # create boolean filter\ntr_idx_to_keep = tr_idx_uniq[tr_idx_uniq_to_keep]  # trace indices to keep\n# select only valid data (number of samples &gt; 10% of window length)\nif len(tr_idx_to_keep) &gt; 0:\nidx_uniq = idx_uniq[np.isin(idx_uniq[:, 1], tr_idx_to_keep)]\n# create dummy array\nelse:\nidx_uniq = np.empty((0, 2), dtype='int')\nxprint('idx_uniq.shape:  ', idx_uniq.shape, kind='debug', verbosity=verbosity)\n# ---------------------------------------------------------------------\n#                       additional selection\n# ---------------------------------------------------------------------\n# create additional stride views (if required)\nN = array.shape[0]  # number of rows in array (time dowrapper_despiking_2D_segy)\nM = win[0]  # number of rows in moving window (time dowrapper_despiking_2D_segy)\n# check if view left out some part of input array\n# i: start row index of moving window (based on dy)\n# (i - dy + M != N): check if previous view already covered rows until end of array, if not --&gt; additional view needed!\n# (N - i &lt; dy): check number of rows in last view are less than dy --&gt; no view then!\nmissing_views = [i for i in range(0, N, dy) if (N - i &lt; dy)]  # (i - dy + M != N) and\nif any(missing_views):\n# compute start sample index for view ranging until last sample\nstart = missing_views[0] - (M - (N - missing_views[0]))\n# get view of moving windows (previously missing data range)\nv_add = moving_window_2D(array[start:], win, dx, dy)\nxprint('v_add.shape:  ', v_add.shape, kind='debug', verbosity=verbosity)\nxprint('v_add.strides:', v_add.strides, kind='debug', verbosity=verbosity)\n# mode of additional views\nv_add_mode = func(np.abs(v_add), axis=(-1))  # (-1): one value per row, (-2,-1): single mode\nxprint('v_add_mode.strides:', v_add_mode.strides, kind='debug', verbosity=verbosity)\n# 4D indices of samples representing potential spike\nv_add_idx = np.where(\nnp.abs(v_add) &gt; threshold * v_add_mode.reshape(v_add_mode.shape + (1,))\n)  # (1,): one value per row, (1,1): single mode\n# convert 4D view indices to 2D data array indices (using `start` variable) &amp; filter for unique ones\nidx_add_uniq = np.unique(\nnp.vstack(\n(start + v_add_idx[0] * dy + v_add_idx[2], v_add_idx[1] * dx + v_add_idx[3])\n).T,\naxis=0,\n)\n# filter indices based on total number of spike samples per trace\n# -&gt; discard traces where total sample number &lt;= 10% of input time window\n# (assumes reasonable user input for spike length)\ntr_idx_add_uniq, tr_idx_add_cnt = np.unique(idx_add_uniq[:, 1], return_counts=True)\ntr_idx_add_uniq_to_keep = np.where(tr_idx_add_cnt &gt; win[0] * 0.1)\ntr_idx_add_to_keep = tr_idx_add_uniq[tr_idx_add_uniq_to_keep]\n# select only valid data (number of samples &gt; 10% of window length)\nif len(tr_idx_add_to_keep) &gt; 0:\nidx_add_uniq = idx_add_uniq[np.isin(idx_add_uniq[:, 1], tr_idx_add_to_keep)]\n# create dummy array\nelse:\nidx_add_uniq = np.empty((0, 2), dtype='int')\nelse:\nidx_add_uniq = np.empty((0, 2), dtype='int')\nxprint('idx_add_uniq.shape:', idx_add_uniq.shape, kind='debug', verbosity=verbosity)\n# ---------------------------------------------------------------------\n#                   combine spike sample indices\n# ---------------------------------------------------------------------\n# merge indices from both selection processes &amp; select unique ones\nidx_merge = np.unique(np.concatenate((idx_uniq, idx_add_uniq), axis=0), axis=0)\nxprint('idx_merge.shape:', idx_merge.shape, kind='debug', verbosity=verbosity)\nif idx_merge.size == 0:\nxprint(\nf'No spikes detected ({array.shape[1]} traces). Consider adjusting the input parameters.',\nkind='info',\nverbosity=verbosity,\n)\nreturn array\n# sort indices based on (1) trace index and (2) sample index\nsorter = np.lexsort((idx_merge[:, 0], idx_merge[:, 1]))\nidx_merge_sorted = idx_merge[sorter]\nxprint('idx_merge_sorted.shape:', idx_merge_sorted.shape, kind='debug', verbosity=verbosity)\n# ---------------------------------------------------------------------\n#                   filter &amp; discard false detections\n# ---------------------------------------------------------------------\n# split index array by trace index\nspike_index_arrays = []\nspikes_per_trace = np.split(idx_merge_sorted, np.where(np.diff(idx_merge_sorted[:, 1]))[0] + 1)\n# loop over every spike array\nfor spike in spikes_per_trace:\n# filter indices if difference to following index &gt; 5% of window length (in samples)\nsample_indices_change_idx = np.where(np.diff(spike[:, 0]) &gt; win[0] * 0.05)[0] + 1\n# split spike indices array by indices of large changes &amp; keep spike only if spike length &gt; 5% of window length (in samples)\nrewrapper_despiking_2D_segying_spikes = [\na\nfor a in np.split(spike, sample_indices_change_idx, axis=0)\nif a.shape[0] &gt; win[0] * 0.05\n]\n# append spike index array to list of all spikes in data array\nspike_index_arrays.extend(rewrapper_despiking_2D_segying_spikes)\n# stack rewrapper_despiking_2D_segying spike index arrays after filtering\ntry:\nidx_merge_sorted = np.concatenate(spike_index_arrays, axis=0)\nxprint(\n'idx_merge_sorted.shape (filtered):',\nidx_merge_sorted.shape,\nkind='debug',\nverbosity=verbosity,\n)\nexcept ValueError:\nxprint(\nf'No spikes detected ({array.shape[1]} traces). Consider adjusting the input parameters.',\nkind='info',\nverbosity=verbosity,\n)\nreturn array\n# ---------------------------------------------------------------------\n#                   replace spike amplitudes\n# ---------------------------------------------------------------------\n# make copy of input data\narray_out = array  # .copy()\n# init list for indices plotting\nspike_time_ranges_indices = []\n# loop over each spike index array\nfor idx_arr in spike_index_arrays:\n# get index of trace\nidx_trace = idx_arr[0, 1]\nxprint('idx_trace.shape:', int(idx_trace), kind='debug', verbosity=verbosity)\n# extract shape of spike index array\nN_spike, M_spike = idx_arr.shape\nxprint('idx_arr.shape:', idx_arr.shape, kind='debug', verbosity=verbosity)\n# get data subset\n## pad minimum and maximum sample index with 10% of spike sample range (for tapering)\n### minimum\nidx_sample_min = idx_arr[0, 0] - int(N_spike * 0.1)\nidx_sample_min = idx_sample_min if idx_sample_min &gt;= 0 else 0\nspike_time_ranges_indices.append((idx_sample_min, idx_trace))\n## maximum\nidx_sample_max = idx_arr[-1, 0] + int(N_spike * 0.1) + 1\nidx_sample_max = idx_sample_max if idx_sample_max &lt;= N else N\nspike_time_ranges_indices.append((idx_sample_max, idx_trace))\n## account for traces at data limits\n### get trace window half-width\ntr_win_idx = win[1] // 2\nxprint(f'tr_win_idx: {tr_win_idx}', kind='debug', verbosity=verbosity)\nidx_trace_min = idx_trace - tr_win_idx\nidx_trace_max = idx_trace + tr_win_idx + 1\nxprint(\nf'idx_trace_min: {int(idx_trace_min)}, idx_trace_max: {int(idx_trace_max)}',\nkind='debug',\nverbosity=verbosity,\n)\n# account for boundaries of seismic section\nif idx_trace_min &lt; 0:\nidx_trace_min = 0  # no negative indexing possible\nxprint(f'tr_win_idx (new): {tr_win_idx}', kind='debug', verbosity=verbosity)\n## select subset of input data based on calculated ranges\n# spike_win = array_out[idx_sample_min:idx_sample_max, idx_trace_min:idx_trace_max]\nspike_win = array_out[\nint(idx_sample_min) : int(idx_sample_max), int(idx_trace_min) : int(idx_trace_max)\n]\nxprint(\n'idx_sample_min, idx_sample_max, tr_win_idx:',\nidx_sample_min,\nidx_sample_max,\ntr_win_idx,\nkind='debug',\nverbosity=verbosity,\n)\nxprint('spike_win:', spike_win.shape, kind='debug', verbosity=verbosity)\n# calc output amplitudes (using user-specified mode)\nspike_amps = spike_win[:, tr_win_idx]  # amplitudes of fishy trace\nif out == 'scaled':\nneighbor_amps = func(\nnp.abs(spike_win), axis=1\n)  # * threshold # adjusted mode amplitude (x threshold)\nspike_win_amps_out = spike_amps / (spike_amps.max() / neighbor_amps)\n# creating taper window\nw = np.blackman(len(spike_win_amps_out))\n# tapering resulting amplitudes\nspike_win_amps_out = spike_win_amps_out * w\nelif out == 'mode':\nneighbor_amps = func(spike_win, axis=1)\nspike_win_amps_out = neighbor_amps\nelif out == 'threshold':\nneighbor_amps = func(spike_win, axis=1)\nspike_win_amps_out = neighbor_amps * threshold\nelif out == 'zeros':\nspike_win_amps_out = np.zeros_like(spike_amps)\nelif out == 'median':\nneighbor_amps = np.median(spike_win, axis=1)\nspike_win_amps_out = neighbor_amps\n# replace spike amplitudes in window with scaled ones\nspike_win[:, tr_win_idx] = spike_win_amps_out.astype(spike_win.dtype)\nreturn array_out\n</code></pre>"},{"location":"api/api_despiking_2D_segy/#pseudo_3D_interpolation.despiking_2D_segy.wrapper_despiking_2D_segy","title":"<code>wrapper_despiking_2D_segy(in_path, args)</code>","text":"<p>Despike input SEG-Y file(s).</p> Source code in <code>pseudo_3D_interpolation\\despiking_2D_segy.py</code> <pre><code>def wrapper_despiking_2D_segy(in_path, args):\n\"\"\"Despike input SEG-Y file(s).\"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=args.verbose)\nif (args.output_dir is not None and os.path.isdir(args.output_dir)) and args.inplace is False:\nxprint('Creating copy of file in output directory', kind='info', verbosity=args.verbose)\nif args.txt_suffix is not None:\nout_path = os.path.join(args.output_dir, f'{basename}_{args.txt_suffix}{suffix}')\nelse:\nout_path = os.path.join(args.output_dir, f'{basename}_despk{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=args.verbose,\n)\nos.remove(out_path)\ncopy2(in_path, out_path)\npath = out_path\nelif args.output_dir is None and args.inplace is True:\nxprint('Updating trace header inplace', kind='info', verbosity=args.verbose)\npath = in_path\nwith segyio.open(path, 'r+', strict=False, ignore_geometry=True) as src:\n# read common metadata\ndt = segyio.tools.dt(src) / 1000  # sample rate [ms]\ntwt = src.samples  # two way travel time (TWTT) [ms]\n# field record number\nfldr = src.attributes(segyio.TraceField.FieldRecord)[:]\n# get DelayRecordingTimes from trace headers\ndelrt = src.attributes(args.byte_delay)[:]  # segyio.TraceField.DelayRecordingTime\n# find indices where DelayRecordingTime changes\ndelrt_idx = np.where(np.roll(delrt, 1) != delrt)[0]\nif delrt_idx.size != 0 and delrt_idx[0] == 0:\ndelrt_idx = delrt_idx[1:]\nglobal data_src, data\n# get seismic data [amplitude]; transpose to fit numpy data structure\ndata_src = src.trace.raw[:].T  # eager version (completely read into memory)\nif data_src.shape[-1] &lt; args.window_traces:\n# input SEG-Y file is too small for despiking\nreturn path, None, None, dt, twt, fldr, delrt\nelse:\ndata = data_src.copy()\n# split seismic section according to DelayRecordingTime\nif args.use_delay and len(delrt_idx) &gt;= 1:\nxprint('Splitting seismic section using `delrt`', kind='info', verbosity=args.verbose)\n# split seismic section based on delrt\ndata_splits = np.array_split(data, delrt_idx, axis=1)\ndata_despiked_list = []\n# despike individual despke splits\nfor d in data_splits:\nd_despiked = despike_2D(\nd,\nwindow=args.window_time,\ndt=dt,\noverlap=args.window_overlap,\nntraces=args.window_traces,\nmode=args.mode,\nthreshold=args.threshold_factor,\nout=args.out_amplitude,\nverbosity=args.verbose,\n)\ndata_despiked_list.append(d_despiked)\n# merge despiked splits into single array\ndata_despiked = np.concatenate(data_despiked_list, axis=1, dtype=data.dtype)\nelse:\n# remove spikes in seismic section\ndata_despiked = despike_2D(\ndata,\nwindow=args.window_time,\ndt=dt,\noverlap=args.window_overlap,\nntraces=args.window_traces,\nmode=args.mode,\nthreshold=args.threshold_factor,\nout=args.out_amplitude,\nverbosity=args.verbose,\n)\n# sanity check\nmsg = 'Input and output data must be of same shape'\nassert data.shape == data_despiked.shape, msg\n# set output amplitudes (transpose to fit SEG-Y format)\nsrc.trace = np.ascontiguousarray(data_despiked.T, dtype=data.dtype)\n# update textual header\ntext = get_textual_header(path)\ntext_updated = add_processing_info_header(text, 'DESPIKE', prefix='_TODAY_', newline=True)\nwrite_textual_header(path, text_updated)\nreturn path, data_src, data_despiked\n</code></pre>"},{"location":"api/api_merge_segys/","title":"<code>merge_segys.py</code>","text":"<p>Utility script to merge short SEG-Y file(s) with neighboring ones.</p>"},{"location":"api/api_merge_segys/#pseudo_3D_interpolation.merge_segys.parse_trace_headers","title":"<code>parse_trace_headers(segyfile)</code>","text":"<p>Parse the SEG-Y file trace headers into a pandas DataFrame.</p> <p>Parameters:</p> <ul> <li> segyfile             (<code>segyio.SegyFile</code>)         \u2013 <p>Input SEG-Y file.</p> </li> </ul> <p>Returns:</p> <ul> <li> df(            <code>pandas.DataFrame</code> )        \u2013 <p>Dataframe of parsed trace headers.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\merge_segys.py</code> <pre><code>def parse_trace_headers(segyfile):\nr\"\"\"\n    Parse the SEG-Y file trace headers into a pandas DataFrame.\n    Parameters\n    ----------\n    segyfile : segyio.SegyFile\n        Input SEG-Y file.\n    Returns\n    -------\n    df : pandas.DataFrame\n        Dataframe of parsed trace headers.\n    \"\"\"\n# get number of traces\nn_traces = segyfile.tracecount\n# get all header keys\nheaders = segyio.tracefield.keys\n# initialize dataframe with trace id as index and headers as columns\ndf = pd.DataFrame(index=range(1, n_traces + 1), columns=headers.keys())\n# Fill dataframe with all header values\nfor k, v in headers.items():\ndf[k] = segyfile.attributes(v)[:]\nreturn df\n</code></pre>"},{"location":"api/api_merge_segys/#pseudo_3D_interpolation.merge_segys.get_files_to_merge","title":"<code>get_files_to_merge(list_segys, fsize_kB=2000, suffix='sgy', verbosity=1)</code>","text":"<p>Create list holding tuples of files to merge.</p> <p>Parameters:</p> <ul> <li> list_segys             (<code>list</code>)         \u2013 <p>Input list of SEG-Y files.</p> </li> <li> fsize_kB             (<code>float, int, optional</code>)         \u2013 <p>Threshold filesize below which files will be merged together (default: 2000 kB).</p> </li> <li> verbosity             (<code>int, optional</code>)         \u2013 <p>Print verbosity (default: 1).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code>         \u2013 <p>List of tuples of files to merge.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\merge_segys.py</code> <pre><code>def get_files_to_merge(list_segys, fsize_kB=2000, suffix='sgy', verbosity=1):\n\"\"\"\n    Create list holding tuples of files to merge.\n    Parameters\n    ----------\n    list_segys : list\n        Input list of SEG-Y files.\n    fsize_kB : float, int, optional\n        Threshold filesize below which files will be merged together (default: 2000 kB).\n    verbosity : int, optional\n        Print verbosity (default: 1).\n    Returns\n    -------\n    list\n        List of tuples of files to merge.\n    \"\"\"\nlist_small_segys = []\nfor idx, file in enumerate(list_segys):\nfsize = os.path.getsize(file) / 1024  # KB\nif fsize &lt; fsize_kB:  # KB -&gt; ~50 traces\nlist_small_segys.append(\n(\nidx,  # index of file in `list_segys`\nidx,  # filename\nfsize,  # file size (KB)\nidx - 1,  # filename (previously recorded)\nidx + 1,  # filename (recorded afterwards)\n)\n)\n# create dataframe\ndf_small_segys = pd.DataFrame(\nlist_small_segys, columns=['list_idx', 'file', 'size_kb', 'file_pre', 'file_post']\n)\nxprint(\nf'Found &lt; {len(df_small_segys)} &gt; files smaller than {fsize_kB} KB',\nkind='info',\nverbosity=verbosity,\n)\n# find consecutively recorded files\ndf_small_segys['diff_idx'] = np.append(0, np.diff(df_small_segys['list_idx']))\n# aggregate consecutive files\ns = df_small_segys['diff_idx'].ne(1).cumsum()\ngrouped = df_small_segys.groupby(s).agg(\n{\n'list_idx': 'first',\n'file': lambda x: tuple(x),  # ','.join(x),\n'size_kb': 'first',\n'file_pre': lambda x: tuple(x),\n'file_post': lambda x: tuple(x),\n}\n)\nxprint(f'Remaining &lt; {len(grouped)} &gt; files after groupby', kind='info', verbosity=verbosity)\ndef _to_merge(row):\nif isinstance(row[1][0], str):\ndiff_pre = int(row[1][0].split('_')[0]) - int(row[3][0].split('_')[0])\ndiff_post = int(row[4][0].split('_')[0]) - int(row[1][0].split('_')[0])\nelif isinstance(row[1][0], int):\ndiff_pre = row[1][0] - row[3][0]\ndiff_post = row[4][0] - row[1][0]\nif diff_pre &lt; diff_post:\nreturn tuple(sorted(set(row[3] + row[1])))\nelse:\nreturn tuple(sorted(set(row[1] + row[4])))\n# create tuple of files to merge\ngrouped['to_merge'] = grouped.apply(_to_merge, axis=1)\nindices2merge = grouped['to_merge'].to_list()\n# get file paths from general list by calculated indices\nfiles2merge = [list_segys[tup[0] : tup[-1] + 1] for tup in indices2merge]\nxprint(f'Prepared &lt; {len(files2merge)} &gt; merged files', kind='info', verbosity=verbosity)\nreturn files2merge\n</code></pre>"},{"location":"api/api_merge_segys/#pseudo_3D_interpolation.merge_segys.wrapper_merge_segys","title":"<code>wrapper_merge_segys(file_list, txt_suffix='merge', verbosity=1)</code>","text":"<p>Merge SEG-Y files provided as list of input paths. Potential gaps will be interpolated (linear) and duplicates removed while keeping the last (aka newest) occurrence.</p> <p>Parameters:</p> <ul> <li> file_list             (<code>tuple, list</code>)         \u2013 <p>Iterable of files to be merged.</p> </li> <li> txt_suffix             (<code>str, optional</code>)         \u2013 <p>Filename suffix for merged file (default: 'merge')</p> </li> <li> verbosity             (<code>int, optional</code>)         \u2013 <p>Verbosity constant for stdout printing (default: 1).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\merge_segys.py</code> <pre><code>def wrapper_merge_segys(file_list, txt_suffix: str = 'merge', verbosity=1):\n\"\"\"\n    Merge SEG-Y files provided as list of input paths.\n    Potential gaps will be interpolated (linear) and duplicates removed while\n    keeping the last (aka newest) occurrence.\n    Parameters\n    ----------\n    file_list : tuple,list\n        Iterable of files to be merged.\n    txt_suffix : str, optional\n        Filename suffix for merged file (default: 'merge')\n    verbosity : int, optional\n        Verbosity constant for stdout printing (default: 1).\n    \"\"\"\nfirst_file = file_list[0]\nbasepath, filename = os.path.split(first_file)\nbasename, suffix = os.path.splitext(filename)\nout_name = f'{basename}_{txt_suffix}{suffix}'\nout_file = os.path.join(basepath, out_name)\ntrace_headers_list = []\nspecs = []\nheader_bin = []\nswdep_list = []\ndata_list = []\ntrace_header_to_check = {\n'tracl': segyio.TraceField.TRACE_SEQUENCE_LINE,\n'tracr': segyio.TraceField.TRACE_SEQUENCE_FILE,\n'fldr': segyio.TraceField.FieldRecord,\n'ns': segyio.TraceField.TRACE_SAMPLE_COUNT,\n'nt': segyio.TraceField.TRACE_SAMPLE_INTERVAL,\n}\ntrace_header_dict = dict(\nzip(\nlist(trace_header_to_check.keys()),\n[np.array([], dtype='int')] * len(trace_header_to_check),\n)\n)\ntrace_header_datetime = {\n'year': segyio.TraceField.YearDataRecorded,\n'day': segyio.TraceField.DayOfYear,\n'hour': segyio.TraceField.HourOfDay,\n'minute': segyio.TraceField.MinuteOfHour,\n'sec': segyio.TraceField.SecondOfMinute,\n}\ntrace_header_datetime_dict = dict(\nzip(list(trace_header_datetime.keys()), [np.array([])] * len(trace_header_datetime))\n)\ntrace_header_coords = {\n'sx': segyio.TraceField.SourceX,\n'sy': segyio.TraceField.SourceY,\n'gx': segyio.TraceField.GroupX,\n'gy': segyio.TraceField.GroupY,\n'cdpx': segyio.TraceField.CDP_X,\n'cdpy': segyio.TraceField.CDP_Y,\n}\ntrace_header_coords_dict = dict(\nzip(list(trace_header_coords.keys()), [np.array([])] * len(trace_header_coords))\n)\nlist_ntraces = []\nfor f in file_list:\nxprint(f'Processing file &lt; {os.path.split(f)[-1]} &gt;', kind='info', verbosity=verbosity)\nwith segyio.open(f, 'r', strict=False, ignore_geometry=True) as file:\nlist_ntraces.append(file.tracecount)\nspecs.append(segyio.tools.metadata(file))\ntrace_headers_list.append(parse_trace_headers(file))\n# load binary header\nheader_bin.append(file.bin)\n# check binary header elements\nbinary_equal = all([header_bin[0] == b for b in header_bin])\nif not binary_equal:\nraise IOError(\n'Specified SEG-Y files have different binary headers. No easy merging possible, please check your data!'\n)\n# load trace header values used for gap checking\nxprint('Check trace headers', kind='debug', verbosity=verbosity)\nfor key, field in trace_header_to_check.items():\n# print(key, field)\n# get already stored values (initially empty array)\nvalues = trace_header_dict.get(key)\n# combine previous and new values\ntr_header_attr = np.sort(\nnp.concatenate((values, file.attributes(field)[:]), axis=None)\n)\n# update dictionary with combined values\ntrace_header_dict[key] = tr_header_attr\n# load datetimes from trace headers\nxprint('Load timestamps from trace headers', kind='debug', verbosity=verbosity)\nfor key, field in trace_header_datetime.items():\n# get already stored values (initially empty array)\ndatetime_values = trace_header_datetime_dict.get(key)\n# combine previous and new values\ntr_header_datetime = np.concatenate(\n(datetime_values, file.attributes(field)[:]), axis=None\n)\n# update dictionary with combined values\ntrace_header_datetime_dict[key] = tr_header_datetime\n# load coordinates from trace headers\nxprint('Load coordinates from headers', kind='debug', verbosity=verbosity)\nfor key, field in trace_header_coords.items():\n# get already stored values (initially empty array)\ncoords = trace_header_coords_dict.get(key)\n# combine previous and new values\ntr_header_coords = np.concatenate((coords, file.attributes(field)[:]), axis=None)\n# update dictionary with combined values\ntrace_header_coords_dict[key] = tr_header_coords\n# SourceWaterDepth\nxprint('Load SourceWaterDepth from headers', kind='debug', verbosity=verbosity)\nswdep_list.extend(file.attributes(segyio.TraceField.SourceWaterDepth)[:])\n# trace data (as 2D np.ndarray)\nxprint('Load seismic section', kind='debug', verbosity=verbosity)\ndata_list.append(file.trace.raw[:])\nxprint('Merge trace headers', kind='debug', verbosity=verbosity)\n# concat trace headers\ntrace_headers = pd.concat(trace_headers_list, ignore_index=True)\n# create index from TRACE_SEQUENCE_LINE and get not existing traces (aka gaps)\ntrace_headers.set_index(trace_headers['TRACE_SEQUENCE_LINE'], inplace=True)\n# drop duplicate traces from different files (only *exact* matches!)\n# trace_headers.drop_duplicates(keep='last', inplace=True) # keep \"newer\" record\nmask_overlapping_duplicates = trace_headers.duplicated(keep='last')\n# drop duplicate traces from same file (!)\ncol_subset = list(trace_headers.columns)\ncol_subset.remove('TRACE_SEQUENCE_FILE')\n# trace_headers.drop_duplicates(subset=col_subset, keep='first', inplace=True)\nmask_internal_duplicates = trace_headers.duplicated(subset=col_subset, keep='first')\nmask = (mask_overlapping_duplicates + mask_internal_duplicates).astype('bool')\n# select non-duplicates\ntrace_headers = trace_headers[~mask]\nnduplicates = np.count_nonzero(mask)\nif nduplicates &gt; 0:\nxprint(f'Removed &lt; {nduplicates} &gt; duplicates', kind='debug', verbosity=verbosity)\n# create gap records\ntrace_headers = trace_headers.reindex(\npd.RangeIndex(\ntrace_headers.iloc[0]['TRACE_SEQUENCE_LINE'],\ntrace_headers.iloc[-1]['TRACE_SEQUENCE_LINE'] + 1,\n)\n)\nxprint('Merge seismic data', kind='debug', verbosity=verbosity)\ndata = np.concatenate(data_list, axis=0)\n# remove duplicates\ndata = data[~mask]\n# get gap indices\nidx_gaps = pd.isnull(trace_headers).any(1).to_numpy().nonzero()[0]\n# if gaps are present\nif len(idx_gaps) &gt; 0:\n# interpolate gaps in merged trace header\nxprint('Interpolate gaps in merged trace header', kind='debug', verbosity=verbosity)\ntrace_headers_interp = trace_headers.interpolate(method='linear').astype('int32')\ntrace_headers_interp['TRACE_SEQUENCE_FILE'] = np.arange(\n1, trace_headers_interp.shape[0] + 1, 1\n)\ntrace_headers_interp.rename(columns=segyio.tracefield.keys, inplace=True)\nxprint('Fill gaps with zero traces', kind='debug', verbosity=verbosity)\nidx_gaps_first = np.nonzero(np.diff(idx_gaps) &gt; 1)[0] + 1\nif idx_gaps_first.size == 0:\nidx = [idx_gaps[0]]\nntr = [idx_gaps.size]\nelse:\nidx = idx_gaps[np.insert(idx_gaps_first, 0, 0)]\nntr = [a.size for a in np.split(idx_gaps, idx_gaps_first)]\nfor i, n in zip(idx, ntr):\ndummy_traces = np.zeros((n, data.shape[1]), dtype=data.dtype)\ndata = np.insert(data, i, dummy_traces, axis=0)\nelse:\ntrace_headers_interp = trace_headers\ntrace_headers_interp['TRACE_SEQUENCE_FILE'] = np.arange(\n1, trace_headers_interp.shape[0] + 1, 1\n)\ntrace_headers_interp = trace_headers_interp.rename(columns=segyio.tracefield.keys)\n# init output SEG-Y\nspec = specs[0]\nspec.samples = specs[0].samples  # get sample TWT from first file!\nspec.tracecount = data.shape[0]  # get number of trace from combined array\n# save merged SEG-Y\nxprint('Write merged SEG-Y to disk', kind='debug', verbosity=verbosity)\nwith segyio.open(first_file, 'r', ignore_geometry=True) as src:\nwith segyio.create(out_file, spec) as dst:\ndst.text[0] = src.text[0]\ndst.bin = src.bin\nfor i, trheader in zip(range(0, spec.tracecount + 1), dst.header):\ntrheader.update(trace_headers_interp.iloc[i].to_dict())\ndst.trace = np.ascontiguousarray(data, dtype=data.dtype)\n# update textual header\ntext = get_textual_header(out_file)\n## add info about new CRS\ninfo = ','.join([os.path.split(f)[-1].split('.')[0] for f in file_list])\ntext_updated = add_processing_info_header(text, info, prefix='MERGED')\nwrite_textual_header(out_file, text_updated)\n# save auxiliary file\nout_file_aux = out_file.split('.')[0] + '.parts'\nwith open(out_file_aux, 'w', newline='\\n') as fout:\nfout.write(f'The merged SEG-Y file &lt; {out_name} &gt; contains the following files:\\n')\nfor f, ntr in zip(file_list, list_ntraces):\nfout.write(f'    - {os.path.split(f)[-1]}    {ntr:&gt;6d} trace(s)\\n')\nstring = f'Trace duplicates (different files):    {np.count_nonzero(mask_overlapping_duplicates):&gt;3d}\\n'\nstring += f'Trace duplicates (within single file): {np.count_nonzero(mask_internal_duplicates):&gt;3d}\\n'\nfout.write(string)\n</code></pre>"},{"location":"api/api_merge_segys/#pseudo_3D_interpolation.merge_segys.main","title":"<code>main(argv=sys.argv)</code>","text":"<p>Merge small SEG-Y files with others.</p> Source code in <code>pseudo_3D_interpolation\\merge_segys.py</code> <pre><code>def main(argv=sys.argv):  # noqa\n\"\"\"Merge small SEG-Y files with others.\"\"\"\nTIMESTAMP = datetime.datetime.now().isoformat(timespec='seconds').replace(':', '')\nSCRIPT = os.path.basename(__file__).split(\".\")[0]\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])  # exclude filename parameter at position 0\nverbosity = args.verbose\n# check input file(s)\nin_path = args.input_dir\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nif suffix == '':\nbasepath = in_path\nbasename, suffix = None, None  # noqa\n# (1) input directory (multiple files)\nif os.path.isdir(in_path):\npattern = '*' + f'{args.filename_suffix}' + f'.{args.suffix}'\nfile_list = glob.glob(os.path.join(in_path, pattern))\n# (2) file input is datalist (multiple files)\nelif os.path.isfile(in_path) and (suffix == '.txt'):\nwith open(in_path, 'r') as datalist:\nfile_list = datalist.readlines()\nfile_list = [\nos.path.join(basepath, line.rstrip())\nif os.path.split(line.rstrip()) not in ['', '.']\nelse line.rstrip()\nfor line in file_list\n]\nif len(file_list) &gt; 0:\n# redirect stdout to logfile\nwith open(\nos.path.join(basepath, f'{TIMESTAMP}_{SCRIPT}.log'),\n'w',\nnewline='\\n',\n) as f:\nwith redirect_stdout(f):\n# get list of tuples of files to merge\nfiles2merge = get_files_to_merge(file_list, verbosity=verbosity)\nxprint(\nf'Processing total of &lt; {len(files2merge)} &gt; files',\nkind='info',\nverbosity=verbosity,\n)\n# merge SEG-Y(s)\nfor i, file_list in enumerate(files2merge):\nxprint(f'Merging {i+1}th set of files', kind='info', verbosity=verbosity)\nwrapper_merge_segys(file_list, verbosity=verbosity)\nelse:\nsys.exit('No input files to process.')\n</code></pre>"},{"location":"api/api_mistie_correction_segy/","title":"<code>mistie_correction_segy.py</code>","text":"<p>Compensate mistie for SEG-Y file(s) via cross-correlation of nearest traces of intersecting lines.</p>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.create_geometries","title":"<code>create_geometries(df, coord_cols=['x', 'y'], idx_col='line_id')</code>","text":"<p>Create <code>shapely</code> geometries from input dataframe.</p> <p>Parameters:</p> <ul> <li> df             (<code>pd.DataFrame</code>)         \u2013 <p>Input dataframe holding merged navigation of all SEG-Y files to process.</p> </li> <li> coord_cols             (<code>list, optional</code>)         \u2013 <p>Column names of X and Y coordinates (default: [<code>x</code>, <code>y</code>]).</p> </li> <li> idx_col             (<code>str, optional</code>)         \u2013 <p>Column name holding corresponding line indices (default: <code>line_id</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> linestrings(            <code>np.ndarray(n)</code> )        \u2013 <p>Array of LineString geometries (<code>shapely.geometry.LineString</code>) of <code>n</code> input lines (i.e. SEG-Y files).</p> </li> <li> points_split(            <code>np.ndarray(n)</code> )        \u2013 <p>Array of coordinate arrays ((k,2)) with X and Y columns where <code>n</code> is the number of lines (i.e. SEG-Y files) and <code>k</code> the number of vertices (i.e. shotpoints) per line.</p> </li> <li> points_line_idx(            <code>np.ndarray (n</code> )        \u2013 <p>Array of line indices for each point (i.e. shotpoint).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>@timeit\ndef create_geometries(\ndf,\ncoord_cols: list = ['x', 'y'],\nidx_col: str = 'line_id',\n):\n\"\"\"\n    Create `shapely` geometries from input dataframe.\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input dataframe holding merged navigation of all SEG-Y files to process.\n    coord_cols : list, optional\n        Column names of X and Y coordinates (default: [`x`, `y`]).\n    idx_col : str, optional\n        Column name holding corresponding line indices (default: `line_id`).\n    Returns\n    -------\n    linestrings : np.ndarray (n,)\n        Array of LineString geometries (`shapely.geometry.LineString`) of `n`\n        input lines (i.e. SEG-Y files).\n    points_split : np.ndarray (n,)\n        Array of coordinate arrays ((k,2)) with X and Y columns where `n` is the\n        number of lines (i.e. SEG-Y files) and `k` the number of vertices\n        (i.e. shotpoints) per line.\n    points_line_idx : np.ndarray (n*k,)\n        Array of line indices for each point (i.e. shotpoint).\n    \"\"\"\n# create LineString geometries (using line indices)\nlinestrings = shapely.linestrings(df[coord_cols].to_numpy(), indices=df[idx_col].to_numpy())\n# prepare geometries (faster predicate operations)\nshapely.prepare(linestrings)\n# create Point array (*no* geometry needed for custom distance function!)\npoints = df[coord_cols].to_numpy()\npoints_line_idx = df[idx_col].to_numpy()\n# split point array into array corresponding to individual lines\npoints_split = np.asarray(\nnp.split(points, np.nonzero(np.diff(points_line_idx) &gt; 0)[0] + 1), dtype='object'\n)\nreturn linestrings, points_split, points_line_idx\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.find_intersections","title":"<code>find_intersections(linestrings)</code>","text":"<p>Find all intersections of input array of LineStrings. Returns array of Points (<code>shapely.geometry.Point</code>) with array of their indices and corresponding line intersection indices array (referring to input).</p> <p>Parameters:</p> <ul> <li> linestrings             (<code>np.ndarray</code>)         \u2013 <p>Array of <code>shapely.geometry.LineString</code> geometries for each SEG-Y file from:</p> <ol> <li>navigation file or</li> <li>read from SEG-Y trace headers.</li> </ol> </li> </ul> <p>Returns:</p> <ul> <li> intersections_pts_exploded(            <code>np.ndarray</code> )        \u2013 <p>Array of exploded intersection points (<code>shapely.geometry.Point</code>).</p> </li> <li> intersections_pts_exploded_idx(            <code>np.ndarray</code> )        \u2013 <p>Index array of exploded intersection points (referring to rows of <code>line_intersections_idx</code>).</p> </li> <li> line_intersections_idx(            <code>np.ndarray</code> )        \u2013 <p>Array of line intersection indices referring to input linestring array.</p> </li> </ul>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.find_intersections--notes","title":"Notes","text":"<p>The point array returns the exploded version of detected geometries (i.e. separated segments of multipart features). For multiple intersections of a line pair, this results in multiple geometries with identical reference indices and, thus, could result in a larger number of output features.</p> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>@timeit\ndef find_intersections(linestrings):\n\"\"\"\n    Find all intersections of input array of LineStrings.\n    Returns array of Points (`shapely.geometry.Point`) with array of their indices and\n    corresponding line intersection indices array (referring to input).\n    Parameters\n    ----------\n    linestrings : np.ndarray\n        Array of `shapely.geometry.LineString` geometries for each SEG-Y file from:\n            1. navigation file or\n            2. read from SEG-Y trace headers.\n    Returns\n    -------\n    intersections_pts_exploded : np.ndarray\n        Array of exploded intersection points (`shapely.geometry.Point`).\n    intersections_pts_exploded_idx : np.ndarray\n        Index array of exploded intersection points (referring to rows of `line_intersections_idx`).\n    line_intersections_idx : np.ndarray\n        Array of line intersection indices referring to input linestring array.\n    Notes\n    -----\n    The point array returns the exploded version of detected geometries\n    (i.e. separated segments of multipart features).\n    For multiple intersections of a line pair, this results in multiple\n    geometries with identical reference indices and, thus, could result\n    in a larger number of output features.\n    \"\"\"\n# build STRTree\ntree = shapely.STRtree(linestrings)\n# query STRTree (bulk)\nline_inter_idx = tree.query(linestrings, predicate='intersects').T  # FIXME\n# sort intersection indice pairs (row-wise)\nline_inter_idx_sorted = np.sort(line_inter_idx, axis=1)\n# get unique combinations of intersecting lines\nuniq, uniq_idx, uniq_inv, uniq_cnt = np.unique(\nline_inter_idx_sorted, axis=0, return_index=True, return_inverse=True, return_counts=True\n)\n# mask = (uniq_cnt != 1) -&gt; mask self-intersections\nline_inter_idx_uniq = uniq[(uniq_cnt != 1)]\n# (0) get intersection points (vectorized)\n# -&gt; unpack colums (LINESTRING_1, LINESTRING_2)\nintersections_pts = shapely.intersection(*linestrings[line_inter_idx_uniq].T)\n# (1) explode multipart geometries (MULTIPOINT, GEOMETRYCOLLECTION)\nintersections_pts_single, intersections_pts_single_idx = shapely.get_parts(\nintersections_pts, return_index=True\n)\n# (2) extract points from LINESTRING\ntmp = [\nnp.asarray([shapely.get_point(g, j) for j in range(shapely.get_num_points(g))])\nif shapely.get_type_id(g) == GEOMETRY.get('LINESTRING')\nelse np.asarray([g])\nfor g in intersections_pts_single\n]\n# get number of points in arrays (POINT: 1, LINSTRING: &gt; 1)\ntmp_len = [a.size for a in tmp]\n# create index array for intersection loop\nintersections_pts_exploded_idx = np.repeat(intersections_pts_single_idx, repeats=tmp_len)\n# merge all points into 1D array\nintersections_pts_exploded = np.concatenate(tmp)\n# get (n,2) array of indices of intersection lines (matching exploded intersections!)\nline_intersections_idx = line_inter_idx_uniq[intersections_pts_exploded_idx]\nreturn intersections_pts_exploded, intersections_pts_exploded_idx, line_intersections_idx\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.nearest_intersection_vertices","title":"<code>nearest_intersection_vertices(points_split, intersections_pts_exploded, line_intersections_idx)</code>","text":"<p>Return nearest intersection vertex indices and corresponding distances to intersection point for both intersecting lines referenced in input index array <code>line_intersections_idx</code>.</p> <p>Parameters:</p> <ul> <li> points_split             (<code>np.ndarray(n)</code>)         \u2013 <p>Array of coordinate arrays ((k,2)) with X and Y columns where <code>n</code> is the number of lines (i.e. SEG-Y files) and <code>k</code> the number of vertices (i.e. shotpoints) per line.</p> </li> <li> intersections_pts_exploded             (<code>np.ndarray(j)</code>)         \u2013 <p>Array of intersection points (exploded).</p> </li> <li> line_intersections_idx             (<code>np.ndarray</code>)         \u2013 <p>Array of line intersection indices referring to input point array.</p> </li> </ul> <p>Returns:</p> <ul> <li> nearest_0(            <code>np.ndarray</code> )        \u2013 <p>Array of nearest line vertex indices and distance for each intersection based on first line (i.e. column 0 in <code>line_intersections_idx</code>).</p> </li> <li> nearest_1(            <code>np.ndarray</code> )        \u2013 <p>Array of nearest line vertex indices and distance for each intersection based on second line (i.e. column 1 in <code>line_intersections_idx</code>).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>@timeit\ndef nearest_intersection_vertices(\npoints_split, intersections_pts_exploded, line_intersections_idx\n):  # noqa\n\"\"\"\n    Return nearest intersection vertex indices and corresponding distances to\n    intersection point for both intersecting lines referenced in input index\n    array `line_intersections_idx`.\n    Parameters\n    ----------\n    points_split : np.ndarray (n,)\n        Array of coordinate arrays ((k,2)) with X and Y columns where `n` is the\n        number of lines (i.e. SEG-Y files) and `k` the number of vertices\n        (i.e. shotpoints) per line.\n    intersections_pts_exploded : np.ndarray (j,)\n        Array of intersection points (exploded).\n    line_intersections_idx : np.ndarray\n        Array of line intersection indices referring to input point array.\n    Returns\n    -------\n    nearest_0 : np.ndarray\n        Array of nearest line vertex indices and distance for each intersection\n        based on first line (i.e. column 0 in `line_intersections_idx`).\n    nearest_1 : np.ndarray\n        Array of nearest line vertex indices and distance for each intersection\n        based on second line (i.e. column 1 in `line_intersections_idx`).\n    \"\"\"\n# prepare intersection points\npts_x = shapely.get_x(intersections_pts_exploded)\npts_y = shapely.get_y(intersections_pts_exploded)\npts_intersections = np.concatenate((pts_x[:, None], pts_y[:, None]), axis=1)\n# select intersection line points\npoints_sel = points_split[line_intersections_idx]\n# calculate distances between LineString verties and intersection point\n# -&gt; return (nearest index, distance)\nnearest_0 = np.empty((pts_intersections.shape[0], 2), dtype=np.float32)\nnearest_1 = np.empty((pts_intersections.shape[0], 2), dtype=np.float32)\nfor k, (pts, pi) in enumerate(zip(points_sel, pts_intersections)):\nnearest_0[k] = euclidian_distance(pts[0], pi)\nnearest_1[k] = euclidian_distance(pts[1], pi)\nreturn nearest_0, nearest_1\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.load_trace","title":"<code>load_trace(path, idx_tr, check_bad_traces=False, ntraces2mix=3)</code>","text":"<p>Load and return single trace from SEG-Y file.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013 <p>File path to SEG-Y on disk.</p> </li> <li> idx_tr             (<code>int</code>)         \u2013 <p>Index of trace to load (starting at 0).</p> </li> <li> check_bad_traces             (<code>bool, optional</code>)         \u2013 <p>Simple check if loaded trace is too bad/noisy by testing if amplitude mean or rescaled trace is greater than 0.4 (default: <code>False</code>).</p> </li> <li> ntraces2mix             (<code>int, optional</code>)         \u2013 <p>Number of traces to load and average if bad trace was detected. The \"bad\" trace is excluded from averaging (default: <code>3</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> trace(            <code>np.ndarray(nsamples)</code> )        \u2013 <p>Array of trace amplitudes.</p> </li> <li> dt(            <code>float</code> )        \u2013 <p>Sampling rate (in ms).</p> </li> <li> twt(            <code>np.ndarray</code> )        \u2013 <p>Array of samples with appropriate intervals (determined by <code>dt</code>).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>def load_trace(path, idx_tr: int, check_bad_traces: bool = False, ntraces2mix: int = 3):\n\"\"\"\n    Load and return single trace from SEG-Y file.\n    Parameters\n    ----------\n    path : str\n        File path to SEG-Y on disk.\n    idx_tr : int\n        Index of trace to load (starting at 0).\n    check_bad_traces : bool, optional\n        Simple check if loaded trace is too bad/noisy by testing if amplitude\n        mean or rescaled trace is greater than 0.4 (default: `False`).\n    ntraces2mix : int, optional\n        Number of traces to load and average if bad trace was detected.\n        The \"bad\" trace is excluded from averaging (default: `3`).\n    Returns\n    -------\n    trace : np.ndarray (nsamples,)\n        Array of trace amplitudes.\n    dt : float\n        Sampling rate (in ms).\n    twt : np.ndarray\n        Array of samples with appropriate intervals (determined by `dt`).\n    \"\"\"\nwith segyio.open(path, 'r', strict=False, ignore_geometry=True) as file:\nntraces = file.tracecount  # total number of traces\ndt = segyio.tools.dt(file) / 1000  # sample rate [ms]\ntwt = file.samples  # two way travel time (TWTT) [ms]\n# load trace\ntrace = file.trace[idx_tr]\ntry:\nm = np.mean(rescale(trace))\nif (m &gt; 0.4) and check_bad_traces:\n# prepare number of neighboring traces to mix\nntraces2mix = ntraces2mix if ntraces2mix % 2 != 0 else ntraces2mix + 1\nnmix_left = ntraces2mix // 2\nnmix_right = ntraces2mix - nmix_left\n# load ``ntraces2mix`` from file\nidx_left = idx_tr - nmix_left if idx_tr - nmix_left &gt;= 0 else 0\nidx_right = idx_tr + nmix_right if idx_tr + nmix_right &lt;= ntraces else ntraces\ntrace = file.trace.raw[slice(idx_left, idx_right)]\n# exclude bad/noisy trace and average neighboring traces\ntrace = np.delete(trace, nmix_left, axis=0).mean(axis=0)\nexcept IndexError as err:\nprint(path)\nprint(idx_tr)\nprint('nmix_left, nmix_right:', nmix_left, nmix_right)\nprint(idx_tr)\nraise IndexError(err)\nif 'env' not in path:\ntrace = envelope(trace)  # calculate envelope (via Hilbert transformation)\nreturn trace, dt, twt\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.compute_misties","title":"<code>compute_misties(segy_dir, line_intersections_names, line_intersections_idx, nearest_0, nearest_1, win=(False, False), quality=0, lookup_df=None, lookup_col=None, check_bad_traces=False, ntraces2mix=3, return_ms=False, return_coeff=False, verbosity=1)</code>","text":"<p>Compute mistie at line intersections (in number of samples). Returns tuple of single shift per input line and shift per line vertex. Option to additionally return shift in milliseconds (ms) and/or cross-correlation coefficients (i.e. quality indicator).</p> <p>Parameters:</p> <ul> <li> segy_dir             (<code>str</code>)         \u2013 <p>Directory path of SEG-Y files.</p> </li> <li> line_intersections_names             (<code>np.ndarray</code>)         \u2013 <p>Array of string objects of SEG-Y names.</p> </li> <li> line_intersections_idx             (<code>np.ndarray</code>)         \u2013 <p>Index array of SEG-Y names.</p> </li> <li> nearest_0             (<code>np.ndarray</code>)         \u2013 <p>Array of nearest line vertex indices and distance for each intersection based on first line (i.e. column 0 in <code>line_intersections_idx</code>).</p> </li> <li> nearest_1             (<code>np.ndarray</code>)         \u2013 <p>Array of nearest line vertex indices and distance for each intersection. based on second line (i.e. column 1 in <code>line_intersections_idx</code>).</p> </li> <li> win             (<code>tuple, optional</code>)         \u2013 <p>Upper and lower limit of two-way traveltime (TWT) window used for cross-correlation. If (False, False), full overlapping trace range is used.</p> </li> <li> quality             (<code>float, optional</code>)         \u2013 <p>Threshold quality of cross-correlations to include to solve for global offsets (default: <code>0</code>, i.e. only positive correlations).</p> </li> <li> lookup_df             (<code>pd.DataFrame, optional</code>)         \u2013 <p>Dataframe with index set corresponding to names in <code>line_intersections_names</code> and column of acutal file names to use. Default: None (i.e. using file name from provided array).</p> </li> <li> lookup_col             (<code>str, optional</code>)         \u2013 <p>Column name with reference SEG-Y file names (default: <code>None</code>).</p> </li> <li> check_bad_traces             (<code>bool, optional</code>)         \u2013 <p>Simple check if loaded trace is too bad/noisy by testing if amplitude mean or rescaled trace is greater than 0.4 (default: <code>False</code>).</p> </li> <li> ntraces2mix             (<code>int, optional</code>)         \u2013 <p>Number of traces to load and average if bad trace was detected. The \"bad\" trace is excluded from averaging (default: <code>3</code>).</p> </li> <li> return_ms             (<code>bool, optional</code>)         \u2013 <p>Return shift in milliseconds (ms) (default: <code>False</code>).</p> </li> <li> return_coeff             (<code>bool, optional</code>)         \u2013 <p>Return cross-correlation coefficients (i.e. quality indicator). Default: False.</p> </li> <li> verbosity             (<code>int</code>)         \u2013 <p>Control level of output messages (default: <code>1</code>, i.e. <code>info</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> offsets(            <code>np.ndarray</code> )        \u2013 <p>Array of sample shift per line with shape (nlines,).</p> </li> <li> offsets_ms(            <code>np.ndarray</code> )        \u2013 <p>Array of shift (in milliseconds) per line with shape (nlines,).</p> </li> <li> coeff(            <code>np.ndarray</code> )        \u2013 <p>Array of quality indications (Pearson\u2019s correlation coefficient).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>@timeit\ndef compute_misties(\nsegy_dir,\nline_intersections_names,  # array (nintersections, 2)\nline_intersections_idx,  # index array (nintersections, 2)\nnearest_0,\nnearest_1,  # arrays (vertex indices, distance)\nwin: tuple = (False, False),  # upper/lower limit for CC\nquality: float = 0,  # correlation quality threshold\nlookup_df=None,  # lookup dataframe (e.g. datalist)\nlookup_col=None,  # lookup column in df\ncheck_bad_traces: bool = False,  # check for bad/noisy traces\nntraces2mix: int = 3,  # odd number of traces to mix\nreturn_ms: bool = False,  # return mistie shift in ms\nreturn_coeff: bool = False,  # return correlation coefficients\nverbosity: int = 1,\n):\n\"\"\"\n    Compute mistie at line intersections (in number of samples).\n    Returns tuple of single shift per input line and shift per line vertex.\n    Option to additionally return shift in milliseconds (ms) and/or\n    cross-correlation coefficients (i.e. quality indicator).\n    Parameters\n    ----------\n    segy_dir : str\n        Directory path of SEG-Y files.\n    line_intersections_names : np.ndarray\n        Array of string objects of SEG-Y names.\n    line_intersections_idx : np.ndarray\n        Index array of SEG-Y names.\n    nearest_0 : np.ndarray\n        Array of nearest line vertex indices and distance for each intersection\n        based on first line (i.e. column 0 in `line_intersections_idx`).\n    nearest_1 : np.ndarray\n        Array of nearest line vertex indices and distance for each intersection.\n        based on second line (i.e. column 1 in `line_intersections_idx`).\n    win : tuple, optional\n        Upper and lower limit of two-way traveltime (TWT) window used for\n        cross-correlation. If (False, False), full overlapping trace\n        range is used.\n    quality : float, optional\n        Threshold quality of cross-correlations to include to solve for\n        global offsets (default: `0`, i.e. only positive correlations).\n    lookup_df : pd.DataFrame, optional\n        Dataframe with index set corresponding to names in\n        `line_intersections_names` and column of acutal file names to use.\n        Default: None (i.e. using file name from provided array).\n    lookup_col : str, optional\n        Column name with reference SEG-Y file names (default: `None`).\n    check_bad_traces : bool, optional\n        Simple check if loaded trace is too bad/noisy by testing if amplitude\n        mean or rescaled trace is greater than 0.4 (default: `False`).\n    ntraces2mix : int, optional\n        Number of traces to load and average if bad trace was detected.\n        The \"bad\" trace is excluded from averaging (default: `3`).\n    return_ms : bool, optional\n        Return shift in milliseconds (ms) (default: `False`).\n    return_coeff : bool, optional\n        Return cross-correlation coefficients (i.e. quality indicator).\n        Default: False.\n    verbosity : int\n        Control level of output messages (default: `1`, i.e. `info`).\n    Returns\n    -------\n    offsets : np.ndarray\n        Array of sample shift per line with shape (nlines,).\n    offsets_ms : np.ndarray\n        Array of shift (in milliseconds) per line with shape (nlines,).\n    coeff : np.ndarray\n        Array of quality indications (Pearson\u2019s correlation coefficient).\n    \"\"\"\n# unpack window limits\nwin_upper, win_lower = win\n# allocate output arrays\nintersections_misties = np.empty((len(line_intersections_names),), dtype=np.int16)\nintersections_coeffs = np.empty((len(line_intersections_names),), dtype=np.float32)\n# loop over all intersection\nfor idx_intersection, lines in enumerate(\ntqdm(\nline_intersections_names,\ndesc='Computing misties',\nncols=80,\ntotal=len(line_intersections_names),\nunit_scale=True,\nunit=' intersections',\ndisable=True if verbosity &lt;= 1 else False,\n)\n):\n# set upper and lower window extent from user input\nwin_up, win_lo = win_upper, win_lower\n# input files\nline_0 = lines[0]\nline_1 = lines[1]\nif (lookup_df is not None) and (lookup_col != None):  # noqa\npath_0 = os.path.join(segy_dir, lookup_df.loc[line_0, 'line'])  # use lookup df\npath_1 = os.path.join(segy_dir, lookup_df.loc[line_1, 'line'])  # use lookup df\nelse:\npath_0 = os.path.join(segy_dir, line_0 + '.sgy')\npath_1 = os.path.join(segy_dir, line_1 + '.sgy')\n# (1) read trace(s) and auxiliary information\n## load trace(s) of first intersecting line\nidx_tr_0 = int(nearest_0[idx_intersection, 0])\ntrace_0, dt_0, twt_0 = load_trace(\npath_0, idx_tr_0, check_bad_traces=True, ntraces2mix=ntraces2mix\n)\n## load trace(s) of second intersecting line\nidx_tr_1 = int(nearest_1[idx_intersection, 0])\ntrace_1, dt_1, twt_1 = load_trace(\npath_1, idx_tr_1, check_bad_traces=True, ntraces2mix=ntraces2mix\n)\nassert dt_0 == dt_1, f'Identical sample rate required! ({dt_0} != {dt_1})'\n# (2) define trace extents to correlate\n## get extent of overlapping trace range (in ms) if not explicitely specified\nif not all([win_up, win_lo]):\nwin_up = max(twt_0.min(), twt_1.min())\nwin_lo = min(twt_0.max(), twt_1.max())\nif (max(twt_0.min(), twt_1.min()) &gt; win_up) or (min(twt_0.max(), twt_1.max()) &lt; win_lo):\nxprint(\n*(\nf'Adjust window range ({win_up}:{win_lo} ms) to valid data range ',\nf'({max(twt_0.min(), twt_1.min())}:{min(twt_0.max(), twt_1.max())} ms)',\n),\nkind='warning',\nverbosity=verbosity,\n)\nif max(twt_0.min(), twt_1.min()) &gt; win_up:\nwin_up = max(twt_0.min(), twt_1.min())\nif min(twt_0.max(), twt_1.max()) &lt; win_lo:\nwin_lo = min(twt_0.max(), twt_1.max())\n## subset trace windows for cross-correlation (in ms)\n# boolean masking of subset samples\nmask_0 = (twt_0 &gt;= win_up) &amp; (twt_0 &lt;= win_lo)\ntr_0 = trace_0[mask_0]\n# boolean masking of subset samples\nmask_1 = (twt_1 &gt;= win_up) &amp; (twt_1 &lt;= win_lo)\ntr_1 = trace_1[mask_1]\n# exclude all zero samples of either trace (from padding)\nmask_zero_samples = (tr_0 == 0) | (tr_1 == 0)\ntr_0 = tr_0[~mask_zero_samples]\ntr_1 = tr_1[~mask_zero_samples]\n# (3) compute cross-correlation\ncc = scipy_correlate(tr_0, tr_1, mode='same', method='fft')\n## get mistie (in samples)\nmistie = cross_correlation_shift(cc)\n## calculate Pearson\u2019s correlation coefficient\ncoeff = scipy_pearsonr(tr_0, tr_1)[0]\n# (4) populate output arrays\nintersections_misties[idx_intersection] = mistie\nintersections_coeffs[idx_intersection] = coeff\n# (5) filter misties &amp; coefficients by quality\nmask_quality = np.abs(intersections_coeffs) &gt;= quality\nxprint(\nf'Filtered &lt; {np.count_nonzero(~mask_quality)} &gt; values below quality threshold ({quality})',\nkind='info',\nverbosity=verbosity,\n)\n## mistie (cross-correlation shift) &lt; 0  ---&gt;  line_0 deeper than line_1\n## mistie (cross-correlation shift) &gt; 0  ---&gt;  line_0 shallower than line_1\nmisties = intersections_misties[mask_quality]\ncoeffs = intersections_coeffs[mask_quality]\n# (6) filter line names/indices by quality\nline_intersections_names = line_intersections_names[mask_quality]\nline_intersections_idx = line_intersections_idx[mask_quality]\nnintersections = len(misties)  # number of intersections (after filtering)\nnlines = len(lookup_df)  # number of SEG-Y lines\n# (7) compute global mistie for each line for all intersections (using least-squares solution)\n## init adjacency matrix (non-square)\nA = np.zeros((nintersections, nlines), dtype=np.int32)\n## populate adjacency matrix (c.f. Bishop and Nunns, 1994)\ni = np.arange(nintersections)\nA[i, line_intersections_idx[:, 0]] = 1  # a_ij =  1, if k(i) = j\nA[i, line_intersections_idx[:, 1]] = -1  # a_ij = -1, if l(i) = j\n## least-squares solution of linear matrix equation\noffsets, residuals, rank, s = np.linalg.lstsq(A, misties, rcond=None)\noffsets = np.around(offsets, 0).astype('int16')  # single shift per line\n# offsets_per_pnt = offsets[points_line_idx]        # shift per line point\nif return_ms and return_coeff:\noffsets_ms = samples2twt(offsets, dt=dt_0)\n# offsets_per_pnt_ms = offsets_ms[points_line_idx]\n# return (offsets, offsets_per_pnt), (offsets_ms, offsets_per_pnt_ms), coeff\nreturn (offsets, residuals), offsets_ms, coeffs\nelif return_ms and not return_coeff:\noffsets_ms = samples2twt(offsets, dt=dt_0)\n# offsets_per_pnt_ms = offsets_ms[points_line_idx]\n# return (offsets, offsets_per_pnt), (offsets_ms, offsets_per_pnt_ms)\nreturn (offsets, residuals), offsets_ms\nelif not return_ms and return_coeff:\nreturn (offsets, residuals), coeffs\nreturn (offsets, residuals)\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.euclidian_distance","title":"<code>euclidian_distance(a, b)</code>","text":"<p>Calculate euclidian distance between array of points (n,2) and single point (2,).</p> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>@nb.jit(nopython=True, fastmath=True)\ndef euclidian_distance(a, b):\n\"\"\"Calculate euclidian distance between array of points (n,2) and single point (2,).\"\"\"\nn = a.shape[0]\ndist = np.empty((n,), dtype=np.float64)\nfor i in range(n):\ndist[i] = np.linalg.norm(a[i] - b)\nidx = np.argmin(dist)\nreturn (idx, dist[idx])\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.cross_correlation_shift","title":"<code>cross_correlation_shift(cc)</code>","text":"<p>Return cross-correlation shift (in samples) between correlated signals.</p> <p>Parameters:</p> <ul> <li> cc             (<code>np.ndarray</code>)         \u2013 <p>Cross-correlation between two signals (1D).</p> </li> </ul> <p>Returns:</p> <ul> <li> shift(            <code>int</code> )        \u2013 <p>Lag shift between both signals.</p> <ul> <li><code>shift</code> &lt; 0  ---&gt;  signal A later than signal B</li> <li><code>shift</code> &gt; 0  ---&gt;  signal A earlier than signal B</li> </ul> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>def cross_correlation_shift(cc):\n\"\"\"\n    Return cross-correlation shift (in samples) between correlated signals.\n    Parameters\n    ----------\n    cc : np.ndarray\n        Cross-correlation between two signals (1D).\n    Returns\n    -------\n    shift : int\n        Lag shift between both signals.\n        - `shift` &lt; 0  ---&gt;  signal A later than signal B\n        - `shift` &gt; 0  ---&gt;  signal A earlier than signal B\n    \"\"\"\nzero_idx = int(np.floor(len(cc) / 2))\nidx = np.argmax(cc) if np.abs(np.max(cc)) &gt;= np.abs(np.min(cc)) else np.argmin(cc)\nreturn zero_idx - idx\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.compensate_mistie","title":"<code>compensate_mistie(data, mistie, verbosity=1)</code>","text":"<p>Apply computed static offsets to seismic traces (2D array).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>2D array of input seismic traces (<code>samples</code> x <code>traces</code>).</p> </li> <li> mistie             (<code>int, float</code>)         \u2013 <p>1D array of mistie offsets (for all traces).</p> </li> </ul> <p>Returns:</p> <ul> <li> data_mistie(            <code>np.ndarray(s)</code> )        \u2013 <p>Compensated seismic section.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>def compensate_mistie(data, mistie, verbosity=1):\n\"\"\"\n    Apply computed static offsets to seismic traces (2D array).\n    Parameters\n    ----------\n    data : np.ndarray\n        2D array of input seismic traces (`samples` x `traces`).\n    mistie : int, float\n        1D array of mistie offsets (for all traces).\n    Returns\n    -------\n    data_mistie : np.ndarray(s)\n        Compensated seismic section.\n    \"\"\"\nmistie_samples = int(np.around(mistie, 0))\n# create copy of original data\ndata_mistie = data.copy()\nn_samples, n_traces = data_mistie.shape\nif mistie_samples &lt; 0:\ndata_mistie = np.concatenate(\n(data_mistie[abs(mistie_samples) :, :], np.zeros((abs(mistie_samples), n_traces)))\n)\nxprint(\nf'#samples:{mistie_samples:&gt;5}   -&gt;   up: {data_mistie.shape}',\nkind='debug',\nverbosity=verbosity,\n)\nelif mistie_samples &gt; 0:\ndata_mistie = np.concatenate(\n(np.zeros((abs(mistie_samples), n_traces)), data_mistie[: -abs(mistie_samples), :])\n)\nxprint(\nf'#samples:{mistie_samples:&gt;5}   -&gt;   down: {data_mistie.shape}',\nkind='debug',\nverbosity=verbosity,\n)\nelse:\npass\nreturn data_mistie\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.write_intersections_QC","title":"<code>write_intersections_QC(args, pts_split, line_intersections_idx, line_intersections_names, nearest_0, nearest_1, intersections_pts, crs='EPSG:32760')</code>","text":"<p>Write GeoPackage of line intersections and nearest traces.</p> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>def write_intersections_QC(\nargs,\npts_split,\nline_intersections_idx,\nline_intersections_names,\nnearest_0,\nnearest_1,\nintersections_pts,\ncrs='EPSG:32760',\n) -&gt; None:\n\"\"\"Write GeoPackage of line intersections and nearest traces.\"\"\"\nTODAY = datetime.date.today().isoformat()\n# gis_dir = 'C:/PhD/processing/gis/sbes'\nwork_dir = args.output_dir if args.output_dir is not None else args.input_path\nbasepath = os.path.basename(args.output_dir)\n# csv_file  = os.path.join(work_dir, f'{TODAY}_QC_{basepath}_intersections.txt')\ngpkg_file = os.path.join(work_dir, f'{TODAY}_QC_{basepath}_intersections.gpkg')\n# (1) NEAREST VERTICES\n# list of number of points per line\nlines_npts = [a.shape[0] for a in pts_split]\n# compute index offsets for first line indices\nlines_npts_cumsum = np.concatenate((np.array([0]), np.cumsum(lines_npts)[:-1].T))\n# get offsets for intersection line pairs\nlines_offset_indices = lines_npts_cumsum[line_intersections_idx]\n# flatten array of coordinate arrays\npoints = np.concatenate(pts_split)\n# get X and Y coordinates of nearest intersection points\npoints_0 = points[nearest_0[:, 0].astype('int') + lines_offset_indices[:, 0]]\npoints_1 = points[nearest_1[:, 0].astype('int') + lines_offset_indices[:, 1]]\n# create GeoDataFrame (line 0)\ngpd_nearest_pts_0 = gpd.GeoDataFrame(\ndata=dict(\ndist=nearest_0[:, 1], geom=gpd.points_from_xy(x=points_0[:, 0], y=points_0[:, 1])\n),\ngeometry='geom',\ncrs=crs,\n)\n# create GeoDataFrame (line 1)\ngpd_nearest_pts_1 = gpd.GeoDataFrame(\ndata=dict(\ndist=nearest_1[:, 1], geom=gpd.points_from_xy(x=points_1[:, 0], y=points_1[:, 1])\n),\ngeometry='geom',\ncrs=crs,\n)\n# export nearest vertices to GeoPackage\ngpd_nearest_pts_0.to_file(gpkg_file, driver='GPKG', layer='nearest_vertices_line_0')\ngpd_nearest_pts_1.to_file(gpkg_file, driver='GPKG', layer='nearest_vertices_line_1')\n# (2) INTERSECTION\n# create DataFrame from intersection points\ndf_intersection_pts = pd.DataFrame(\ndata=dict(\nx=shapely.get_x(intersections_pts),\ny=shapely.get_y(intersections_pts),\nline_0=line_intersections_names[:, 0],\ndist_0=nearest_0[:, 1],\nx_0=points_0[:, 0],\ny_0=points_0[:, 1],\nline_1=line_intersections_names[:, 1],\ndist_1=nearest_1[:, 1],\nx_1=points_1[:, 0],\ny_1=points_1[:, 1],\n)\n)\n# df_intersection_pts.to_csv(csv_file, sep=',', index=False)\n# cconvert into GeoDataFrame\ngpd_intersection_pts = gpd.GeoDataFrame(\ndf_intersection_pts, geometry=intersections_pts, crs=crs\n)\n# export intersection points to GeoPackage\ngpd_intersection_pts.to_file(gpkg_file, driver='GPKG', layer='intersections')\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.main_misties","title":"<code>main_misties(args)</code>","text":"<p>Compute line intersection misties.</p> <p>Parameters:</p> <ul> <li> args             (<code>argparse.Namespace</code>)         \u2013 <p>Input parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> list_segy(            <code>list</code> )        \u2013 <p>List of SEG-Y files to correct.</p> </li> <li> offsets(            <code>np.ndarray</code> )        \u2013 <p>Vertical misties per intersection (in samples).</p> </li> <li> offsets_ms(            <code>np.ndarray</code> )        \u2013 <p>Vertical mistie per intersection (in milliseconds TWT).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>def main_misties(args):\n\"\"\"\n    Compute line intersection misties.\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Input parameter.\n    Returns\n    -------\n    list_segy : list\n        List of SEG-Y files to correct.\n    offsets : np.ndarray\n        Vertical misties per intersection (in samples).\n    offsets_ms : np.ndarray\n        Vertical mistie per intersection (in milliseconds TWT).\n    \"\"\"\n# coords_path = args.coords_path if args.coords_path is not None else args.input_path\n# (1) Load SEG-Y coordinates (navigation)\n## OPTION A: from auxiliary files (*.nav)\nif args.coords_origin == 'aux':\n# set file suffix\nfsuffix = args.coords_fsuffix if args.coords_fsuffix is not None else 'nav'\n# read navigation data\n# df = pd.read_csv(args.coords_path, index_col='tracl') # index_col=0\nxprint(\nf'Load navigation from auxiliary files (*.{fsuffix})',\nkind='info',\nverbosity=args.verbose,\n)\ndf_nav = read_auxiliary_files(args.coords_path, fsuffix=fsuffix, index_cols=None)\n## OPTION B: from SEG-Y headers\nelif args.coords_origin == 'header':\n# set file suffix\nfsuffix = args.coords_fsuffix if args.coords_fsuffix is not None else 'sgy'\n# read navigation data\nxprint(\n'Extract and load navigation from headers of SEG-Y files',\nkind='info',\nverbosity=args.verbose,\n)\ndf_nav = extract_navigation_from_segy(args.coords_path, fsuffix=fsuffix, write_aux=False)\n# create int representation of line names\ndf_nav['line_id'] = df_nav['line'].factorize()[0]\n# create array of unique SEG-Y files (from navigation file)\nintersections_lines = df_nav[['line_id', 'line']].groupby('line').first().index.to_numpy()\n# (2) Create list of SEG-Y files to process\n## OPTION A: from datalist\nif os.path.isfile(args.input_path):\ndf_segy = pd.read_csv(args.input_path, header=None, names=['line'])\n# extract first part of line names (to match names from navigation file)\ndf_segy['line_core'] = df_segy['line'].str.extract(r'(.*)_UTM60S')\ndf_segy.set_index('line_core', inplace=True)\n# create list of SEG-Y file for later import\nlist_segy = df_segy['line'].to_list()\ndir_segy, file_datalist = os.path.split(args.input_path)\n## OPTION B: from directory of SEG-Y files\nelif os.path.isdir(args.input_path):\ndir_segy = args.input_path\npattern = '*'\npattern += f'{args.filename_suffix}' if args.filename_suffix is not None else pattern\npattern += f'.{args.suffix}' if args.suffix is not None else '.sgy'\n# create list of SEG-Y file for later import\nlist_segy = glob.glob(os.path.join(args.input_path, pattern))\nlist_segy = [os.path.basename(f) for f in list_segy]\n# create lookup dataframe\ndf_segy = pd.DataFrame(list_segy, columns=['line'])\ndf_segy['line_core'] = df_segy['line'].str.extract(r'(.*)_UTM60S')\ndf_segy.set_index('line_core', inplace=True)\nif (args.quality_threshold &lt; 0) or (args.quality_threshold &gt; 1):\nraise ValueError('`quality_threshold` must be float in range [0-1]')\nelse:\nquality = args.quality_threshold\nif args.win_cc is None:\nwin = (False, False)\nelse:\nupper, lower = args.win_cc[0], args.win_cc[1]\nif upper &gt; lower:\nupper, lower = args.win_cc[1], args.win_cc[0]\nwin = (float(upper), float(lower))\n# ===== PROCESSING =====\n# (1) create shapely geometries\nlinestrings, pts_split, pts_line_idx = create_geometries(\ndf_nav, coord_cols=['x', 'y'], idx_col='line_id'\n)\n# (2) find intersections (Points, MultiPoints, LineStrings, GeometryCollections)\nintersections_pts, intersections_pts_idx, line_intersections_idx = find_intersections(\nlinestrings\n)\n# get line names for all intersections\nline_intersections_names = intersections_lines[line_intersections_idx]\nassert line_intersections_names.shape == line_intersections_idx.shape\n# (3) get array of nearest vertices (with distance)\nnearest_0, nearest_1 = nearest_intersection_vertices(\npts_split, intersections_pts, line_intersections_idx\n)\n# (4) get mistie in both samples and milliseconds\n(offsets, residuals), offsets_ms, coeffs = compute_misties(\ndir_segy,\nline_intersections_names,\nline_intersections_idx,\nnearest_0,\nnearest_1,\nwin=win,\nquality=quality,\nlookup_df=df_segy,\nlookup_col='line',\ncheck_bad_traces=True,\nntraces2mix=3,\nreturn_ms=True,\nreturn_coeff=True,\nverbosity=args.verbose,\n)\nif args.write_QC:\nxprint(\n'Save line intersections and nearest traces as CSV and GeoPackage',\nkind='info',\nverbosity=args.verbose,\n)\nwrite_intersections_QC(\nargs,\npts_split,\nline_intersections_idx,\nline_intersections_names,\nnearest_0,\nnearest_1,\nintersections_pts,\n)\nreturn list_segy, offsets, offsets_ms, residuals\n</code></pre>"},{"location":"api/api_mistie_correction_segy/#pseudo_3D_interpolation.mistie_correction_segy.wrapper_mistie_correction_segy","title":"<code>wrapper_mistie_correction_segy(in_path, offset, offset_ms, args)</code>","text":"<p>Apply computed misties to each line (SEG-Y file).</p> Source code in <code>pseudo_3D_interpolation\\mistie_correction_segy.py</code> <pre><code>def wrapper_mistie_correction_segy(in_path, offset, offset_ms, args):\n\"\"\"Apply computed misties to each line (SEG-Y file).\"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=args.verbose)\nif os.path.isdir(args.output_dir) and args.inplace is False:\nxprint('Creating copy of file in output directory', kind='info', verbosity=args.verbose)\nif args.txt_suffix is not None:\nout_name = f'{basename}_{args.txt_suffix}'\nelse:\nout_name = f'{basename}_mistie'\nout_path = os.path.join(args.output_dir, f'{out_name}{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=args.verbose,\n)\nos.remove(out_path)\ncopy2(in_path, out_path)\npath = out_path\nelif not os.path.isdir(args.output_dir) and args.inplace is True:\nxprint('Updating SEG-Y inplace', kind='warning', verbosity=args.verbose)\npath = in_path\n# read SEGY file\nwith segyio.open(path, 'r+', strict=False, ignore_geometry=True) as src:\nn_traces = src.tracecount  # total number of traces\ndt = segyio.tools.dt(src) / 1000  # sample rate [ms]\nn_samples = src.samples.size  # total number of samples\n# twt = src.samples                   # two way travel time (TWTT) [ms]\nxprint(f'n_traces:  {n_traces}', kind='debug', verbosity=args.verbose)\nxprint(f'n_samples: {n_samples}', kind='debug', verbosity=args.verbose)\nxprint(f'dt:        {dt}', kind='debug', verbosity=args.verbose)\ntracl = src.attributes(segyio.TraceField.TRACE_SEQUENCE_LINE)[:]\ntracr = src.attributes(segyio.TraceField.TRACE_SEQUENCE_FILE)[:]\nfldr = src.attributes(segyio.TraceField.FieldRecord)[:]\n# get seismic data [amplitude]; transpose to fit numpy data structure\ndata_src = src.trace.raw[:].T  # eager version (completely read into memory)\n# apply tidal compensation to seismic traces\nxprint('Apply mistie compensation', kind='info', verbosity=args.verbose)\ndata_comp = compensate_mistie(data_src, offset, verbosity=args.verbose)\n# set output amplitudes (transpose to fit SEG-Y format)\nxprint('Writing compensated data to disk', kind='info', verbosity=args.verbose)\nsrc.trace = np.ascontiguousarray(data_comp.T, dtype=data_src.dtype)\n# update textual header\ntext = get_textual_header(path)\ntext_updated = add_processing_info_header(text, 'MISTIE', prefix='_TODAY_', newline=True)\nwrite_textual_header(path, text_updated)\nif args.write_aux:\nxprint(f'Creating auxiliary file &lt; {out_name}.mst &gt;', kind='info', verbosity=args.verbose)\naux_path = os.path.join(args.output_dir, f'{out_name}.mst')\nwith open(aux_path, 'w', newline='\\n') as fout:\nheader = 'tracl,tracr,fldr,mistie_samples,mistie_ms\\n'\nfout.write(header)\nfor i in range(len(tracr)):\nline = f'{tracl[i]},{tracr[i]},{fldr[i]},' + f'{offset},{offset_ms:.2f}\\n'\nfout.write(line)\n</code></pre>"},{"location":"api/api_reproject_segy/","title":"<code>reproject_segy.py</code>","text":"<p>Utility script to reproject/transform SEG-Y header coordinates between coordinate reference systems (CRS).</p>"},{"location":"api/api_reproject_segy/#pseudo_3D_interpolation.reproject_segy.dms2dd","title":"<code>dms2dd(dms)</code>","text":"<p>Convert DMS (Degrees, Minutes, Seconds) coordinate string to DD (Decimal Degrees).</p> Source code in <code>pseudo_3D_interpolation\\reproject_segy.py</code> <pre><code>def dms2dd(dms):\n\"\"\"Convert DMS (Degrees, Minutes, Seconds) coordinate string to DD (Decimal Degrees).\"\"\"\ndms_str = str(dms)\ndegrees, minutes, seconds = int(dms_str[:3]), int(dms_str[3:5]), int(dms_str[5:])\nif len(str(seconds)) &gt; 2:\nseconds = float(str(seconds)[:2] + '.' + str(seconds)[2:])\nreturn float(degrees) + float(minutes) / 60 + float(seconds) / (60 * 60)\n</code></pre>"},{"location":"api/api_reproject_segy/#pseudo_3D_interpolation.reproject_segy.wrapper_reproject_segy","title":"<code>wrapper_reproject_segy(in_path, src_coords_bytes, dst_coords_bytes, crs_src, crs_dst, out_dir, inplace, scalar_coords, txt_suffix, verbosity=0)</code>","text":"<p>Reproject SEG-Y header coordinates.</p> <p>Parameters:</p> <ul> <li> in_path             (<code>str</code>)         \u2013 <p>SEG-Y input path.</p> </li> <li> src_coords_bytes             (<code>tuple</code>)         \u2013 <p>Input byte position of coordinates in trace header.</p> </li> <li> dst_coords_bytes             (<code>tuple</code>)         \u2013 <p>Output byte position of coordinates in trace header.</p> </li> <li> crs_src             (<code>str</code>)         \u2013 <p>Input coordinate reference system (e.g., <code>epsg:4326</code>).</p> </li> <li> crs_dst             (<code>str</code>)         \u2013 <p>Input coordinate reference system (e.g., <code>epsg:32760</code>).</p> </li> <li> out_dir             (<code>str</code>)         \u2013 <p>Onput path of reprojected SEG-Y file.</p> </li> <li> inplace             (<code>bool</code>)         \u2013 <p>Edit coordinates without creating SEG-Y copy.</p> </li> <li> scalar_coords             (<code>int</code>)         \u2013 <p>Output coordinate scalar.</p> </li> <li> txt_suffix             (<code>str</code>)         \u2013 <p>Additional text to append to output filename.</p> </li> <li> verbosity             (<code>int, optional</code>)         \u2013 <p>Level of output verbosity (default: 0).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\reproject_segy.py</code> <pre><code>def wrapper_reproject_segy(\nin_path: str,\nsrc_coords_bytes: tuple,\ndst_coords_bytes: tuple,\ncrs_src: str,\ncrs_dst: str,\nout_dir: str,\ninplace: bool,\nscalar_coords: int,\ntxt_suffix: str,\nverbosity: int = 0,\n):\n\"\"\"\n    Reproject SEG-Y header coordinates.\n    Parameters\n    ----------\n    in_path : str\n        SEG-Y input path.\n    src_coords_bytes : tuple\n        Input byte position of coordinates in trace header.\n    dst_coords_bytes : tuple\n        Output byte position of coordinates in trace header.\n    crs_src : str\n        Input coordinate reference system (e.g., `epsg:4326`).\n    crs_dst : str\n        Input coordinate reference system (e.g., `epsg:32760`).\n    out_dir : str\n        Onput path of reprojected SEG-Y file.\n    inplace : bool\n        Edit coordinates without creating SEG-Y copy.\n    scalar_coords : int\n        Output coordinate scalar.\n    txt_suffix : str\n        Additional text to append to output filename.\n    verbosity : int, optional\n        Level of output verbosity (default: 0).\n    \"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=verbosity)\nif os.path.isdir(out_dir) and inplace is False:\nxprint('Creating copy of file in output directory', kind='info', verbosity=verbosity)\nif txt_suffix is not None:\nout_path = os.path.join(out_dir, f'{basename}_{txt_suffix}{suffix}')\nelse:\nout_path = os.path.join(out_dir, f'{basename}_reproj{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=verbosity,\n)\nos.remove(out_path)\ncopy2(in_path, out_path)\npath = out_path\nelif not os.path.isdir(out_dir) and inplace is True:\nxprint('Updating coordinates inplace', kind='warning', verbosity=verbosity)\npath = in_path\n# read SEGY file\nwith segyio.open(path, 'r+', strict=False, ignore_geometry=True) as file:\n# get scaled coordinates\nxcoords, ycoords, coordinate_units = scale_coordinates(file, src_coords_bytes)\n# get source &amp; destination CRS\ncrs_src = pyproj.crs.CRS(crs_src)\nif coordinate_units != 1 and crs_src.is_projected:\nxprint(\n'Forced source CRS to be geographic (WGS84 - EPSG:4326)!',\nkind='warning',\nverbosity=verbosity,\n)\ncrs_src = pyproj.crs.CRS('epsg:4326')\ncrs_dst = pyproj.crs.CRS(crs_dst)\n# create CRS transformer\ntransformer = pyproj.transformer.Transformer.from_crs(crs_src, crs_dst, always_xy=True)\n# convert coordinates\nxcoords_t, ycoords_t = transformer.transform(xcoords, ycoords, errcheck=True)\n# update coordinates in trace header\n# update_coordinates(file, xcoords_t, ycoords_t, crs_dst, dst_coords_bytes, scalar_coords)\nset_coordinates(\nfile,\nxcoords_t,\nycoords_t,\ncrs_dst,\ndst_coords_bytes,\ncoordinate_units=1,\nscale_factor=scalar_coords,\n)\n# update textual header\ntext = get_textual_header(path)\n## add info about new CRS\ninfo = f'EPSG:{crs_dst.to_epsg()}'\ntext_updated = add_processing_info_header(text, info, prefix='CRS (projected)')\n# add info about byte position of reprojected coords\ninfo = f'REPROJECT (bytes:{dst_coords_bytes[0]},{dst_coords_bytes[1]})'\ntext_updated = add_processing_info_header(text_updated, info, prefix='_TODAY_')\nwrite_textual_header(path, text_updated)\n</code></pre>"},{"location":"api/api_reproject_segy/#pseudo_3D_interpolation.reproject_segy.main","title":"<code>main(argv=sys.argv)</code>","text":"<p>Reproject trace header coordinats of SEG-Y file(s).</p> Source code in <code>pseudo_3D_interpolation\\reproject_segy.py</code> <pre><code>def main(argv=sys.argv):  # noqa\n\"\"\"Reproject trace header coordinats of SEG-Y file(s).\"\"\"\nTIMESTAMP = datetime.datetime.now().isoformat(timespec='seconds').replace(':', '')\nSCRIPT = os.path.basename(__file__).split(\".\")[0]\nparser = define_input_args()\nargs = parser.parse_args(argv[1:])  # exclude filename parameter at position 0\n# input and output coordinate byte positions\nsrc_coords_bytes = trace_header_coords.get(args.src_coords)\ndst_coords_bytes = trace_header_coords.get(args.dst_coords)\n# input and output coordinate reference systems\ncrs_src = args.crs_src\ncrs_dst = args.crs_dst\n# additional input parameter\ninplace = args.inplace  # edit SEG-Y file inplace\nout_dir = args.output_dir  # output directory\nin_suffix = args.suffix  # file suffix to search for in input dir\nfn_suffix = args.filename_suffix  # filename suffix to restrict glob* method\nscalar_coords = args.scalar_coords  # output coordinate scalar\ntxt_suffix = args.txt_suffix  # additional text suffix for filename\nverbosity = args.verbose  # level of verbostiy\n# sanity check\nif (inplace is False and out_dir is None) or (inplace is True and out_dir is not None):\nsys.exit('[ERROR]    Either \"output_dir\" OR \"inplace\" have to be specified!')\n# check input file(s)\nin_path = args.input_path\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nif suffix == '':\nbasepath = in_path\nbasename, suffix = None, None  # noqa\n# (1) single input file\nif os.path.isfile(in_path) and (suffix != '.txt'):\n# perform coordinate transformation\nwrapper_reproject_segy(\nin_path,\nsrc_coords_bytes,\ndst_coords_bytes,\ncrs_src,\ncrs_dst,\nout_dir,\ninplace,\nscalar_coords,\ntxt_suffix,\nverbosity,\n)\nsys.exit()\n# (2) input directory (multiple files)\nelif os.path.isdir(in_path):\npattern = '*'\npattern += f'{fn_suffix}' if fn_suffix is not None else pattern\npattern += f'.{in_suffix}' if in_suffix is not None else '.sgy'\nfile_list = glob.glob(os.path.join(in_path, pattern))\n# (3) file input is datalist (multiple files)\nelif os.path.isfile(in_path) and (suffix == '.txt'):\nwith open(in_path, 'r') as datalist:\nfile_list = datalist.readlines()\nfile_list = [\nos.path.join(basepath, line.rstrip())\nif os.path.split(line.rstrip()) not in ['', '.']\nelse line.rstrip()\nfor line in file_list\n]\n# compute files from options (2) or (3)\nif len(file_list) &gt; 0:\n# redirect stdout to logfile\nwith open(\nos.path.join(basepath, f'{TIMESTAMP}_{SCRIPT}.log'),\n'w',\nnewline='\\n',\n) as f:\nwith redirect_stdout(f):\nxprint(\nf'Processing total of &lt; {len(file_list)} &gt; files',\nkind='info',\nverbosity=verbosity,\n)\nfor file_path in file_list:\n# perform coordinate transformation\nwrapper_reproject_segy(\nfile_path,\nsrc_coords_bytes,\ndst_coords_bytes,\ncrs_src,\ncrs_dst,\nout_dir,\ninplace,\nscalar_coords,\ntxt_suffix,\nverbosity,\n)\nelse:\nsys.exit('No input files to process. Exit process.')\n</code></pre>"},{"location":"api/api_static_correction_segy/","title":"<code>static_correction_segy.py</code>","text":"<p>Compensate static on seismic profile(s) using either \"SourceWaterDepth\" (mode: <code>swdep</code>) or first positive amplitude peak of seafloor reflection (mode: <code>amp</code>).</p>"},{"location":"api/api_static_correction_segy/#pseudo_3D_interpolation.static_correction_segy.get_static","title":"<code>get_static(data, kind='diff', interp_kind='cubic', win_mad=None, win_sg=7, limit_perc=99, limit_samples=10, limit_by_MAD=False, limit_depressions=False)</code>","text":"<p>Compute static (as deviation from global reference level) for each trace. Input data array is first filtered and interpolated to remove detecion outliers. Depending on selected option, the static will be calculated as:</p> <ul> <li><code>diff</code>: difference between filtered input data (removed outlier) and           Savitzky-Golay lowpass filter result</li> <li><code>deriv</code>: output of Savitzky-Golay hihgpass filter (2nd derivative)</li> </ul> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data array (1D).</p> </li> <li> kind             (<code>str, optional</code>)         \u2013 <p>Static calculation option (default: 'diff').</p> </li> <li> interp_kind             (<code>str, optional</code>)         \u2013 <p>Interpolation type used for scipy.interpolate.interp1d (default: 'cubic').</p> </li> <li> win_mad             (<code>int, optional</code>)         \u2013 <p>Windwow length used for initial data filtering (in traces). If None, it will about 5% of the input data length (at least 7 traces!).</p> </li> <li> win_sg             (<code>int, optional</code>)         \u2013 <p>Window length for Savitzky-Golay filter (in traces). It should be slightly larger than the period of the observed static amplitudes (default: 7).</p> </li> <li> limit_perc         \u2013 <p>Clip calculated static shift (in samples) using np.percentile with given value (default: 99).</p> </li> <li> limit_samples         \u2013 <p>Clip calculated static shift (in samples) using user-specified number of samples (default: 10).</p> </li> <li> limit_by_MAD             (<code>float, optional</code>)         \u2013 <p>Clip calculated static shift (in samples) using median absolute deviation (MAD) multiplied by <code>limit_by_MAD</code>. If true, <code>limit_by_MAD</code> defaults to 3 (-&gt; 3-sigma rule of thumb).</p> </li> <li> limit_depressions             (<code>tuple(float, float, float), optional</code>)         \u2013 <p>Account for seafloor depressions that represent significant topographic changes over short distances by limiting the maximum vertical shift using a linear function from depression center to flanks, e.g. limits: 10, 8, 6, 4, 6, 8, 10 (max samples shift).</p> <pre><code>tuple(\nlenght of transition zone to each side [default: 10],\nmax shift at outer edge of transition zone [default: 10],\nmax shift at depression center [default: 5],\n)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> static(            <code>np.ndarray</code> )        \u2013 <p>Computed static as deviation from reference level.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\static_correction_segy.py</code> <pre><code>def get_static(\ndata,\nkind='diff',\ninterp_kind='cubic',\nwin_mad=None,\nwin_sg=7,\nlimit_perc=99,\nlimit_samples=10,\nlimit_by_MAD=False,\nlimit_depressions=False,\n):\n\"\"\"\n    Compute static (as deviation from global reference level) for each trace.\n    Input data array is first filtered and interpolated to remove detecion outliers.\n    Depending on selected option, the static will be calculated as:\n    - `diff`: difference between filtered input data (removed outlier) and\n              Savitzky-Golay lowpass filter result\n    - `deriv`: output of Savitzky-Golay hihgpass filter (2nd derivative)\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data array (1D).\n    kind : str, optional\n        Static calculation option (default: 'diff').\n    interp_kind : str, optional\n        Interpolation type used for scipy.interpolate.interp1d (default: 'cubic').\n    win_mad : int, optional\n        Windwow length used for initial data filtering (in traces).\n        If None, it will about 5% of the input data length (at least 7 traces!).\n    win_sg : int, optional\n        Window length for Savitzky-Golay filter (in traces). It should be slightly larger\n        than the period of the observed static amplitudes (default: 7).\n    limit_perc: float, optional\n        Clip calculated static shift (in samples) using np.percentile with given value (default: 99).\n    limit_samples: float, optional\n        Clip calculated static shift (in samples) using user-specified number of samples (default: 10).\n    limit_by_MAD : float, optional,\n        Clip calculated static shift (in samples) using median absolute deviation (MAD)\n        multiplied by `limit_by_MAD`. If true, `limit_by_MAD` defaults to 3 (-&gt; 3-sigma rule of thumb).\n    limit_depressions : tuple(float, float, float), optional\n        Account for seafloor depressions that represent significant topographic changes\n        over short distances by limiting the maximum vertical shift using a linear function\n        from depression center to flanks, e.g. limits: 10, 8, 6, 4, 6, 8, 10 (max samples shift).\n        ```python\n        tuple(\n            lenght of transition zone to each side [default: 10],\n            max shift at outer edge of transition zone [default: 10],\n            max shift at depression center [default: 5],\n            )\n        ```\n    Returns\n    -------\n    static : np.ndarray\n        Computed static as deviation from reference level.\n    \"\"\"\nif data.ndim != 1:\nraise ValueError(f'Input array must have only one dimension not {data.ndim}.')\nif kind not in ['diff', 'deriv']:\nraise ValueError(f'Kind &lt; {kind} &gt; is not supported')\ndata = np.asarray(data)\nif win_mad is None:\nwin_mad = int(data.size * 0.05)\nwin_mad = win_mad + 1 if win_mad % 2 == 0 else win_mad  # must be odd\nwin_mad = 7 if win_mad &lt; 7 else win_mad  # at least 7 traces\n# (1) outlier detection &amp; removal\ndata_mad_r = filter_interp_1d(\ndata, method='r_doubleMAD', kind=interp_kind, threshold=3, win=win_mad\n)\nif kind == 'diff':\n# (2) apply Savitzky-Golay filter (lowpass)\ndata_lowpass = savgol_filter(data_mad_r, window_length=win_sg, polyorder=1, deriv=0)\nstatic = (\ndata_lowpass - data_mad_r\n)  # fit sign convention (&lt;0: add at top of trace, &gt;0: add at bottom of trace)\nelif kind == 'deriv':\n# (2) apply Savitzky-Golay filter (highpass)\norder = win_sg - 2\nstatic = savgol_filter(data_mad_r, window_length=win_sg, polyorder=order, deriv=2)\nif kind == 'diff' and limit_depressions:\n# apply polynominal filter to lowpass-filtered data\ndetrend = polynominal_filter(data_lowpass, order=11) * -1\n# detect \"outlier\" (depressions)\nidx_detrend = mad_filter(detrend, threshold=3, mad_mode='double')\nif idx_detrend.size == 0:\nreturn static\n# get indices &lt; 0 (only depressions)\npockmark_idx = np.nonzero(detrend[idx_detrend] &lt; 0)\nidx_detrend_filt = idx_detrend[pockmark_idx]\nif idx_detrend_filt.size == 0:\nreturn static\n# split indices array into individual arrays (per pockmark)\nidx_detrend_filt_splits = np.split(\nidx_detrend_filt, np.where(np.diff(idx_detrend_filt) &gt; 1)[0] + 1\n)\n# remove detection where ntraces &lt; 3\nidx_detrend_filt_splits = [a for a in idx_detrend_filt_splits if a.size &gt;= 3]\nif len(idx_detrend_filt_splits) == 0:\nreturn static\n# define indices and limits to clip static shift sample array\nnpad, limit_outer, limit_center = limit_depressions\n# npad = 10         # number of traces for transition zone\n# limit_outer = 10  # max. shift at boundary of transition zone\n# limit_center = 5  # max. shift for pockmark traces\n# get padded indices arrays per depression\npockmark_limits_idx = np.concatenate(\n[np.arange(p[0] - npad, p[-1] + npad + 1, dtype='int') for p in idx_detrend_filt_splits]\n)\n# get custom limits per depression\npockmark_limits = np.concatenate(\n[\nnp.concatenate(\n(\nnp.linspace(limit_outer, limit_center + 1, npad),\nnp.full_like(a, limit_center),\nnp.linspace(limit_center + 1, limit_outer, npad),\n)\n).astype('int')\nfor a in idx_detrend_filt_splits\n]\n)\nassert pockmark_limits_idx.shape == pockmark_limits.shape\n# account for boundary conditions\nmask_valid = np.nonzero((pockmark_limits_idx &lt; detrend.size) &amp; (pockmark_limits_idx &gt;= 0))[\n0\n]\npockmark_limits_idx = pockmark_limits_idx[mask_valid]\npockmark_limits = pockmark_limits[mask_valid]\n# clip static shift using custom limits for depressions\nstatic[pockmark_limits_idx] = np.where(\nnp.abs(static[pockmark_limits_idx]) &gt; pockmark_limits,\npockmark_limits * np.sign(static[pockmark_limits_idx]),\nstatic[pockmark_limits_idx],\n)\n# if set: clip static values using given percentile\nif limit_perc is not None and limit_perc is not False:\nclip = np.percentile(np.abs(static), limit_perc)\nstatic = np.where(np.abs(static) &gt; clip, clip * np.sign(static), static)\n# if set: clip static values using user-specified number of samples\nif isinstance(limit_samples, (float, int)):\nstatic = np.where(np.abs(static) &gt; limit_samples, limit_samples * np.sign(static), static)\n# if set: clip static values using median absolute deviation (multiplied by factor)\nif limit_by_MAD is True or isinstance(limit_by_MAD, (int, float)):\nlimit_by_MAD = limit_by_MAD if isinstance(limit_by_MAD, (int, float)) else 3\nthreshold = int(np.ceil(np.median(np.abs(static)) * limit_by_MAD))\nstatic = np.where(np.abs(static) &gt; threshold, threshold * np.sign(static), static)\nreturn static\n</code></pre>"},{"location":"api/api_static_correction_segy/#pseudo_3D_interpolation.static_correction_segy.compensate_static","title":"<code>compensate_static(data, static, dt=None, units='ms', cnv_d2s=False, v=1500, verbosity=1)</code>","text":"<p>Apply computed static offsets to seismic traces.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>2D array of input seismic traces (samples x traces).</p> </li> <li> static             (<code>np.ndarray</code>)         \u2013 <p>1D array of static offsets (per trace).</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in specified units (default: milliseconds).</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Time unit (for <code>dt</code>) (default: <code>s</code>).</p> </li> <li> cnv_d2s             (<code>bool, optional</code>)         \u2013 <p>Convert static offset provided as depth (in m) to samples (default: <code>False</code>).</p> </li> <li> v             (<code>int, optional</code>)         \u2013 <p>Sound velocity (m/s) for depth/time/samples conversion. Only used if <code>cnv_d2s</code> is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray(s)</code>         \u2013 <p>Compensated seismic section (and # of samples if converted).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\static_correction_segy.py</code> <pre><code>def compensate_static(data, static, dt=None, units='ms', cnv_d2s=False, v=1500, verbosity=1):\n\"\"\"\n    Apply computed static offsets to seismic traces.\n    Parameters\n    ----------\n    data : np.ndarray\n        2D array of input seismic traces (samples x traces).\n    static : np.ndarray\n        1D array of static offsets (per trace).\n    dt : float, optional\n        Sampling interval in specified units (default: milliseconds).\n    units : str, optional\n        Time unit (for `dt`) (default: `s`).\n    cnv_d2s : bool, optional\n        Convert static offset provided as depth (in m) to samples (default: `False`).\n    v : int, optional\n        Sound velocity (m/s) for depth/time/samples conversion. Only used if `cnv_d2s` is True.\n    Returns\n    -------\n    np.ndarray(s)\n        Compensated seismic section (and # of samples if converted).\n    \"\"\"\nif dt is not None:\nif units == 's':\npass\nelif units == 'ms':\ndt = dt / 1000\nelif units == 'ns':\ndt = dt / 1e-6\nif cnv_d2s:\nif dt is None:\nprint('[ERROR]   `dt` is required when converting depth to samples')\nreturn None\nstatic_samples = depth2samples(static, dt=dt, v=v, units='s')\nstatic_samples = np.around(static_samples, 0).astype(np.int32)\nelse:\nstatic_samples = np.around(static, 0).astype(np.int32)\n# create copy of original data\ndata_static = data.T.copy()\nfor i, col in enumerate(data_static):\noffset = static_samples[i]\nif offset &lt; 0:\ncol[:] = np.hstack((col[abs(offset) :], np.zeros(abs(offset))))\nxprint(\nf'trace #{i}:{offset:&gt;5}   -&gt;   up: {col.shape}', kind='debug', verbosity=verbosity\n)\nelif offset &gt; 0:\ncol[:] = np.hstack((np.zeros(abs(offset)), col[: -abs(offset)]))\nxprint(\nf'trace #{i}:{offset:&gt;5}   -&gt;   down: {col.shape}',\nkind='debug',\nverbosity=verbosity,\n)\nelse:\npass  # no static correction\nreturn data_static.T, static_samples\n</code></pre>"},{"location":"api/api_static_correction_segy/#pseudo_3D_interpolation.static_correction_segy.wrapper_static_correction_segy","title":"<code>wrapper_static_correction_segy(in_path, args)</code>","text":"<p>Applies static correction to single SEG-Y file.</p> Source code in <code>pseudo_3D_interpolation\\static_correction_segy.py</code> <pre><code>def wrapper_static_correction_segy(in_path, args):  # noqa\n\"\"\"Applies static correction to single SEG-Y file.\"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=args.verbose)\nif (args.output_dir is not None and os.path.isdir(args.output_dir)) and args.inplace is False:\nxprint('Creating copy of file in output directory', kind='info', verbosity=args.verbose)\nif args.txt_suffix is not None:\nout_name = f'{basename}_{args.txt_suffix}'\nelse:\nout_name = f'{basename}_static'\nout_path = os.path.join(args.output_dir, f'{out_name}{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=args.verbose,\n)\nos.remove(out_path)\ncopy2(in_path, out_path)\npath = out_path\nelif args.output_dir is None and args.inplace is True:\nxprint('Updating SEG-Y inplace', kind='info', verbosity=args.verbose)\npath = in_path\nwith segyio.open(path, 'r+', strict=False, ignore_geometry=True) as src:\nn_traces = src.tracecount  # total number of traces\ndt = segyio.tools.dt(src) / 1000  # sample rate [ms]\ntwt = src.samples  # two way travel time (TWTT) [ms]\ntracl = src.attributes(segyio.TraceField.TRACE_SEQUENCE_LINE)[:]\ntracr = src.attributes(segyio.TraceField.TRACE_SEQUENCE_FILE)[:]\nfldr = src.attributes(segyio.TraceField.FieldRecord)[:]\n# get DelayRecordingTimes from trace headers\ndelrt = src.attributes(args.byte_delay)[:]  # segyio.TraceField.DelayRecordingTime\nswdep = src.attributes(segyio.TraceField.SourceWaterDepth)[:]\nscalel = src.attributes(segyio.TraceField.ElevationScalar)[:]\nif all(s &gt; 0 for s in scalel):\nswdep = swdep * np.abs(scalel)\nelif all(s &lt; 0 for s in scalel):\nswdep = swdep / np.abs(scalel)\n# eager version (completely read into memory)\ndata_src = src.trace.raw[:].T\n# extract infos from binary header\nhns = src.bin[segyio.BinField.Samples]  # samples per trace\nnso = src.bin[segyio.BinField.SamplesOriginal]  # samples per trace (original)\n# (A) using SourceWaterDepth\nif args.mode == 'swdep' and (np.count_nonzero(swdep) == n_traces):\n# compute static offset using recorded SourceWaterDepth\nstatic_depth = get_static(\nswdep,\nkind='diff',\ninterp_kind='cubic',\nwin_mad=args.win_mad,\nwin_sg=args.win_sg,\nlimit_perc=False,\nlimit_samples=args.limit_shift,\nlimit_by_MAD=3,\nlimit_depressions=args.limit_depressions,\n)\n# compute static corrected data (and return sample index array)\ndata_corr, static_samples = compensate_static(\ndata_src, static_depth, dt=dt, units='ms', cnv_d2s=True, verbosity=args.verbose\n)\n# (B) using peak seafloor amplitude\nelif args.mode == 'amp':\n# find indices where DelayRecordingTime changes\ndelrt_idx = np.where(np.roll(delrt, 1) != delrt)[0]\nif delrt_idx.size != 0 and delrt_idx[0] == 0:\ndelrt_idx = delrt_idx[1:]\n## (B.1) padded SEG-Y --&gt; extrace only non-zero part of traces\nif 'pad' in path or (hns != nso):\ndata_src_sliced, idx_start_slice = slice_valid_data(data_src, nso)\nidx_amp = detect_seafloor_reflection(data_src_sliced, win=args.win_samples)\nidx_amp += idx_start_slice\ntwt_seafloor = twt[idx_amp]\n## (B.2) unpadded SEG-Y\nelse:\n### (B.2.1) single DelayRecordingTimes\nidx_amp = detect_seafloor_reflection(data_src, win=args.win_samples)\n_idx_amp = idx_amp.copy()\n### (B.2.2) variable DelayRecordingTimes\nif args.use_delay and (len(delrt_idx) &gt;= 1):\nxprint(\n'Account for variable DelayRecordingTimes (`delrt`)',\nkind='info',\nverbosity=args.verbose,\n)\n# get DelayRecordingTime offset as samples\ndelrt_min = delrt.min()\ndelrt_offset = (delrt - delrt_min).astype('int')\ndelrt_offset_samples = twt2samples(delrt_offset, dt=dt).astype('int')\n# add offset to indices of peak seafloor amplitude\nidx_amp += delrt_offset_samples\nif args.write_seafloor2trace:\n_delrt_offset = (delrt - delrt[0]).astype('int')\n_delrt_offset_samples = twt2samples(_delrt_offset, dt=dt).astype('int')\n_idx_amp4twt = _idx_amp + _delrt_offset_samples\ntwt_seafloor = twt[_idx_amp4twt]\n# compute static offset using peak seafloor amplitude sample indices\nstatic_samples = get_static(\nidx_amp,\nkind='diff',\ninterp_kind='cubic',\nwin_mad=args.win_mad,\nwin_sg=args.win_sg,\nlimit_perc=False,\nlimit_samples=args.limit_shift,\nlimit_by_MAD=3,\nlimit_depressions=args.limit_depressions,\n)\n# compute static corrected data\ndata_corr, static_samples = compensate_static(\ndata_src, static_samples, dt=dt, units='ms'\n)\n# === OUTPUT ===\nstatic_ms = samples2twt(static_samples, dt=dt)\nif args.write_aux:\nheader_names = ['tracl', 'tracr', 'fldr', 'static_samples', 'static_ms']\nif args.mode == 'swdep':\nheader_names += ['swdep_m']\nelif args.mode == 'amp':\nheader_names += ['seafloor_ms']\nwith open(\nos.path.join(args.output_dir, f'{out_name}.sta'), mode='w', newline='\\n'\n) as sta:\nsta.write(','.join(header_names) + '\\n')\nfor i in range(len(tracr)):\nline = (\nf'{tracl[i]},{tracr[i]},{fldr[i]},'\n+ f'{static_samples[i]:d},{static_ms[i]:.3f},'\n)\nif args.mode == 'swdep':\nline += f'{swdep[i]:.2f}\\n'\nelif args.mode == 'amp':\nline += f'{twt_seafloor[i]:.2f}\\n'\nsta.write(line)\n# update textual header\nfield_static = segyio.TraceField.TotalStaticApplied\nfield_scalar = segyio.TraceField.UnassignedInt1\nfield_amp = segyio.TraceField.UnassignedInt2\n# global text, text_updated, text_out\ntext = get_textual_header(path)\ninfo = (\nf'STATIC CORRECTION:{args.mode} (byte:{field_static}) with SCALAR (byte:{field_scalar})'\n)\ntext_updated = add_processing_info_header(text, info, prefix='_TODAY_')\nif args.mode == 'amp' and args.write_seafloor2trace:\ninfo = f'-&gt; SEAFLOOR (byte:{field_amp}) with SCALAR (byte:{field_scalar})'\ntext_updated = add_processing_info_header(text_updated, info, prefix='_TODAY_')\nwrite_textual_header(path, text_updated)\n# update trace header info\nstatic_scalar = 1000\nstatic_ms = (static_ms * static_scalar).astype('int32')  # ms to ns\nif args.mode == 'amp' and args.write_seafloor2trace:\ntwt_seafloor = (twt_seafloor * static_scalar).astype('int32')  # ms to ns\nfor i, h in enumerate(src.header[:]):\nh.update(\n{\nfield_static: static_ms[i],  # byte 103: TotalStaticApplied (ms)\nfield_scalar: -static_scalar,\n}\n)  # byte 233: custom scalar for TotalStaticApplied\nif args.mode == 'amp' and args.write_seafloor2trace:\nh.update(\n{field_amp: twt_seafloor[i]}\n)  # byte 237: save detected seafloor amplitude TWT (ms)\n# write corrected traces to file\nsrc.trace = np.ascontiguousarray(data_corr.T, dtype=data_src.dtype)\ntry:\nsrc.close()\nexcept IOError:\nxprint('SEG-Y file was already closed.', kind='warning', verbosity=args.verbose)\nreturn path, data_src, data_corr  # , static_samples, dt, tracl, tracr, delrt\n</code></pre>"},{"location":"api/api_tide_compensation_segy/","title":"<code>tide_compensation_segy.py</code>","text":"<p>Compensate tidal effect for SEG-Y file(s). Using the OSU TPXO9 atlas tide model with the <code>tpxo-tide-prediction</code> package as an interface.</p>"},{"location":"api/api_tide_compensation_segy/#pseudo_3D_interpolation.tide_compensation_segy--references","title":"References","text":"<ol> <li> <p>Oregon State University (OSU), https://www.tpxo.net/home \u21a9</p> </li> <li> <p>TPXO9-atlas models, https://www.tpxo.net/global/tpxo9-atlas \u21a9</p> </li> <li> <p><code>tpxo-tide-prediction</code>, https://github.com/fwrnke/tpxo-tide-prediction \u21a9</p> </li> </ol>"},{"location":"api/api_tide_compensation_segy/#pseudo_3D_interpolation.tide_compensation_segy.compensate_tide","title":"<code>compensate_tide(data, tide, dt, tide_units='meter', units='ms', v=1500, verbosity=1)</code>","text":"<p>Apply predicted tide offset to seismic traces.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Seismic section (samples x traces).</p> </li> <li> tide             (<code>np.ndarray</code>)         \u2013 <p>Predicted tide for each trace location (1D array).</p> </li> <li> dt             (<code>float</code>)         \u2013 <p>Sampling interval in specified units (default: <code>ms</code>).</p> </li> <li> tide_units             (<code>str, optional</code>)         \u2013 <p>Units of predicted tide values. Either elevation (<code>meter</code>, default), two-way travel time (<code>s</code>, <code>ms</code>) or <code>samples</code>.</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Time unit (for dt) (default: <code>ms</code>).</p> </li> <li> v             (<code>int, optional</code>)         \u2013 <p>Acoustic velocity used for depth-time conversion (default: <code>1500</code> m/s).</p> </li> </ul> <p>Returns:</p> <ul> <li> data_comp(            <code>np.ndarray(s)</code> )        \u2013 <p>Compensated seismic section.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\tide_compensation_segy.py</code> <pre><code>def compensate_tide(\ndata, tide, dt: float, tide_units: str = 'meter', units: str = 'ms', v: int = 1500, verbosity=1\n):\n\"\"\"\n    Apply predicted tide offset to seismic traces.\n    Parameters\n    ----------\n    data : np.ndarray\n        Seismic section (samples x traces).\n    tide : np.ndarray\n        Predicted tide for each trace location (1D array).\n    dt : float\n        Sampling interval in specified units (default: `ms`).\n    tide_units : str, optional\n        Units of predicted tide values. Either elevation (`meter`, _default_),\n        two-way travel time (`s`, `ms`) or `samples`.\n    units : str, optional\n        Time unit (for dt) (default: `ms`).\n    v : int, optional\n        Acoustic velocity used for depth-time conversion (default: `1500` m/s).\n    Returns\n    -------\n    data_comp : np.ndarray(s)\n        Compensated seismic section.\n    \"\"\"\n# convert dt to seconds\nif units == 's':\npass\nelif units == 'ms':\ndt = dt / 1000\nelif units == 'ns':\ndt = dt / 1e-6\nif tide_units == 'meter':\ntide_samples = depth2samples(tide, dt, v=v, units='s')\nelif tide_units in ['s', 'ms']:\ntide_samples = twt2samples(tide, dt, units='s')\nelif tide_units == 'samples':\ntide_samples = tide\nelse:\nraise ValueError(f'Provided unknown unit &lt; {tide_units} &gt; for tide values.')\ntide_samples = np.around(tide_samples, 0).astype('int32')\n# create (transposed) copy of original data\ndata_comp = data.T.copy()\nfor i, col in enumerate(data_comp):\noffset = tide_samples[i]\nif offset &gt; 0:\ncol[:] = np.hstack((col[abs(offset) :], np.zeros(abs(offset))))\nxprint(\nf'trace #{i}:{offset:&gt;5}   -&gt;   up: {col.shape}', kind='debug', verbosity=verbosity\n)\nelif offset &lt; 0:\ncol[:] = np.hstack((np.zeros(abs(offset)), col[: -abs(offset)]))\nxprint(\nf'trace #{i}:{offset:&gt;5}   -&gt;   down: {col.shape}',\nkind='debug',\nverbosity=verbosity,\n)\nelse:\npass  # no static correction\nreturn data_comp.T\n</code></pre>"},{"location":"api/api_tide_compensation_segy/#pseudo_3D_interpolation.tide_compensation_segy.wrapper_tide_compensation","title":"<code>wrapper_tide_compensation(in_path, args)</code>","text":"<p>Compensate tidal effect for single SEG-Y.</p> Source code in <code>pseudo_3D_interpolation\\tide_compensation_segy.py</code> <pre><code>def wrapper_tide_compensation(in_path, args):\n\"\"\"Compensate tidal effect for single SEG-Y.\"\"\"\nbasepath, filename = os.path.split(in_path)\nbasename, suffix = os.path.splitext(filename)\nxprint(f'Processing file &lt; {filename} &gt;', kind='info', verbosity=args.verbose)\nif os.path.isdir(args.output_dir) and args.inplace is False:\nxprint('Creating copy of file in output directory', kind='info', verbosity=args.verbose)\nif args.txt_suffix is not None:\nout_name = f'{basename}_{args.txt_suffix}'\nelse:\nout_name = f'{basename}_tide'\nout_path = os.path.join(args.output_dir, f'{out_name}{suffix}')\n# sanity check\nif os.path.isfile(out_path):\nxprint(\n'*** Output file already exists and will be removed! ***',\nkind='warning',\nverbosity=args.verbose,\n)\nos.remove(out_path)\ncopy2(in_path, out_path)\npath = out_path\nelif not os.path.isdir(args.output_dir) and args.inplace is True:\nxprint('Updating SEG-Y inplace', kind='warning', verbosity=args.verbose)\npath = in_path\n# get coordinate byte positions\nsrc_coords_bytes = TRACE_HEADER_COORDS.get(args.src_coords)\n# read SEGY file\nwith segyio.open(path, 'r+', strict=False, ignore_geometry=True) as src:\nn_traces = src.tracecount  # total number of traces\ndt = segyio.tools.dt(src) / 1000  # sample rate [ms]\nn_samples = src.samples.size  # total number of samples\n# twt = src.samples                   # two way travel time (TWTT) [ms]\nxprint(f'n_traces:  {n_traces}', kind='debug', verbosity=args.verbose)\nxprint(f'n_samples: {n_samples}', kind='debug', verbosity=args.verbose)\nxprint(f'dt:        {dt}', kind='debug', verbosity=args.verbose)\ntracl = src.attributes(segyio.TraceField.TRACE_SEQUENCE_LINE)[:]\ntracr = src.attributes(segyio.TraceField.TRACE_SEQUENCE_FILE)[:]\nfldr = src.attributes(segyio.TraceField.FieldRecord)[:]\n# get seismic data [amplitude]; transpose to fit numpy data structure\ndata_src = src.trace.raw[:].T  # eager version (completely read into memory)\n# get scaled coordinates\nxprint('Reading coordinates from SEG-Y file', kind='debug', verbosity=args.verbose)\nxcoords, ycoords, coordinate_units = scale_coordinates(src, src_coords_bytes)\n# get source &amp; destination CRS\ncrs_src = pyproj.crs.CRS(args.crs_src)\ncrs_dst = pyproj.crs.CRS('epsg:4326')\n# transform coordinate to geographical coordinates\ntransformer = pyproj.transformer.Transformer.from_crs(crs_src, crs_dst, always_xy=True)\nlon, lat = transformer.transform(xcoords, ycoords, errcheck=True)\n# filter duplicate lat/lon locations\nlatlon = np.vstack((lat, lon)).T\nlatlon_unique = np.unique(\nlatlon, axis=0, return_index=True, return_inverse=True, return_counts=True\n)\nlatlon_uniq, latlon_uniq_idx, latlon_uniq_inv, latlon_uniq_cnts = latlon_unique\n# get unique lat/lons locations\nlat, lon = latlon_uniq[:, 0], latlon_uniq[:, 1]\n# get datetime elements from trace headers\nxprint('Reading timestamps from SEG-Y file', kind='debug', verbosity=args.verbose)\ntimes = []\nfor h in src.header:\nyear = h[segyio.TraceField.YearDataRecorded]\nday_of_year = h[segyio.TraceField.DayOfYear]\nhour = h[segyio.TraceField.HourOfDay]\nminute = h[segyio.TraceField.MinuteOfHour]\nsecond = h[segyio.TraceField.SecondOfMinute]\ndt_string = datetime.datetime.strptime(\nf'{year}-{day_of_year} {hour}:{minute}:{second}', '%Y-%j %H:%M:%S'\n).strftime('%Y-%m-%dT%H:%M:%S')\ntimes.append(np.datetime64(dt_string, 's'))\ntimes = np.asarray(times)\n# get times masked by unique lat/lons locations\ntimes = times[latlon_uniq_idx]\n# predict tides along SEG-Y file at given times\nxprint('Predicting tidal elevation along profile', kind='debug', verbosity=args.verbose)\ntides_track = tide_predict(\nargs.model_dir,\nlat,\nlon,\ntimes,\nargs.constituents,\ncorrect_minor=args.correct_minor,\nmode='track',\n)[\nlatlon_uniq_inv\n]  # set tide for original lat/lon locations (INCLUDING duplicates)\n# reset times (INCLUDING duplicates) -&gt; for output\ntimes = times[latlon_uniq_inv]\n# apply tidal compensation to seismic traces\nxprint('Compensating tide', kind='debug', verbosity=args.verbose)\ndata_comp = compensate_tide(\ndata_src, tides_track, dt, tide_units='meter', units='ms', verbosity=args.verbose\n)\n# set output amplitudes (transpose to fit SEG-Y format)\nxprint('Writing compensated data to disk', kind='debug', verbosity=args.verbose)\nsrc.trace = np.ascontiguousarray(data_comp.T, dtype=data_src.dtype)\n# update textual header\ntext = get_textual_header(path)\ntext_updated = add_processing_info_header(\ntext, 'TIDE COMPENSATION', prefix='_TODAY_', newline=True\n)\nwrite_textual_header(path, text_updated)\nif args.write_aux:\ntides_twt = depth2twt(tides_track)\ntides_samples = np.around(depth2samples(tides_track, dt=dt, units='ms'), 0)\naux_path = os.path.join(args.output_dir, f'{out_name}.tid')\nxprint(f'Creating auxiliary file &lt; {out_name}.tid &gt;', kind='debug', verbosity=args.verbose)\nwith open(aux_path, 'w', newline='\\n') as fout:\nfout.write('tracl,tracr,fldr,time,tide_m,tide_ms,tide_samples\\n')\nfor i in range(tides_track.size):\nline = (\nf'{tracl[i]},{tracr[i]},{fldr[i]},'\n+ f'{np.datetime_as_string(times[i],\"s\")},'\n+ f'{tides_track[i]:.6f},{tides_twt[i]*1000:.3f},'\n+ f'{tides_samples[i]:.0f}\\n'\n)\nfout.write(line)\n</code></pre>"},{"location":"api/functions/api_POCS/","title":"<code>POCS.py</code>","text":"<p>Functions used for POCS interpolation script.</p>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.get_number_scales","title":"<code>get_number_scales(x)</code>","text":"<p>Compute number of shearlet scales based on input array shape.</p>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.get_number_scales--references","title":"References","text":"<ol> <li> <p>https://github.com/grlee77/PyShearlets/blob/master/FFST/_scalesShearsAndSpectra.py \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\POCS.py</code> <pre><code>def get_number_scales(x):\n\"\"\"\n    Compute number of shearlet scales based on input array shape.\n    References\n    ----------\n    [^1]: [https://github.com/grlee77/PyShearlets/blob/master/FFST/_scalesShearsAndSpectra.py](https://github.com/grlee77/PyShearlets/blob/master/FFST/_scalesShearsAndSpectra.py)\n    \"\"\"\nscales = int(np.floor(0.5 * np.log2(np.max(x.shape))))\nreturn scales if scales &gt;= 1 else 1\n</code></pre>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.threshold","title":"<code>threshold(data, thresh, sub=0, kind='soft')</code>","text":"<p>Apply user-defined threshold to input data (2D).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data.</p> </li> <li> thresh             (<code>float, complex</code>)         \u2013 <p>Threshold cut-off value.</p> </li> <li> sub             (<code>int, float, optional</code>)         \u2013 <p>Substitution value (default: <code>0</code>).</p> </li> <li> kind             (<code>str, optional</code>)         \u2013 <p>Threshold method:</p> <ul> <li><code>soft</code> (default)</li> <li><code>garrote</code></li> <li><code>hard</code></li> <li><code>soft-percentile</code></li> <li><code>garrote-percentile</code></li> <li><code>hard-percentile</code></li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Updated input array using specified thresholding function.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\POCS.py</code> <pre><code>def threshold(data, thresh, sub=0, kind='soft'):\n\"\"\"\n    Apply user-defined threshold to input data (2D).\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data.\n    thresh : float, complex\n        Threshold cut-off value.\n    sub : int, float, optional\n        Substitution value (default: `0`).\n    kind : str, optional\n        Threshold method:\n          - `soft` (**default**)\n          - `garrote`\n          - `hard`\n          - `soft-percentile`\n          - `garrote-percentile`\n          - `hard-percentile`\n    Returns\n    -------\n    np.ndarray\n        Updated input array using specified thresholding function.\n    \"\"\"\ndata = np.asarray(data)\nif kind == 'soft':\nreturn _soft_threshold(data, thresh, sub)\nelif kind == 'hard':\nreturn _hard_threshold(data, thresh, sub)\nelif kind == 'soft-percentile':\nreturn _soft_threshold_perc(data, thresh, sub)\nelif kind == 'hard-percentile':\nreturn _hard_threshold_perc(data, thresh, sub)\nelif kind in ['garotte', 'garrote']:\nreturn _nn_garrote(data, thresh, sub)\nelif kind in ['garotte-percentile', 'garrote-percentile']:\nreturn _nn_garrote_perc(data, thresh, sub)\n</code></pre>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.threshold_wavelet","title":"<code>threshold_wavelet(data, thresh, sub=0, kind='soft')</code>","text":"<p>Apply user-defined threshold to input data (2D). Compatible with output from <code>pywavelet.wavedec2</code> (multilevel Discrete Wavelet Transform).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data.</p> </li> <li> thresh             (<code>float, complex</code>)         \u2013 <p>Threshold cut-off value.</p> </li> <li> sub             (<code>int, float, optional</code>)         \u2013 <p>Substitution value (default: <code>0</code>).</p> </li> <li> kind             (<code>str, optional</code>)         \u2013 <p>Threshold method:</p> <ul> <li><code>soft</code> (default)</li> <li><code>garrote</code></li> <li><code>hard</code></li> <li><code>soft-percentile</code></li> <li><code>garrote-percentile</code></li> <li><code>hard-percentile</code></li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Updated input array using specified thresholding function.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\POCS.py</code> <pre><code>def threshold_wavelet(data, thresh, sub=0, kind='soft'):\n\"\"\"\n    Apply user-defined threshold to input data (2D).\n    Compatible with output from `pywavelet.wavedec2` (multilevel Discrete Wavelet Transform).\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data.\n    thresh : float, complex\n        Threshold cut-off value.\n    sub : int, float, optional\n        Substitution value (default: `0`).\n    kind : str, optional\n        Threshold method:\n          - `soft` (**default**)\n          - `garrote`\n          - `hard`\n          - `soft-percentile`\n          - `garrote-percentile`\n          - `hard-percentile`\n    Returns\n    -------\n    np.ndarray\n        Updated input array using specified thresholding function.\n    \"\"\"\nthresh = [list(d) for d in list(thresh)]\ndlen = len(data[-1])\nif kind == 'soft':\nreturn [\n[_soft_threshold(data[lvl][d], thresh[lvl][d], sub) for d in range(dlen)]\nfor lvl in range(len(data))\n]\nelif kind == 'hard':\nreturn [\n[_hard_threshold(data[lvl][d], thresh[lvl][d], sub) for d in range(dlen)]\nfor lvl in range(len(data))\n]\nelif kind == 'soft-percentile':\nreturn [\n[_soft_threshold_perc(data[lvl][d], thresh[lvl][d], sub) for d in range(dlen)]\nfor lvl in range(len(data))\n]\nelif kind == 'hard-percentile':\nreturn [\n[_hard_threshold_perc(data[lvl][d], thresh[lvl][d], sub) for d in range(dlen)]\nfor lvl in range(len(data))\n]\nelif kind in ['garotte', 'garrote']:\nreturn [\n[_nn_garrote(data[lvl][d], thresh[lvl][d], sub) for d in range(dlen)]\nfor lvl in range(len(data))\n]\nelif kind in ['garotte-percentile', 'garrote-percentile']:\nreturn [\n[_nn_garrote_perc(data[lvl][d], thresh[lvl][d], sub) for d in range(dlen)]\nfor lvl in range(len(data))\n]\n</code></pre>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.get_threshold_decay","title":"<code>get_threshold_decay(thresh_model, niter, transform_kind=None, p_max=0.99, p_min=0.001, x_fwd=None, kind='values')</code>","text":"<p>Calculate iteration-based decay for thresholding function. Can be one of the following:</p> <ul> <li><code>values</code> (based on max value in data)</li> <li><code>factors</code> (for usage as multiplier).</li> </ul> <p>Parameters:</p> <ul> <li> thresh_model             (<code>str</code>)         \u2013 <p>Thresholding decay function.</p> <ul> <li><code>linear</code>                  Gao et al. (2010)</li> <li><code>exponential</code>             Yang et al. (2012), Zhang et al. (2015), Zhao et al. (2021)</li> <li><code>data-driven</code>             Gao et al. (2013)</li> <li><code>inverse_proportional</code>    Ge et al. (2015)</li> </ul> </li> <li> niter             (<code>int</code>)         \u2013 <p>Maximum number of iterations.</p> </li> <li> transform_kind             (<code>str</code>)         \u2013 <p>Name of the specified transform (e.g. FFT, WAVELET, SHEARLET, CURVELET).</p> </li> <li> p_max             (<code>float, optional</code>)         \u2013 <p>Maximum regularization percentage (float).</p> </li> <li> p_min             (<code>float, str, optional</code>)         \u2013 <p>Minimum regularization percentage (float) or 'adaptive': adaptive calculation of minimum threshold according to sparse coefficient.</p> </li> <li> x_fwd             (<code>np.ndarray, optional</code>)         \u2013 <p>Forward transformed input data (required for thresh_model=<code>data-driven</code> and kind=<code>values</code>).</p> </li> <li> kind             (<code>str, optional</code>)         \u2013 <p>Return either data <code>values</code> or multiplication <code>factors</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> tau(            <code>np.ndarray</code> )        \u2013 <p>Array of decay values or factors (based on \"kind\" paramter).</p> </li> </ul>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.get_threshold_decay--references","title":"References","text":"<ol> <li> <p>Gao, J.-J., Chen, X.-H., Li, J.-Y., Liu, G.-C., &amp; Ma, J. (2010). Irregular seismic data reconstruction based on exponential threshold model of POCS method. Applied Geophysics, 7(3), 229\u2013238. https://doi.org/10.1007/s11770-010-0246-5 \u21a9</p> </li> <li> <p>Yang, P., Gao, J., &amp; Chen, W. (2012). Curvelet-based POCS interpolation of nonuniformly sampled seismic records. Journal of Applied Geophysics, 79, 90\u201399. https://doi.org/10.1016/j.jappgeo.2011.12.004 \u21a9</p> </li> <li> <p>Zhang, H., Chen, X., &amp; Li, H. (2015). 3D seismic data reconstruction based on complex-valued curvelet transform in frequency domain. Journal of Applied Geophysics, 113, 64\u201373. https://doi.org/10.1016/j.jappgeo.2014.12.004 \u21a9</p> </li> <li> <p>Zhao, H., Yang, T., Ni, Y.-D., Liu, X.-G., Xu, Y.-P., Zhang, Y.-L., &amp; Zhang, G.-R. (2021). Reconstruction method of irregular seismic data with adaptive thresholds based on different sparse transform bases. Applied Geophysics, 18(3), 345\u2013360. https://doi.org/10.1007/s11770-021-0903-5 \u21a9</p> </li> <li> <p>Gao, J., Stanton, A., Naghizadeh, M., Sacchi, M. D., &amp; Chen, X. (2013). Convergence improvement and noise attenuation considerations for beyond alias projection onto convex sets reconstruction. Geophysical Prospecting, 61, 138\u2013151. https://doi.org/10.1111/j.1365-2478.2012.01103.x \u21a9</p> </li> <li> <p>Ge, Z.-J., Li, J.-Y., Pan, S.-L., &amp; Chen, X.-H. (2015). A fast-convergence POCS seismic denoising and reconstruction method. Applied Geophysics, 12(2), 169\u2013178. https://doi.org/10.1007/s11770-015-0485-1 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\POCS.py</code> <pre><code>def get_threshold_decay(\nthresh_model,\nniter: int,\ntransform_kind: str = None,\np_max: float = 0.99,\np_min: float = 1e-3,\nx_fwd=None,\nkind: str = 'values',\n):\n\"\"\"\n    Calculate iteration-based decay for thresholding function.\n    Can be one of the following:\n      - `values` (based on max value in data)\n      - `factors` (for usage as multiplier).\n    Parameters\n    ----------\n    thresh_model : str\n        Thresholding decay function.\n            - `linear`                  Gao et al. (2010)\n            - `exponential`             Yang et al. (2012), Zhang et al. (2015), Zhao et al. (2021)\n            - `data-driven`             Gao et al. (2013)\n            - `inverse_proportional`    Ge et al. (2015)\n    niter : int\n        Maximum number of iterations.\n    transform_kind : str\n        Name of the specified transform (e.g. FFT, WAVELET, SHEARLET, CURVELET).\n    p_max : float, optional\n        Maximum regularization percentage (float).\n    p_min : float, str, optional\n        Minimum regularization percentage (float) or\n        'adaptive': adaptive calculation of minimum threshold according to sparse coefficient.\n    x_fwd : np.ndarray, optional\n        Forward transformed input data (required for thresh_model=`data-driven` and kind=`values`).\n    kind : str, optional\n        Return either data `values` or multiplication `factors`.\n    Returns\n    -------\n    tau : np.ndarray\n        Array of decay values or factors (based on \"kind\" paramter).\n    References\n    ----------\n    [^1]: Gao, J.-J., Chen, X.-H., Li, J.-Y., Liu, G.-C., &amp; Ma, J. (2010).\n        Irregular seismic data reconstruction based on exponential threshold model of POCS method.\n        Applied Geophysics, 7(3), 229\u2013238. [https://doi.org/10.1007/s11770-010-0246-5](https://doi.org/10.1007/s11770-010-0246-5)\n    [^2]: Yang, P., Gao, J., &amp; Chen, W. (2012).\n        Curvelet-based POCS interpolation of nonuniformly sampled seismic records.\n        Journal of Applied Geophysics, 79, 90\u201399. [https://doi.org/10.1016/j.jappgeo.2011.12.004](https://doi.org/10.1016/j.jappgeo.2011.12.004)\n    [^3]: Zhang, H., Chen, X., &amp; Li, H. (2015).\n        3D seismic data reconstruction based on complex-valued curvelet transform in frequency domain.\n        Journal of Applied Geophysics, 113, 64\u201373. [https://doi.org/10.1016/j.jappgeo.2014.12.004](https://doi.org/10.1016/j.jappgeo.2014.12.004)\n    [^4]: Zhao, H., Yang, T., Ni, Y.-D., Liu, X.-G., Xu, Y.-P., Zhang, Y.-L., &amp; Zhang, G.-R. (2021).\n        Reconstruction method of irregular seismic data with adaptive thresholds based on different sparse transform bases.\n        Applied Geophysics, 18(3), 345\u2013360. [https://doi.org/10.1007/s11770-021-0903-5](https://doi.org/10.1007/s11770-021-0903-5)\n    [^5]: Gao, J., Stanton, A., Naghizadeh, M., Sacchi, M. D., &amp; Chen, X. (2013).\n        Convergence improvement and noise attenuation considerations for beyond alias projection onto convex sets reconstruction.\n        Geophysical Prospecting, 61, 138\u2013151. [https://doi.org/10.1111/j.1365-2478.2012.01103.x](https://doi.org/10.1111/j.1365-2478.2012.01103.x)\n    [^6]: Ge, Z.-J., Li, J.-Y., Pan, S.-L., &amp; Chen, X.-H. (2015).\n        A fast-convergence POCS seismic denoising and reconstruction method.\n        Applied Geophysics, 12(2), 169\u2013178. [https://doi.org/10.1007/s11770-015-0485-1](https://doi.org/10.1007/s11770-015-0485-1)\n    \"\"\"\nTRANSFORMS = ('FFT', 'WAVELET', 'SHEARLET', 'CURVELET', 'DCT')\nif transform_kind is None:\npass\nelif transform_kind.upper() not in TRANSFORMS and (\nkind == 'values' or thresh_model == 'data-driven'\n):\nraise ValueError(f'Unsupported transform. Please select one of: {TRANSFORMS}')\nelif transform_kind is not None:\ntransform_kind = transform_kind.upper()\nif x_fwd is None and (kind == 'values' or thresh_model == 'data-driven'):\nraise ValueError(\n'`x_fwd` must be specified for thresh_model=\"data-driven\" or kind=\"values\"!'\n)\n# (A) inversely proportional threshold model (Ge et al., 2015)\nif all([s in thresh_model for s in ['inverse', 'proportional']]):\nif transform_kind == 'WAVELET':\nx_fwd_max = np.asarray([[np.abs(d).max() for d in level] for level in x_fwd])\nx_fwd_min = np.asarray([[np.abs(d).min() for d in level] for level in x_fwd])\n_iiter = np.arange(1, niter + 1)[:, None, None]\nelif transform_kind == 'SHEARLET':\nx_fwd_max = np.max(np.abs(x_fwd), axis=(0, 1))\nx_fwd_min = np.min(np.abs(x_fwd), axis=(0, 1))\n_iiter = np.arange(1, niter + 1)[:, None]\nelif transform_kind in ['FFT', 'CURVELET', 'DCT']:\nx_fwd_max = np.abs(x_fwd).max()\nx_fwd_min = np.abs(x_fwd).min()\n_iiter = np.arange(1, niter + 1)\n# arbitrary variable to adjust descent rate (most cases: 1 &lt;= q &lt;=3)\nq = thresh_model.split('-')[-1] if '-' in thresh_model else 1.0\ntry:\nq = float(q)\nexcept:  # noqa\nq = 1.0\na = (niter**q * (x_fwd_max - x_fwd_min)) / (niter**q - 1)\nb = (niter**q * x_fwd_min - x_fwd_max) / (niter**q - 1)\nreturn a / (_iiter**q) + b\n# (B) \"classic\" thresholding models\nif kind == 'values':\n# max (absolute) value in forward transformed data\nif transform_kind == 'WAVELET':\n# x_fwd_max = np.asarray([[np.abs(d).max() for d in level] for level in x_fwd])\nx_fwd_max = np.asarray([[d.max() for d in level] for level in x_fwd])\nelif transform_kind == 'SHEARLET':\naxis = (0, 1)\n# x_fwd_max = np.max(np.abs(x_fwd), axis=axis)\nx_fwd_max = np.max(x_fwd, axis=axis)\nelif transform_kind in ['FFT', 'CURVELET', 'DCT']:\naxis = None\n# x_fwd_max = np.abs(x_fwd).max()  # FIXME\nx_fwd_max = x_fwd.max()\nelse:\nraise ValueError(\n'`transform_kind` must be specified for thresh_model=\"data-driven\" or kind=\"values\"!'\n)\n# min/max regularization factors\n#   adaptive calculation of minimum threshold (Zhao et al., 2021)\nif isinstance(p_min, str) and p_min == 'adaptive':\n# single-scale transform\nif transform_kind in ['FFT', 'DCT']:\ntau_min = 0.01 * np.sqrt(np.linalg.norm(x_fwd, axis=axis) ** 2 / x_fwd.size)\n# mulit-scale transform\nelif transform_kind in ['SHEARLET']:\n# calculate regularization factor `tau_min` for each scale\nnscales = get_number_scales(x_fwd)\nj = np.hstack(\n(\nnp.array([0]),  # low-pass solution\nnp.repeat(\nnp.arange(1, nscales + 1), [2 ** (j + 2) for j in range(nscales)]\n),\n)\n)\ntau_min = (\n1\n/ 3\n* np.median(  # noqa\nnp.log10(j + 1)\n* np.sqrt(np.linalg.norm(x_fwd, axis=axis) ** 2 / x_fwd.size)\n)\n)\nelse:\nraise NotImplementedError(\nf'p_min=`adaptive` is not implemented for {transform_kind} transform'\n)\nelse:\ntau_min = p_min * x_fwd_max\ntau_max = p_max * x_fwd_max\nelif kind == 'factors':\ntau_max = p_max\ntau_min = p_min\nelse:\nraise ValueError('Parameter `kind` only supports arguments \"values\" or \"factors\"')\n# --- iteration-based threshold factor ---\n_iiter = np.arange(1, niter + 1)\nif transform_kind == 'WAVELET':\nimultiplier = ((_iiter - 1) / (niter - 1))[:, None, None]\nelif transform_kind == 'SHEARLET':\nimultiplier = ((_iiter - 1) / (niter - 1))[:, None]\nelif transform_kind in ['FFT', 'CURVELET', 'DCT']:\nimultiplier = (_iiter - 1) / (niter - 1)\nelif transform_kind is None:\nimultiplier = (_iiter - 1) / (niter - 1)\n# --- thresholding operator ---\nif thresh_model == 'linear':\ntau = tau_max - (tau_max - tau_min) * imultiplier\nelif 'exponential' in thresh_model:\nq = float(thresh_model.split('-')[-1]) if '-' in thresh_model else 1.0  # Zhao et al. (2021)\nc = np.log(tau_min / tau_max)\ntau = tau_max * np.exp(c * imultiplier**q)\nelif thresh_model == 'data-driven' and transform_kind in ['FFT', 'DCT', 'CURVELET']:\ntau = np.zeros((_iiter.size,), dtype=x_fwd.dtype)\nidx = (x_fwd &gt; tau_min) &amp; (x_fwd &lt; tau_max)\nv = np.sort(x_fwd[idx])[::-1]\nNv = v.size\ntau[0] = v[0]\ntau[1:] = v[np.ceil((_iiter[1:] - 1) * (Nv - 1) / (niter - 1)).astype('int')]\nelse:\nraise NotImplementedError(\nf'{thresh_model} is not implemented for {transform_kind} transform!'\n)\nreturn tau\n</code></pre>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.POCS_algorithm","title":"<code>POCS_algorithm(x, mask, auxiliary_data=None, transform=None, itransform=None, transform_kind=None, niter=50, thresh_op='hard', thresh_model='exponential', eps=1e-09, alpha=1.0, p_max=0.99, p_min=1e-05, sqrt_decay=False, decay_kind='values', verbose=False, version='regular', results_dict=None, path_results=None)</code>","text":"<p>Interpolate sparse input grid using Point Onto Convex Sets (POCS) algorithm. Applying a user-specified transform method:</p> <ul> <li><code>FFT</code></li> <li><code>Wavelet</code></li> <li><code>Shearlet</code></li> <li><code>Curvelet</code></li> </ul> <p>Parameters:</p> <ul> <li> x             (<code>np.ndarray</code>)         \u2013 <p>Sparse input data (2D).</p> </li> <li> mask             (<code>np.ndarray</code>)         \u2013 <p>Boolean mask of input data (<code>1</code>: data cell, <code>0</code>: nodata cell).</p> </li> <li> auxiliary_data         \u2013 <p>Auxiliary data only required by <code>shearlet</code> transform.</p> </li> <li> transform             (<code>callable</code>)         \u2013 <p>Forward transform to apply.</p> </li> <li> itransform             (<code>callable</code>)         \u2013 <p>Inverse transform to apply.</p> </li> <li> transform_kind             (<code>str</code>)         \u2013 <p>Name of the specified transform.</p> </li> <li> niter             (<code>int, optional</code>)         \u2013 <p>Maximum number of iterations (default: <code>50</code>).</p> </li> <li> thresh_op             (<code>str, optional</code>)         \u2013 <p>Threshold operator (default: <code>soft</code>).</p> </li> <li> thresh_model             (<code>str, optional</code>)         \u2013 <p>Thresholding decay function.</p> <ul> <li><code>linear</code>                   Gao et al. (2010)</li> <li><code>exponential</code>              Yang et al. (2012), Zhang et al. (2015), Zhao et al. (2021)</li> <li><code>data-driven</code>              Gao et al. (2013)</li> <li><code>inverse_proportional</code>     Ge et al. (2015)</li> </ul> </li> <li> eps             (<code>float, optional</code>)         \u2013 <p>Covergence threshold (default: <code>1e-9</code>).</p> </li> <li> alpha             (<code>float, optional</code>)         \u2013 <p>Weighting factor to scale re-insertion of input data (default: <code>1.0</code>).</p> </li> <li> sqrt_decay             (<code>bool, optional</code>)         \u2013 <p>Use squared decay values for thresholding (default: <code>False</code>).</p> </li> <li> decay_kind             (<code>str, optional</code>)         \u2013 <p>Return either data \"values\" or multiplication \"factors\".</p> </li> <li> verbose             (<code>bool, optional</code>)         \u2013 <p>Print information about iteration steps (default: <code>False</code>).</p> </li> <li> version             (<code>str, optional</code>)         \u2013 <p>Version of POCS algorithm. One of the following:</p> <ul> <li><code>regular</code>     Abma and Kabir (2006), Yang et al. (2012)</li> <li><code>fast</code>        Yang et al. (2013), Gan et al (2015)</li> <li><code>adaptive</code>    Wang et al. (2015, 2016)</li> </ul> </li> <li> results_dict             (<code>dict, optional</code>)         \u2013 <p>If provided: return dict with total iterations, runtime (in seconds) and cost function.</p> </li> </ul> <p>Returns:</p> <ul> <li> x_inv(            <code>np.ndarray</code> )        \u2013 <p>Reconstructed (i.e. interpolated) input data.</p> </li> </ul>"},{"location":"api/functions/api_POCS/#pseudo_3D_interpolation.functions.POCS.POCS_algorithm--references","title":"References","text":"<ol> <li> <p>Gao, J.-J., Chen, X.-H., Li, J.-Y., Liu, G.-C., &amp; Ma, J. (2010). Irregular seismic data reconstruction based on exponential threshold model of POCS method. Applied Geophysics, 7(3), 229\u2013238. https://doi.org/10.1007/s11770-010-0246-5 \u21a9</p> </li> <li> <p>Yang, P., Gao, J., &amp; Chen, W. (2012). Curvelet-based POCS interpolation of nonuniformly sampled seismic records. Journal of Applied Geophysics, 79, 90\u201399. https://doi.org/10.1016/j.jappgeo.2011.12.004 \u21a9</p> </li> <li> <p>Zhang, H., Chen, X., &amp; Li, H. (2015). 3D seismic data reconstruction based on complex-valued curvelet transform in frequency domain. Journal of Applied Geophysics, 113, 64\u201373. https://doi.org/10.1016/j.jappgeo.2014.12.004 \u21a9</p> </li> <li> <p>Zhao, H., Yang, T., Ni, Y.-D., Liu, X.-G., Xu, Y.-P., Zhang, Y.-L., &amp; Zhang, G.-R. (2021). Reconstruction method of irregular seismic data with adaptive thresholds based on different sparse transform bases. Applied Geophysics, 18(3), 345\u2013360. https://doi.org/10.1007/s11770-021-0903-5 \u21a9</p> </li> <li> <p>Gao, J., Stanton, A., Naghizadeh, M., Sacchi, M. D., &amp; Chen, X. (2013). Convergence improvement and noise attenuation considerations for beyond alias projection onto convex sets reconstruction. Geophysical Prospecting, 61, 138\u2013151. https://doi.org/10.1111/j.1365-2478.2012.01103.x \u21a9</p> </li> <li> <p>Ge, Z.-J., Li, J.-Y., Pan, S.-L., &amp; Chen, X.-H. (2015). A fast-convergence POCS seismic denoising and reconstruction method. Applied Geophysics, 12(2), 169\u2013178. https://doi.org/10.1007/s11770-015-0485-1 \u21a9</p> </li> <li> <p>Abma, R., &amp; Kabir, N. (2006). 3D interpolation of irregular data with a POCS algorithm. Geophysics, 71(6), E91\u2013E97. https://doi.org/10.1190/1.2356088 \u21a9</p> </li> <li> <p>Yang, P., Gao, J., &amp; Chen, W. (2013) On analysis-based two-step interpolation methods for randomly sampled seismic data. Computers &amp; Geosciences, 51, 449\u2013461. https://doi.org/10.1016/j.cageo.2012.07.023 \u21a9</p> </li> <li> <p>Gan, S., Wang, S., Chen, Y., Zhang, Y., &amp; Jin, Z. (2015). Dealiased Seismic Data Interpolation Using Seislet Transform With Low-Frequency Constraint. IEEE Geoscience and Remote Sensing Letters, 12(10), 2150\u20132154. https://doi.org/10.1109/LGRS.2015.2453119 \u21a9</p> </li> <li> <p>Wang, B., Wu, R.-S., Chen, X., &amp; Li, J. (2015). Simultaneous seismic data interpolation and denoising with a new adaptive method based on dreamlet transform. Geophysical Journal International, 201(2), 1182\u20131194. https://doi.org/10.1093/gji/ggv072 \u21a9</p> </li> <li> <p>Wang, B., Chen, X., Li, J., &amp; Cao, J. (2016). An Improved Weighted Projection Onto Convex Sets Method for Seismic Data Interpolation and Denoising. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 9(1), 228\u2013235. https://doi.org/10.1109/jstars.2015.2496374 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\POCS.py</code> <pre><code>def POCS_algorithm(\nx,\nmask,\nauxiliary_data=None,\ntransform=None,\nitransform=None,\ntransform_kind: str = None,\nniter: int = 50,\nthresh_op: str = 'hard',\nthresh_model: str = 'exponential',\neps: float = 1e-9,\nalpha: int = 1.0,\np_max: float = 0.99,\np_min: float = 1e-5,\nsqrt_decay: str = False,\ndecay_kind: str = 'values',\nverbose: bool = False,\nversion: str = 'regular',\nresults_dict: dict = None,\npath_results: str = None,\n):\n\"\"\"\n    Interpolate sparse input grid using Point Onto Convex Sets (POCS) algorithm.\n    Applying a user-specified **transform** method:\n      - `FFT`\n      - `Wavelet`\n      - `Shearlet`\n      - `Curvelet`\n    Parameters\n    ----------\n    x : np.ndarray\n        Sparse input data (2D).\n    mask : np.ndarray\n        Boolean mask of input data (`1`: data cell, `0`: nodata cell).\n    auxiliary_data: np.ndarray\n        Auxiliary data only required by `shearlet` transform.\n    transform : callable\n        Forward transform to apply.\n    itransform : callable\n        Inverse transform to apply.\n    transform_kind : str\n        Name of the specified transform.\n    niter : int, optional\n        Maximum number of iterations (default: `50`).\n    thresh_op : str, optional\n        Threshold operator (default: `soft`).\n    thresh_model : str, optional\n        Thresholding decay function.\n            - `linear`                   Gao et al. (2010)\n            - `exponential`              Yang et al. (2012), Zhang et al. (2015), Zhao et al. (2021)\n            - `data-driven`              Gao et al. (2013)\n            - `inverse_proportional`     Ge et al. (2015)\n    eps : float, optional\n        Covergence threshold (default: `1e-9`).\n    alpha : float, optional\n        Weighting factor to scale re-insertion of input data (default: `1.0`).\n    sqrt_decay : bool, optional\n        Use squared decay values for thresholding (default: `False`).\n    decay_kind : str, optional\n        Return either data \"values\" or multiplication \"factors\".\n    verbose : bool, optional\n        Print information about iteration steps (default: `False`).\n    version : str, optional\n        Version of POCS algorithm. One of the following:\n            - `regular`     Abma and Kabir (2006), Yang et al. (2012)\n            - `fast`        Yang et al. (2013), Gan et al (2015)\n            - `adaptive`    Wang et al. (2015, 2016)\n    results_dict : dict, optional\n        If provided: return dict with total iterations, runtime (in seconds) and cost function.\n    Returns\n    -------\n    x_inv : np.ndarray\n        Reconstructed (i.e. interpolated) input data.\n    References\n    ----------\n    [^1]: Gao, J.-J., Chen, X.-H., Li, J.-Y., Liu, G.-C., &amp; Ma, J. (2010).\n        Irregular seismic data reconstruction based on exponential threshold model of POCS method.\n        Applied Geophysics, 7(3), 229\u2013238. [https://doi.org/10.1007/s11770-010-0246-5](https://doi.org/10.1007/s11770-010-0246-5)\n    [^2]: Yang, P., Gao, J., &amp; Chen, W. (2012).\n        Curvelet-based POCS interpolation of nonuniformly sampled seismic records.\n        Journal of Applied Geophysics, 79, 90\u201399. [https://doi.org/10.1016/j.jappgeo.2011.12.004](https://doi.org/10.1016/j.jappgeo.2011.12.004)\n    [^3]: Zhang, H., Chen, X., &amp; Li, H. (2015).\n        3D seismic data reconstruction based on complex-valued curvelet transform in frequency domain.\n        Journal of Applied Geophysics, 113, 64\u201373. [https://doi.org/10.1016/j.jappgeo.2014.12.004](https://doi.org/10.1016/j.jappgeo.2014.12.004)\n    [^4]: Zhao, H., Yang, T., Ni, Y.-D., Liu, X.-G., Xu, Y.-P., Zhang, Y.-L., &amp; Zhang, G.-R. (2021).\n        Reconstruction method of irregular seismic data with adaptive thresholds based on different sparse transform bases.\n        Applied Geophysics, 18(3), 345\u2013360. [https://doi.org/10.1007/s11770-021-0903-5](https://doi.org/10.1007/s11770-021-0903-5)\n    [^5]: Gao, J., Stanton, A., Naghizadeh, M., Sacchi, M. D., &amp; Chen, X. (2013).\n        Convergence improvement and noise attenuation considerations for beyond alias projection onto convex sets reconstruction.\n        Geophysical Prospecting, 61, 138\u2013151. [https://doi.org/10.1111/j.1365-2478.2012.01103.x](https://doi.org/10.1111/j.1365-2478.2012.01103.x)\n    [^6]: Ge, Z.-J., Li, J.-Y., Pan, S.-L., &amp; Chen, X.-H. (2015).\n        A fast-convergence POCS seismic denoising and reconstruction method.\n        Applied Geophysics, 12(2), 169\u2013178. [https://doi.org/10.1007/s11770-015-0485-1](https://doi.org/10.1007/s11770-015-0485-1)\n    [^7]: Abma, R., &amp; Kabir, N. (2006). 3D interpolation of irregular data with a POCS algorithm.\n        Geophysics, 71(6), E91\u2013E97. [https://doi.org/10.1190/1.2356088](https://doi.org/10.1190/1.2356088)\n    [^8]: Yang, P., Gao, J., &amp; Chen, W. (2013)\n        On analysis-based two-step interpolation methods for randomly sampled seismic data.\n        Computers &amp; Geosciences, 51, 449\u2013461. [https://doi.org/10.1016/j.cageo.2012.07.023](https://doi.org/10.1016/j.cageo.2012.07.023)\n    [^9]: Gan, S., Wang, S., Chen, Y., Zhang, Y., &amp; Jin, Z. (2015).\n        Dealiased Seismic Data Interpolation Using Seislet Transform With Low-Frequency Constraint.\n        IEEE Geoscience and Remote Sensing Letters, 12(10), 2150\u20132154. [https://doi.org/10.1109/LGRS.2015.2453119](https://doi.org/10.1109/LGRS.2015.2453119)\n    [^10]:  Wang, B., Wu, R.-S., Chen, X., &amp; Li, J. (2015).\n        Simultaneous seismic data interpolation and denoising with a new adaptive method based on dreamlet transform.\n        Geophysical Journal International, 201(2), 1182\u20131194. [https://doi.org/10.1093/gji/ggv072](https://doi.org/10.1093/gji/ggv072)\n    [^11]: Wang, B., Chen, X., Li, J., &amp; Cao, J. (2016).\n        An Improved Weighted Projection Onto Convex Sets Method for Seismic Data Interpolation and Denoising.\n        IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 9(1), 228\u2013235.\n        [https://doi.org/10.1109/jstars.2015.2496374](https://doi.org/10.1109/jstars.2015.2496374)\n    \"\"\"\n# sanity checks\nif np.max(mask) &gt; 1:\nraise ValueError(f'mask should be quasi-boolean (0 or 1) but has maximum of {np.max(mask)}')\nif any(v is None for v in [transform, itransform]):\nraise ValueError('Forward and inverse transform function have to be supplied')\nTRANSFORMS = ('FFT', 'WAVELET', 'SHEARLET', 'CURVELET', 'DCT')\nif transform_kind.upper() not in TRANSFORMS:\nraise ValueError(f'Unsupported transform. Please select one of: {TRANSFORMS}')\nelse:\ntransform_kind = transform_kind.upper()\nif transform_kind == 'SHEARLET' and auxiliary_data is None:\nraise ValueError(\nf'{transform_kind} requires pre-computed shearlets in Fourier domain (Psi)'\n)\n# get input paramter\nis_complex_input = np.iscomplexobj(x)\nshape = x.shape\noriginal_shape = tuple(slice(s) for s in shape)\nif np.count_nonzero(x) == 0:\nniterations = 0\nruntime = 0\ncost = 0\ncosts = [0]\nx_inv = x\nelse:\n# initial forward transform\nif transform_kind == 'WAVELET':  # and isinstance(x_fwd, list):\nx_fwd = transform(x)[1:]  # exclude low-pass filter\nelif transform_kind == 'SHEARLET':  # and isinstance(x_fwd, tuple):\nx_fwd = transform(x, Psi=auxiliary_data)  # [0]   # output is like (ST, Psi)\nelif (\ntransform_kind == 'CURVELET'\nand hasattr(transform, '__name__')\nand transform.__name__ == 'matvec'\n):\nx_fwd = transform(x.ravel())\nelse:\nx_fwd = transform(x)\n# get threshold decay array\ndecay = get_threshold_decay(\nthresh_model=thresh_model,\nniter=niter,\ntransform_kind=transform_kind,\np_max=p_max,\np_min=p_min,\nx_fwd=x_fwd,\nkind=decay_kind,\n)\n# init data variables\nx_old = x\nx_inv = x\n# init variable for improved convergence (Yang et al., 2013)\nif version == 'fast':\nv = 1\nt0 = time.perf_counter()\nif path_results is not None:\ncosts = []\nfor iiter in range(niter):\nif verbose:\nprint(f'[Iteration: &lt;{iiter+1:3d}&gt;]')\nif version == 'regular':\nx_input = x_old\nelif version == 'fast':  # Yang et al. (2013)\n# improved convergence\nv1 = (1 + np.sqrt(1 + 4 * v**2)) / 2\nfrac = (v - 1) / (v1 + 1)  # Gan et al. (2015)\nv = v1\nx_input = x_inv + frac * (x_inv - x_old)  # prediction\nelif version == 'adaptive':  # Wang et al. (2015, 2016)\n# init adaptive input data\nx_tmp = alpha * x + (1 - alpha * mask) * x_old\nx_input = x_tmp + (1 - alpha) * (x - mask * x_old)\n# x_input = x_inv + (1 - alpha) * (x - mask * x_old)\n# (1) forward transform\nif (\ntransform_kind == 'CURVELET'\nand hasattr(transform, '__name__')\nand transform.__name__ == 'matvec'\n):\nX = transform(x_input.ravel())\nelif transform_kind == 'WAVELET':\nX = transform(x_input)\nlowpass = X[0].copy()\nX = X[1:]\nelif transform_kind == 'SHEARLET':\nX = transform(x_input, Psi=auxiliary_data)\nelse:\nX = transform(x_input)\n# (2) thresholding\n_decay = np.sqrt(decay[iiter]) if sqrt_decay else decay[iiter]\nif transform_kind == 'WAVELET' and isinstance(X, list):\nX_thresh = threshold_wavelet(X, _decay, kind=thresh_op)\nelse:\nX_thresh = threshold(X, _decay, kind=thresh_op)\n# (3) inverse transform\nif (\ntransform_kind == 'CURVELET'\nand hasattr(itransform, '__name__')\nand itransform.__name__ == 'rmatvec'\n):\nx_inv = itransform(X_thresh).reshape(shape)\nelif transform_kind == 'WAVELET':\nx_inv = itransform([lowpass] + X_thresh)[original_shape]\nelif transform_kind == 'SHEARLET':\nx_inv = itransform(X_thresh, Psi=auxiliary_data)\nelse:\nx_inv = itransform(X_thresh)\n# (4) apply mask (scaled by weighting factor)\nx_inv *= 1 - alpha * mask\n# (5) add original data (scaled by weighting factor)\nx_inv += x * alpha\n# cost function from Gao et al. (2013)\ncost = np.sum(np.abs(x_inv) - np.abs(x_old)) ** 2 / np.sum(np.abs(x_inv)) ** 2\nif path_results is not None:\ncosts.append(cost)\nif verbose:\nprint('[INFO]   cost:', cost)\n# set result from previous iteration as new input\nx_old = x_inv\nif iiter &gt; 2 and cost &lt; eps:\nbreak\nniterations = iiter + 1\nruntime = time.perf_counter() - t0\nif verbose:\nprint('\\n' + '-' * 20)\nprint(f'# iterations:  {niterations:4d}')\nprint(f'cost function: {cost}')\nprint(f'runtime:       {runtime:.3f} s')\nprint('-' * 20)\nif isinstance(results_dict, dict):\nresults_dict['niterations'] = niterations\nresults_dict['runtime'] = round(runtime, 3)\nresults_dict['cost'] = cost\nif path_results is not None:\nwith open(path_results, mode=\"a\", newline='\\n') as f:\nf.write(';'.join([str(i) for i in [niterations, runtime] + costs]) + '\\n')\nif is_complex_input:\nreturn x_inv\nreturn np.real(x_inv)\n</code></pre>"},{"location":"api/functions/api_backends/","title":"<code>backends.py</code>","text":"<p>Check availability of different packages.</p>"},{"location":"api/functions/api_filter/","title":"<code>filter.py</code>","text":"<p>Utility functions to filter (multidimensional) data.</p>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_window","title":"<code>moving_window(a, window_length, step_size=1)</code>","text":"<p>Create moving windows of given window length over input array (as view).</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>1D input array.</p> </li> <li> window_length             (<code>int</code>)         \u2013 <p>Length of moving window.</p> </li> <li> step_size             (<code>int</code>)         \u2013 <p>Step size of moving window (default: 1).</p> </li> </ul> <p>Returns:</p> <ul> <li> view(            <code>np.ndarray</code> )        \u2013 <p>View of array according to <code>window_length</code> and <code>step_size</code>.</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_window--references","title":"References","text":"<ol> <li> <p>https://stackoverflow.com/a/6811241 \u21a9</p> </li> <li> <p>https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html \u21a9</p> </li> <li> <p>https://gist.github.com/codehacken/708f19ae746784cef6e68b037af65788 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_window(a, window_length: int, step_size: int = 1):\n\"\"\"\n    Create moving windows of given window length over input array (as view).\n    Parameters\n    ----------\n    a : np.ndarray\n        1D input array.\n    window_length : int\n        Length of moving window.\n    step_size : int\n        Step size of moving window (default: 1).\n    Returns\n    -------\n    view : np.ndarray\n        View of array according to `window_length` and `step_size`.\n    References\n    ----------\n    [^1]: [https://stackoverflow.com/a/6811241](https://stackoverflow.com/a/6811241)\n    [^2]: [https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html](https://rigtorp.se/2011/01/01/rolling-statistics-numpy.html)\n    [^3]: [https://gist.github.com/codehacken/708f19ae746784cef6e68b037af65788](https://gist.github.com/codehacken/708f19ae746784cef6e68b037af65788)\n    \"\"\"\nshape = a.shape[:-1] + (a.shape[-1] - window_length + 1 - step_size + 1, window_length)\nstrides = a.strides + (a.strides[-1] * step_size,)\nreturn np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_average","title":"<code>moving_average(a, win=3)</code>","text":"<p>Apply simple non-weighted moving average.</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>1D input data.</p> </li> <li> win             (<code>int, optional</code>)         \u2013 <p>Number of data points within moving window (default: <code>3</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Moving average of input data.</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_average--reference","title":"Reference","text":"<ol> <li> <p>https://stackoverflow.com/a/42867926 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_average(a, win: int = 3):\n\"\"\"\n    Apply simple non-weighted moving average.\n    Parameters\n    ----------\n    a : np.ndarray\n        1D input data.\n    win : int, optional\n        Number of data points within moving window (default: `3`).\n    Returns\n    -------\n    np.ndarray\n        Moving average of input data.\n    Reference\n    ---------\n    [^1]: [https://stackoverflow.com/a/42867926](https://stackoverflow.com/a/42867926)\n    \"\"\"\nret = np.cumsum(a, dtype=float)\nret[win:] = ret[win:] - ret[:-win]\nreturn ret[win - 1 :] / win\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_average_convolve","title":"<code>moving_average_convolve(x, win=3)</code>","text":"<p>Compute moving average of array with given window length.</p> <p>Parameters:</p> <ul> <li> x             (<code>np.array</code>)         \u2013 <p>1D input data.</p> </li> <li> win             (<code>int, optional</code>)         \u2013 <p>Number of data points within moving window (default: <code>3</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.array</code>         \u2013 <p>Moving average of input data.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_average_convolve(x, win: int = 3):\n\"\"\"\n    Compute moving average of array with given window length.\n    Parameters\n    ----------\n    x : np.array\n        1D input data.\n    win : int, optional\n        Number of data points within moving window (default: `3`).\n    Returns\n    -------\n    np.array\n        Moving average of input data.\n    \"\"\"\nreturn np.convolve(x, np.ones(win), mode='valid') / win\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_median","title":"<code>moving_median(a, win=3, padded=False)</code>","text":"<p>Apply moving median of given window size. Optional padding of input array using half the window size to avoid edge effects.</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>Input data (1D).</p> </li> <li> win             (<code>int, optional</code>)         \u2013 <p>Number of data points within moving window (default: <code>3</code>).</p> </li> <li> padded             (<code>bool, optional</code>)         \u2013 <p>Pad start and end of array (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Moving median of input data.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_median(a, win: int = 3, padded=False):\n\"\"\"\n    Apply moving median of given window size.\n    Optional padding of input array using half the window size to avoid edge effects.\n    Parameters\n    ----------\n    a : np.ndarray\n        Input data (1D).\n    win : int, optional\n        Number of data points within moving window (default: `3`).\n    padded : bool, optional\n        Pad start and end of array (default: `False`).\n    Returns\n    -------\n    np.ndarray\n        Moving median of input data.\n    \"\"\"\nif padded:\nhalf_win = (win - 1) // 2\na = pad_array(a, half_win)\nwindows = moving_window(a, window_length=win)\nreturn np.median(windows, axis=-1)\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_window_2D","title":"<code>moving_window_2D(a, w, dx=1, dy=1, writeable=False)</code>","text":"<p>Create an array of moving windows (as view) into the input array using given step sizes in both dimensions.</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>2D input array.</p> </li> <li> w             (<code>tuple</code>)         \u2013 <p>Moving window shape.</p> </li> <li> dx             (<code>int, optional</code>)         \u2013 <p>Horizontal step size (columns, e.g. traces) (default: 1).</p> </li> <li> dy             (<code>int, optional</code>)         \u2013 <p>vertical step size (rows, e.g. time samples) (default: 1).</p> </li> <li> writeable             (<code>bool, optional</code>)         \u2013 <p>Set if view should be writeable (default: False). Use with care!</p> </li> </ul> <p>Returns:</p> <ul> <li> view(            <code>numpy.ndarray</code> )        \u2013 <p>4D array representing view of input array.</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_window_2D--references","title":"References","text":"<ol> <li> <p>https://colab.research.google.com/drive/1Zru_-zzbtylgitbwxbi0eDBNhwr8qYl6#scrollTo=tXDRG-5-2jBV \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_window_2D(a, w, dx=1, dy=1, writeable=False):\n\"\"\"\n    Create an array of moving windows (as view) into the input array using given step sizes in both dimensions.\n    Parameters\n    ----------\n    a : np.ndarray\n        2D input array.\n    w : tuple\n        Moving window shape.\n    dx : int, optional\n        Horizontal step size (columns, e.g. traces) (default: 1).\n    dy : int, optional\n        vertical step size (rows, e.g. time samples) (default: 1).\n    writeable : bool, optional\n        Set if view should be writeable (default: False). **Use with care!**\n    Returns\n    -------\n    view : numpy.ndarray\n        4D array representing view of input array.\n    References\n    ----------\n    [^1]: [https://colab.research.google.com/drive/1Zru_-zzbtylgitbwxbi0eDBNhwr8qYl6#scrollTo=tXDRG-5-2jBV](https://colab.research.google.com/drive/1Zru_-zzbtylgitbwxbi0eDBNhwr8qYl6#scrollTo=tXDRG-5-2jBV)\n    \"\"\"\nshape = (\na.shape[:-2] + ((a.shape[-2] - w[-2]) // dy + 1,) + ((a.shape[-1] - w[-1]) // dx + 1,) + w\n)\nstrides = a.strides[:-2] + (a.strides[-2] * dy,) + (a.strides[-1] * dx,) + a.strides[-2:]\nreturn np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides, writeable=writeable)\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.median_abs_deviation","title":"<code>median_abs_deviation(x, axis=-1)</code>","text":"<p>Return the median absolute deviation (MAD) from given input array.</p> <p>Parameters:</p> <ul> <li> x             (<code>np.ndarray</code>)         \u2013 <p>Input array.</p> </li> </ul> <p>Returns:</p> <ul> <li> mad(            <code>np.ndarray</code> )        \u2013 <p>Median absolute deviation (MAD) of input array.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def median_abs_deviation(x, axis=-1):\n\"\"\"\n    Return the median absolute deviation (MAD) from given input array.\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array.\n    Returns\n    -------\n    mad : np.ndarray\n        Median absolute deviation (MAD) of input array.\n    \"\"\"\nif x.ndim == 1:\nmad = np.median(np.abs(x - np.median(x, axis=axis)))\nelif x.ndim == 2:\nmad = np.median(np.abs(x.T - np.median(x, axis=axis)).T, axis=axis)\nelse:\nraise ValueError(f'Input arrays with &lt; {x.ndim} &gt; dimensions are not supported!')\nreturn mad\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.median_abs_deviation_double","title":"<code>median_abs_deviation_double(x, axis=-1)</code>","text":"<p>Return the median absolute deviation (MAD) for unsymmetric distributions. Computes the deviation from median for both sides (left &amp; right).</p> <p>Parameters:</p> <ul> <li> x             (<code>np.ndarray</code>)         \u2013 <p>Input array.</p> </li> <li> axis             (<code>TYPE, optional</code>)         \u2013 <p>Axis to compute median on (default: -1).</p> </li> </ul> <p>Returns:</p> <ul> <li> mad(            <code>np.ndarray</code> )        \u2013 <p>Median absolute deviation (MAD) of input array.</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.median_abs_deviation_double--references","title":"References","text":"<ol> <li> <p>https://eurekastatistics.com/using-the-median-absolute-deviation-to-find-outliers/ \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def median_abs_deviation_double(x, axis=-1):\n\"\"\"\n    Return the median absolute deviation (MAD) for unsymmetric distributions.\n    Computes the deviation from median for both sides (left &amp; right).\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array.\n    axis : TYPE, optional\n        Axis to compute median on (default: -1).\n    Returns\n    -------\n    mad : np.ndarray\n        Median absolute deviation (MAD) of input array.\n    References\n    ----------\n    [^1]: [https://eurekastatistics.com/using-the-median-absolute-deviation-to-find-outliers/](https://eurekastatistics.com/using-the-median-absolute-deviation-to-find-outliers/)\n    \"\"\"\nif x.ndim == 1:\nmed = np.median(x, axis=axis)\ndiff = np.abs(x - med)\nmad_left = np.median(diff[x &lt;= med])\nmad_right = np.median(diff[x &gt;= med])\nif mad_left == 0 or mad_right == 0:\nraise ValueError('one side of median absolute deviation is zero')\nmad = np.repeat(mad_left, len(x))\nmad[x &gt; med] = mad_right\nelif x.ndim == 2:\n# compute median for each window\nmed = np.median(x, axis=axis)\n# difference from median (per window)\ndiff = np.abs(x - med[:, None])\n# define column of reference value (in window)\nidx_col = x.shape[-1] // 2\n# left side MAD\nmad_left = np.median(diff[(x &lt;= med[:, None])[:, idx_col]], axis=axis)\nmad_left[mad_left == 0] = 1\n# right side MAD\nmad_right = np.median(diff[(x &gt;= med[:, None])[:, idx_col]], axis=axis)\nmad_right[mad_right == 0] = 1\n# create and fill output array\nmad = np.ones((x.shape[0],), dtype=x.dtype)\nmad[(x &lt;= med[:, None])[:, idx_col]] = mad_left\nmad[(x &gt;= med[:, None])[:, idx_col]] = mad_right\nelse:\nraise ValueError(f'Input arrays with &lt; {x.ndim} &gt; dimensions are not supported!')\nreturn mad.astype(x.dtype)\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.smooth","title":"<code>smooth(data, window_len=11, window='hanning')</code>","text":"<p>Smooth the data using a window with requested size.</p> <p>This method is based on the convolution of a scaled window with the signal. The signal is prepared by introducing reflected copies of the signal (with the window size) in both ends so that transient parts are minimized in the begining and end part of the output signal.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>1D input data array.</p> </li> <li> window_len             (<code>int, optional</code>)         \u2013 <p>Input window length, should be odd integer (default: 11).</p> </li> <li> window             (<code>str, optional</code>)         \u2013 <p>Tpye of smoothing window function (default: 'hanning').</p> </li> </ul> <p>Returns:</p> <ul> <li> out        \u2013 <p>smoothed input data</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.smooth--references","title":"References","text":"<ol> <li> <p>https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def smooth(data, window_len=11, window='hanning'):\n\"\"\"\n    Smooth the data using a window with requested size.\n    This method is based on the convolution of a scaled window with the signal.\n    The signal is prepared by introducing reflected copies of the signal\n    (with the window size) in both ends so that transient parts are minimized\n    in the begining and end part of the output signal.\n    Parameters\n    ----------\n    data : np.ndarray\n        1D input data array.\n    window_len : int, optional\n        Input window length, should be odd integer (default: 11).\n    window : str, optional\n        Tpye of smoothing window function (default: 'hanning').\n    Returns\n    -------\n    out :\n        smoothed input data\n    References\n    ----------\n    [^1]: [https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html](https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html)\n    \"\"\"\nif data.ndim != 1:\nraise ValueError('smooth only accepts 1 dimension arrays.')\nif data.size &lt; window_len:\nraise ValueError(\nf'Input data should be longer ({data.size}) than the window length ({window_len}).'\n)\nif window_len &lt; 3:\nreturn data\nif window not in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\nraise ValueError(\"Window is one of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\nif window_len % 2 == 0:  # even\nleft, right = window_len // 2, window_len // 2 + 1\nelse:  # odd\nleft, right = window_len // 2 + 1, window_len // 2 + 1\n# print(f'left: {left}, right {right} ')\ns = np.r_[data[left - 1 : 0 : -1], data, data[-2 : -right - 1 : -1]]\n# print('padded signal: ', len(s))\nif window == 'flat':\nw = np.ones(window_len, 'd')\nelse:\nw = eval('np.' + window + '(window_len)')\nout = np.convolve(s, w / w.sum(), mode='valid')\nreturn out\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.zscore_filter","title":"<code>zscore_filter(data, axis=-1)</code>","text":"<p>Z-score filter for outlier detection. Return array of outlier indices.</p> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def zscore_filter(data, axis=-1):\n\"\"\"Z-score filter for outlier detection. Return array of outlier indices.\"\"\"\nz_score = (data - np.mean(data, axis=axis)) / np.std(data, axis=axis)\nreturn np.nonzero(np.logical_or(z_score &lt; -1, z_score &gt; 1))[0]\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_zscore_filter","title":"<code>moving_zscore_filter(data, win, axis=-1)</code>","text":"<p>Return array of outlier indices using moving z-score filter for outlier detection of length <code>win</code>.</p> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_zscore_filter(data, win, axis=-1):  # noqa\n\"\"\"\n    Return array of outlier indices using moving z-score filter for outlier detection of length `win`.\n    \"\"\"\nmean = smooth(data, window_len=win, window='hanning')\nz_score = (data - mean) / np.std(data, axis=axis)\nreturn np.nonzero(np.logical_or(z_score &lt; -1, z_score &gt; 1))[0]\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.iqr_filter","title":"<code>iqr_filter(a, axis=-1)</code>","text":"<p>Inter-quartile range (IQR) filter for outlier detection. Return array of outlier indices.</p> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def iqr_filter(a, axis=-1):\n\"\"\"Inter-quartile range (IQR) filter for outlier detection. Return array of outlier indices.\"\"\"\nquantiles = np.quantile(a, [0.25, 0.75], axis=axis, keepdims=True)\nq1 = quantiles[0]\nq3 = quantiles[1]\niqr = q3 - q1\niqr_upper = q3 + 1.5 * iqr\niqr_lower = q1 - 1.5 * iqr\nreturn np.nonzero(np.logical_or(a &lt; iqr_lower, a &gt; iqr_upper))[0]\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.mad_filter","title":"<code>mad_filter(a, threshold=3, axis=-1, mad_mode='single')</code>","text":"<p>Median Absolute Deviation (MAD) filter. Return array of outlier indices.</p> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def mad_filter(a, threshold=3, axis=-1, mad_mode='single'):\n\"\"\"Median Absolute Deviation (MAD) filter. Return array of outlier indices.\"\"\"\nmed = np.median(a, axis=axis)\nif mad_mode == 'single':\nmad = median_abs_deviation(a)\nelif mad_mode == 'double':\nmad = median_abs_deviation_double(a)\nreturn np.nonzero((np.abs(a - med) / mad) &gt; threshold)[0]\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.moving_mad_filter","title":"<code>moving_mad_filter(a, win, threshold=3, axis=-1, mad_mode='single')</code>","text":"<p>Moving Median Absolute Deviation (MAD) filter of length <code>win</code>. Return array of outlier indices.</p> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def moving_mad_filter(a, win, threshold=3, axis=-1, mad_mode='single'):  # noqa\n\"\"\"Moving Median Absolute Deviation (MAD) filter of length `win`. Return array of outlier indices.\"\"\"\nif (type(win) != int) or (win % 2 != 1):\nraise ValueError('window length must be odd integer')\nwin_half = (win - 1) // 2\n# pad start and end of input array\na_pad = pad_array(a, win_half)\n# create moving windows (as views)\nwindows = moving_window(a_pad, window_length=win)\n# compute moving median\nmoving_med = np.median(windows, axis=-1)\n# compute moving MAD\nif mad_mode == 'single':\nmoving_mad = median_abs_deviation(windows)\nelif mad_mode == 'double':\nmoving_mad = median_abs_deviation_double(windows)\n# account for case MAD == 0 (prone to false outlier detection)\nmoving_mad[moving_mad == 0] = 1\nreturn np.nonzero((np.abs(a - moving_med) / moving_mad) &gt; threshold)[0]\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.polynominal_filter","title":"<code>polynominal_filter(data, order=3, kind='high')</code>","text":"<p>Apply polynominal filter to input data.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data.</p> </li> <li> order             (<code>int, optional</code>)         \u2013 <p>Filter order (default: <code>3</code>).</p> </li> <li> kind             (<code>str, optional</code>)         \u2013 <p>Filter kind (default: <code>high</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> data(            <code>np.ndarray</code> )        \u2013 <p>Filtered input data.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def polynominal_filter(data, order=3, kind='high'):\n\"\"\"\n    Apply polynominal filter to input data.\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data.\n    order : int, optional\n        Filter order (default: `3`).\n    kind : str, optional\n        Filter kind (default: `high`).\n    Returns\n    -------\n    data : np.ndarray\n        Filtered input data.\n    \"\"\"\ndata = data.copy().astype('float')\nx = np.arange(len(data))\nfit = np.polyval(np.polyfit(x, data, deg=order), x)\nif kind == 'high':\ndata -= fit\nelif kind == 'low':\ndata = data - (data - fit)\nelse:\nraise ValueError('filter kind `{kind}` is not available')\nreturn data\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.filter_interp_1d","title":"<code>filter_interp_1d(data, method='IQR', kind='cubic', win=11, threshold=3.0, filter_boundaries=True)</code>","text":"<p>Remove outliers using the IQR (inter-quartile range) method and interpolate using user-specified <code>kind</code> (default: 'cubic'). Return outlier-removed and interpolated input array.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data (1D).</p> </li> <li> method             (<code>str, optional</code>)         \u2013 <p>Filter method to use (default: <code>IQR</code>).</p> </li> <li> kind             (<code>str, optional</code>)         \u2013 <p>Interpolation method for scipy.interpolate.interp1d (default: <code>cubic</code>).</p> </li> <li> win             (<code>int, optional</code>)         \u2013 <p>Size of moving window if required by chosen method (default: <code>11</code>).</p> </li> <li> threshold             (<code>float, optional</code>)         \u2013 <p>Threshold used for median absolute deviation (MAD) (default: <code>3.0</code>).</p> </li> <li> filter_boundaries             (<code>bool, optional</code>)         \u2013 <p>Filter flagged outlier indices at start and end of input array to avoid edge effects (if present despite padding) (default: <code>True</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> data_interp(            <code>np.ndarray</code> )        \u2013 <p>Filtered and interpolated data.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def filter_interp_1d(\ndata, method='IQR', kind='cubic', win=11, threshold=3.0, filter_boundaries=True\n):  # noqa\n\"\"\"\n    Remove outliers using the IQR (inter-quartile range) method and\n    interpolate using user-specified `kind` (default: 'cubic').\n    Return outlier-removed and interpolated input array.\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data (1D).\n    method : str, optional\n        Filter method to use (default: `IQR`).\n    kind : str, optional\n        Interpolation method for scipy.interpolate.interp1d (default: `cubic`).\n    win : int, optional\n        Size of moving window if required by chosen method (default: `11`).\n    threshold : float, optional\n        Threshold used for median absolute deviation (MAD) (default: `3.0`).\n    filter_boundaries : bool, optional\n        Filter flagged outlier indices at start and end of input array to avoid\n        edge effects (if present despite padding) (default: `True`).\n    Returns\n    -------\n    data_interp : np.ndarray\n        Filtered and interpolated data.\n    \"\"\"\nMETHODS = ['IQR', 'z-score', 'r_z-score', 'MAD', 'doubleMAD', 'r_doubleMAD', 'r_singleMAD']\nKIND_LIST = [\n'linear',\n'nearest',\n'nearest-up',\n'zero',\n'slinear',\n'quadratic',\n'cubic',\n'previous',\n'next',\n]\nif data.ndim != 1:\nraise ValueError('data must be 1D array!')\nif kind not in KIND_LIST:\nraise ValueError(f'Parameter `kind` must be one of {KIND_LIST}')\n# get outlier indices\nif method == 'IQR':\nidx = iqr_filter(data)\nelif method == 'z-score':\nidx = zscore_filter(data)\nelif method == 'r_z-score':\nidx = moving_zscore_filter(data, win=win)\nelif method == 'MAD':\nidx = mad_filter(data, threshold=threshold, mad_mode='single')\nelif method == 'doubleMAD':\nidx = mad_filter(data, threshold=threshold, mad_mode='double')\nelif method == 'r_doubleMAD':\nidx = moving_mad_filter(data, win=win, threshold=threshold, mad_mode='double')\nelif method == 'r_singleMAD':\nidx = moving_mad_filter(data, win=win, threshold=threshold, mad_mode='single')\nelse:\nraise ValueError(f'Given method ist not valid. Choose from {METHODS}')\n# filter flagged outlier indices at start and end of input array\nif filter_boundaries:\n# find consecutive flagged values\n## get differences\ndiff_idx = np.diff(idx)\n## split into arrays holding consecutive flagged values\ndiff_idx_split = np.split(diff_idx, np.nonzero(diff_idx &gt; 1)[0])\n# check if first index is in input\nif np.isin(0, idx):\n# number of consecutive indices at start (add one due to split location)\nn_exclude_start = diff_idx_split[0].size + 1\n# exclude indices from flagged ones\nidx = idx[n_exclude_start:]\n# check last index is in input\nif np.isin(data.size - 1, idx):\n# number of consecutive indices at end\nn_exclude_end = diff_idx_split[-1].size\n# exclude indices from flagged ones\nidx = idx[:-n_exclude_end]\n# compute sampling indices\nx = np.arange(data.size)  # updated/altered input data\n# mask outliers for interpolation\nmask = np.ones(data.size, dtype='bool')\nmask[idx] = 0\n_data = data[mask]\n_x = x[mask]\n# create interpolation function\n_interp = interp.interp1d(_x, _data, kind=kind)\n# interpolate masked values\ndata_interp = _interp(x)\nreturn data_interp\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.sta_lta_filter","title":"<code>sta_lta_filter(a, nsta, nlta, axis=-1)</code>","text":"<p>Compute the STA/LTA ratio (short-time-average / longe-time-average) by continuously calculating the average values of the absolute amplitude of a seismic trace in two consecutive moving-time windows.</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>Seismic trace (1D) or section (2D).</p> </li> <li> nsta             (<code>int</code>)         \u2013 <p>Length of short time average window (samples).</p> </li> <li> nlta             (<code>int</code>)         \u2013 <p>Length of long time average window (samples).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>Axis for which to compute STA/LTA ratio (default: -1).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Either 1D or 2D array of STA/LTA ratio (per trace).</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.sta_lta_filter--references","title":"References","text":"<ol> <li> <p>Withers et al. (1998) A comparison of select trigger algorithms for automated global seismic phase and event detection,   http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.245&amp;rep=rep1&amp;type=pdf \u21a9</p> </li> <li> <p>Trnkoczy, A. (2012) Understanding and parameter setting of STA/LTA trigger algorithm,   https://gfzpublic.gfz-potsdam.de/rest/items/item_4097_3/component/file_4098/content \u21a9</p> </li> <li> <p>ObsPy, https://docs.obspy.org/_modules/obspy/signal/trigger.html#classic_sta_lta_py \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def sta_lta_filter(a, nsta: int, nlta: int, axis=-1):  # noqa\n\"\"\"\n    Compute the STA/LTA ratio (short-time-average / longe-time-average)\n    by continuously calculating the average values of the absolute amplitude\n    of a seismic trace in two consecutive moving-time windows.\n    Parameters\n    ----------\n    a : np.ndarray\n        Seismic trace (1D) or section (2D).\n    nsta : int\n        Length of short time average window (samples).\n    nlta : int\n        Length of long time average window (samples).\n    axis : int, optional\n        Axis for which to compute STA/LTA ratio (default: -1).\n    Returns\n    -------\n    np.ndarray\n        Either 1D or 2D array of STA/LTA ratio (per trace).\n    References\n    ----------\n    [^1]: Withers et al. (1998) A comparison of select trigger algorithms for automated global seismic phase and event detection,\n          [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.245&amp;rep=rep1&amp;type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.116.245&amp;rep=rep1&amp;type=pdf)\n    [^2]: Trnkoczy, A. (2012) Understanding and parameter setting of STA/LTA trigger algorithm,\n          [https://gfzpublic.gfz-potsdam.de/rest/items/item_4097_3/component/file_4098/content](https://gfzpublic.gfz-potsdam.de/rest/items/item_4097_3/component/file_4098/content)\n    [^3]: ObsPy, [https://docs.obspy.org/_modules/obspy/signal/trigger.html#classic_sta_lta_py](https://docs.obspy.org/_modules/obspy/signal/trigger.html#classic_sta_lta_py)\n    \"\"\"\nif any(s == 1 for s in a.shape):\na = np.squeeze(a, axis=-1).copy()\n# calculate moving average\nsta = np.cumsum(a**2, axis=axis).astype('float')\n# copy for LTA\nlta = sta.copy()\n# compute the STA and the LTA\nif a.ndim == 1:\nsta[nsta:] = sta[nsta:] - sta[:-nsta]\nsta /= nsta\nlta[nlta:] = lta[nlta:] - lta[:-nlta]\nlta /= nlta\n# pad zeros\nsta[: nlta - 1] = 0\nelif a.ndim == 2:\nsta[nsta:, :] = sta[nsta:, :] - sta[:-nsta, :]\nsta /= nsta\nlta[nlta:, :] = lta[nlta:, :] - lta[:-nlta, :]\nlta /= nlta\n# pad zeros\nsta[: nlta - 1, :] = 0\n# avoid division by zero!\nreturn np.divide(sta, lta, out=np.zeros_like(sta, dtype=sta.dtype), where=(lta != 0))\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.detect_seafloor_reflection","title":"<code>detect_seafloor_reflection(data, idx_slice_start=None, nsta=None, nlta=None, win=30, win_mad=None, win_mad_post=None, win_median=11, n=5, post_detection_filter=True)</code>","text":"<p>Detect seafloor reflection using the STA/LTA algorithm. Its commonly applied in seismology that evaluates the ratio of short- and long-term energy density. The initially sample indices found by the STA/LTA algorithm are used to create individual search windows per trace (idx - win &lt;= x &lt;= idx + win). Return indices of maximum amplitude(s) within individual search windows (shape: (ntraces,)).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input seismic section (samples x traces).</p> </li> <li> idx_slice_start             (<code>np.ndarray, optional</code>)         \u2013 <p>Index of first non-padded sample in original data.</p> </li> <li> nsta             (<code>TYPE, optional</code>)         \u2013 <p>Length of short time average window (in samples). If <code>None</code>: 0.1% of total samples.</p> </li> <li> nlta             (<code>TYPE, optional</code>)         \u2013 <p>Length of long time average window (in samples). If <code>None</code>: 5% of total samples.</p> </li> <li> win             (<code>int, optional</code>)         \u2013 <p>Number of samples to pad search window with (default: <code>20</code>). Set search window to <code>win</code> samples deeper and <code>win</code> x 2 samples shallower than baseline.</p> </li> <li> win_mad             (<code>int, optional</code>)         \u2013 <p>Number of traces used for Median Absolute Deviation (MAD) filtering. If None (default), this window is set to 5% of total traces.</p> </li> <li> win_mad_post             (<code>int, optional</code>)         \u2013 <p>Number of traces used for Median Absolute Deviation (MAD) filtering (after detection). If None (default), this window is set to 1% of total traces.</p> </li> <li> win_median             (<code>int, optional</code>)         \u2013 <p>Number of traces for rolling median filter, should be odd integer (default: <code>11</code>).</p> </li> <li> post_detection_filter             (<code>bool, optional</code>)         \u2013 <p>Apply optional Median Absolute Deviation (MAD) filtering after actual seafloor detection (default: <code>True</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Indices of samples at maximum amplitude (per trace).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def detect_seafloor_reflection(\ndata,\nidx_slice_start=None,\nnsta=None,\nnlta=None,\nwin=30,\nwin_mad=None,\nwin_mad_post=None,\nwin_median=11,\nn: int = 5,\npost_detection_filter: bool = True,\n):\n\"\"\"\n    Detect seafloor reflection using the STA/LTA algorithm.\n    Its commonly applied in seismology that evaluates the ratio of short- and long-term energy density.\n    The initially sample indices found by the STA/LTA algorithm are used to\n    create individual search windows per trace (idx - win &lt;= x &lt;= idx + win).\n    Return indices of maximum amplitude(s) within individual search windows (shape: (ntraces,)).\n    Parameters\n    ----------\n    data : np.ndarray\n        Input seismic section (samples x traces).\n    idx_slice_start : np.ndarray, optional\n        Index of first non-padded sample in original data.\n    nsta : TYPE, optional\n        Length of short time average window (in samples). If `None`: 0.1% of total samples.\n    nlta : TYPE, optional\n        Length of long time average window (in samples). If `None`: 5% of total samples.\n    win : int, optional\n        Number of samples to pad search window with (default: `20`).\n        Set search window to `win` samples deeper and `win` x 2 samples shallower than baseline.\n    win_mad : int, optional\n        Number of traces used for Median Absolute Deviation (MAD) filtering.\n        If None (default), this window is set to 5% of total traces.\n    win_mad_post : int, optional\n        Number of traces used for Median Absolute Deviation (MAD) filtering (after detection).\n        If None (default), this window is set to 1% of total traces.\n    win_median : int, optional\n        Number of traces for rolling median filter, should be odd integer (default: `11`).\n    post_detection_filter : bool, optional\n        Apply optional Median Absolute Deviation (MAD) filtering\n        after actual seafloor detection (default: `True`).\n    Returns\n    -------\n    np.ndarray\n        Indices of samples at maximum amplitude (per trace).\n    \"\"\"\nnsamples, ntraces = data.shape\n# check for zero traces (e.g. from merging)\ncnt_zero_traces = np.count_nonzero(data, axis=0)\nn_zero_traces = ntraces - np.count_nonzero(cnt_zero_traces, axis=0)\n# mask zero traces if found\nif n_zero_traces &gt; 0:\nmask_nonzero_traces = cnt_zero_traces.astype('bool')\ndata = data[:, mask_nonzero_traces]\nif nsta is None:\nnsta = int(np.around(nsamples * 0.001))\nif nlta is None:\nnlta = int(np.around(nsamples * 0.05))\nif nsta &lt; 3:\nnsta = 3\nnlta = 50\nprint(f'[WARNING]    Changed nsta={nsta} and nlta={nlta}!')\n# (1) calc standard STA/LTA from data array\nsta_lta = sta_lta_filter(data, nsta, nlta, axis=0)\n# (2) detect first significant amplitude peak (sample indices)\n# CAUTION: could be outlier (e.g. noise bursts in water column)\n#          but that misdetection will be filtered in the subsequent step!\nidx_sta_lta = np.argmax(sta_lta, axis=0)\nif idx_slice_start is not None:\nidx_sta_lta += idx_slice_start\nif idx_slice_start is not None:\n# (3) replace seafloor detections outside sample range with median value\nidx_sta_lta = np.where(\nnp.logical_or(idx_sta_lta &gt; nsamples - idx_slice_start, idx_sta_lta &lt; idx_slice_start),\nnp.median(idx_sta_lta),\nidx_sta_lta,\n)\n# # (3) outlier detection &amp; removal #TODO: unnecessary?\nif win_mad is None:\nwin_mad = int(idx_sta_lta.size * 0.02)\nwin_mad = win_mad + 1 if win_mad % 2 == 0 else win_mad  # must be odd\nwin_mad = 7 if win_mad &lt; 7 else win_mad                 # at least 7 traces\nidx_sta_lta = filter_interp_1d(\nidx_sta_lta, method='r_doubleMAD', kind='cubic', threshold=3, win=win_mad\n).astype('int')\n# (4) apply moving median filter to remove large outliers\nidx_sta_lta = moving_median(idx_sta_lta, win_median, padded=True).astype('int')\n# (5) detect `actual` first break amplitude\n# init index array\nidx_arr = np.arange(nsamples)[:, None]\n# create mask from slices (upper index &lt;= slice &lt;= lower index)\nidx_upper, idx_lower = (idx_sta_lta - win), (idx_sta_lta + win)  # *2\nmask = (idx_arr &gt;= idx_upper) &amp; (idx_arr &lt;= idx_lower)\n# get indices from mask\nindices = np.apply_along_axis(np.nonzero, 0, mask).squeeze()\n# subset input array using indices of search window\nsta_lta_win = np.take_along_axis(data, indices, axis=0)\n# get `n` largest values for each trace subset\n# n = 5\nidx_nlargest = np.argpartition(-sta_lta_win, n, axis=0)[:n]\n# sort the indices for each trace (ascending order)\nidx_nlargest = np.take_along_axis(\nidx_nlargest, axis=0, indices=np.argsort(idx_nlargest, axis=0)\n)\n# get indices to split `idx_nlargest` into groups of different peak amplitudes\nidx_nlargest_sel = [\nnp.nonzero(tr &gt; 1)[0][0] if np.nonzero(tr &gt; 1)[0].size &gt; 0 else n\nfor tr in np.diff(idx_nlargest, 1, axis=0).T\n]\n# split the index array of `n` largest values and select first significant (positive) amplitude\nidx_nlargest_sel = [\nnp.split(tr, [i])[0] if i != 0 else np.array([tr[i]])\nfor tr, i in zip(idx_nlargest.T, idx_nlargest_sel)\n]\n# get index of max. amplitude within selected maxima of first significant (positive) amplitude (NOTE: subset index!)\nidx_peak_amp = np.asarray(\n[\nnlarge[np.argmax(tr[i])]\nfor nlarge, tr, i in zip(idx_nlargest.T, sta_lta_win.T, idx_nlargest_sel)\n]\n)\n# convert subset indices to indices of seismic section\nidx_peak_amp += idx_upper\nif n_zero_traces &gt; 0:\nx = np.arange(0, ntraces)  # create trace idx WITH zero traces\nx_masked = x[mask_nonzero_traces]  # masked zero traces\n# create interpolation function\n_interp = interp.interp1d(x_masked, idx_peak_amp, kind='linear')\n# interpolate masked indices of zero traces\nidx_peak_amp = _interp(x).astype('int')\n# (6) additional outlier detection &amp; removal\nif post_detection_filter:\nif win_mad_post is None:\nwin_mad_post = int(idx_sta_lta.size * 0.01)\nwin_mad_post = (\nwin_mad_post + 1 if win_mad_post % 2 == 0 else win_mad_post\n)  # must be odd\nwin_mad_post = 7 if win_mad_post &lt; 7 else win_mad_post  # at least 7 traces\nidx_peak_amp = filter_interp_1d(\nidx_peak_amp, method='r_doubleMAD', kind='cubic', threshold=3, win=win_mad_post\n).astype('int')\nreturn idx_peak_amp.astype('int')\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.butterworth_filter","title":"<code>butterworth_filter(data, btype, cutoff, fs, order=9, axis=-1)</code>","text":"<p>Apply butterworth filter to input signal. Can be <code>lowpass</code>, <code>highpass</code>, or <code>bandpass</code>.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data.</p> </li> <li> cutoff             (<code>float | tuple</code>)         \u2013 <p>Cutoff frequency (in Hz).</p> </li> <li> fs             (<code>float</code>)         \u2013 <p>Sampling frequency (in Hz).</p> </li> <li> order             (<code>int, optional</code>)         \u2013 <p>Butterworth filter order (default: 9).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>The axis of x to which the filter is applied (default: -1).</p> </li> </ul> <p>Returns:</p> <ul> <li> y(            <code>np.ndarray</code> )        \u2013 <p>Filtered input signal.</p> </li> </ul>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.butterworth_filter--references","title":"References","text":"<ol> <li> <p>https://stackoverflow.com/a/48677312 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def butterworth_filter(\ndata: np.ndarray, btype: str, cutoff: float, fs: float, order: int = 9, axis: int = -1\n) -&gt; np.ndarray:\n\"\"\"\n    Apply butterworth filter to input signal. Can be `lowpass`, `highpass`, or `bandpass`.\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data.\n    cutoff : float | tuple\n        Cutoff frequency (in Hz).\n    fs : float\n        Sampling frequency (in Hz).\n    order : int, optional\n        Butterworth filter order (default: 9).\n    axis : int, optional\n        The axis of x to which the filter is applied (default: -1).\n    Returns\n    -------\n    y : np.ndarray\n        Filtered input signal.\n    References\n    ----------\n    [^1]: [https://stackoverflow.com/a/48677312](https://stackoverflow.com/a/48677312)\n    \"\"\"\nif btype not in ['lowpass', 'highpass', 'bandpass']:\nraise ValueError('``btype`` has to be ``lowpass``, ``highpass``, or ``bandpass``!')\nsos = _butterworth_filter_coefficients(btype, order, cutoff, fs)\ny = signal.sosfiltfilt(sos, data, axis=axis)\nreturn y\n</code></pre>"},{"location":"api/functions/api_filter/#pseudo_3D_interpolation.functions.filter.filter_frequency","title":"<code>filter_frequency(data, freqs, fs, filter_type, gpass=1, gstop=10, axis=-1)</code>","text":"<p>Apply freqeuncy filter by specifing passband and stopband frequencies. Possible filter types:</p> <ul> <li><code>bandpass</code>:     freqs = [f1, f2, f3, f4]</li> <li><code>lowpass</code>:      freqs = [f_stopband, f_cutoff]</li> <li><code>highpass</code>:     freqs = [f_cutoff, f_stopband]</li> </ul> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input data.</p> </li> <li> freqs             (<code>list</code>)         \u2013 <p>List of frequencies defining filter (same unit as <code>fs</code>!).</p> </li> <li> fs             (<code>float</code>)         \u2013 <p>Sampling frequency (in Hz).</p> </li> <li> filter_type             (<code>str</code>)         \u2013 <p>Filter type to apply (<code>bandpass</code>, <code>lowpass</code>, <code>highpass</code>)</p> </li> <li> gpass             (<code>int, optional</code>)         \u2013 <p>The maximum loss in the passband (dB).</p> </li> <li> gstop             (<code>int, optional</code>)         \u2013 <p>The minimum attenuation in the stopband (dB).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>The axis of x to which the filter is applied (default: <code>-1</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Filtered input data.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\filter.py</code> <pre><code>def filter_frequency(\ndata: np.ndarray,\nfreqs: list,\nfs: float,\nfilter_type: str,\ngpass: int = 1,\ngstop: int = 10,\naxis: int = -1,\n) -&gt; np.ndarray:\n\"\"\"\n    Apply freqeuncy filter by specifing passband and stopband frequencies.\n    Possible filter types:\n      - `bandpass`:     freqs = [*f1*, *f2*, *f3*, *f4*]\n      - `lowpass`:      freqs = [*f_stopband*, *f_cutoff*]\n      - `highpass`:     freqs = [*f_cutoff*, *f_stopband*]\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data.\n    freqs : list\n        List of frequencies defining filter (same unit as `fs`!).\n    fs : float\n        Sampling frequency (in Hz).\n    filter_type : str\n        Filter type to apply (`bandpass`, `lowpass`, `highpass`)\n    gpass : int, optional\n        The maximum loss in the passband (dB).\n    gstop : int, optional\n        The minimum attenuation in the stopband (dB).\n    axis : int, optional\n        The axis of x to which the filter is applied (default: `-1`).\n    Returns\n    -------\n    np.ndarray\n        Filtered input data.\n    \"\"\"\nif filter_type == 'bandpass':\nwp = [freqs[0], freqs[-1]]\nws = [freqs[1], freqs[2]]\nif not freqs == sorted(freqs):\nraise ValueError('Invalid filter frequencies!')\nelif filter_type == 'lowpass':\nwp, ws = freqs\nif wp &gt; ws:\nraise ValueError('Invalid filter frequencies!')\nelif filter_type == 'highpass':\nwp, ws = freqs\nif wp &lt; ws:\nraise ValueError('Invalid filter frequencies!')\n# get Butterworth filter order and natural frequency\nN, Wn = signal.buttord(wp, ws, gpass, gstop, analog=False, fs=fs)\n# filter coefficients\nsos = signal.butter(N, Wn, analog=False, btype=filter_type, output='sos', fs=fs)\nreturn signal.sosfiltfilt(sos, data, axis=axis)\n</code></pre>"},{"location":"api/functions/api_header/","title":"<code>header.py</code>","text":"<p>Utility functions for SEG-Y header manipulations.</p>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.scale_coordinates","title":"<code>scale_coordinates(segyfile, src_coords_bytes=(73, 77))</code>","text":"<p>Scale coordinates with <code>SourceGroupScalar</code> from seismic header. Returns arrays of scaled X and Y coordinates.</p> <p>Parameters:</p> <ul> <li> segyfile             (<code>segyio.SegyFile</code>)         \u2013 <p>Input SEG-Y file object.</p> </li> <li> src_coords_bytes             (<code>tuple, optional</code>)         \u2013 <p>Byte position of coordinates in trace header (default: (73, 77)).</p> </li> </ul> <p>Returns:</p> <ul> <li> x(            <code>np.ndarray</code> )        \u2013 <p>Array of X coordinates.</p> </li> <li> y(            <code>np.ndarray</code> )        \u2013 <p>Array of Y coordinates.</p> </li> <li> CoordinateUnits(            <code>int</code> )        \u2013 <p>Identifier for coordinate units.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def scale_coordinates(segyfile, src_coords_bytes: tuple = (73, 77)):\n\"\"\"\n    Scale coordinates with `SourceGroupScalar` from seismic header.\n    Returns arrays of scaled X and Y coordinates.\n    Parameters\n    ----------\n    segyfile : segyio.SegyFile\n        Input SEG-Y file object.\n    src_coords_bytes : tuple, optional\n        Byte position of coordinates in trace header (default: (73, 77)).\n    Returns\n    -------\n    x : np.ndarray\n        Array of X coordinates.\n    y : np.ndarray\n        Array of Y coordinates.\n    CoordinateUnits : int\n        Identifier for coordinate units.\n    \"\"\"\nxcoords_byte, ycoords_byte = src_coords_bytes  # unpack source coordinates byte IDs\nCoordinateUnits = segyfile.header[0].get(segyio.TraceField.CoordinateUnits)  # 89\n# print('CoordinateUnits:   ', CoordinateUnits)\nx, y = [], []\nfor header in segyfile.header:\nx.append(header.get(xcoords_byte))  # get X coordinate\ny.append(header.get(ycoords_byte))  # get Y coordinate\nx, y = np.array(x), np.array(y)  # convert lists to arrays\nif CoordinateUnits == 1:  # length (meter or feet)\nSourceGroupScalar = segyfile.header[0].get(segyio.TraceField.SourceGroupScalar)  # 71\n# print('SourceGroupScalar: ', SourceGroupScalar)\nif SourceGroupScalar &lt; 0:\nx = x / np.abs(SourceGroupScalar)\ny = y / np.abs(SourceGroupScalar)\nelif SourceGroupScalar &gt; 0:\nx = x * np.abs(SourceGroupScalar)\ny = y * np.abs(SourceGroupScalar)\nelif SourceGroupScalar == 0:\npass  # no scaling applied\nelif CoordinateUnits == 2:  # seconds of arc\nx = x / 3600000\ny = y / 3600000\nelif CoordinateUnits == 3:  # decimal degrees\nraise NotImplementedError('Functionality to convert DD data is not implemented.')\nelif CoordinateUnits == 4:  # degrees, minutes, seconds (DMS)\nraise NotImplementedError('Functionality to convert DMS data is not implemented.')\nreturn x, y, CoordinateUnits\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.unscale_coordinates","title":"<code>unscale_coordinates(X, Y, coords_bytes=(73, 77), coords_units=1, scale_factor=-100)</code>","text":"<p>Convert X/Y coordinates to int32 using scale factor for SEG-Y writing. Returns arrays of 32-bit integer X and Y coordinates.</p> <p>Parameters:</p> <ul> <li> X             (<code>np.ndarray</code>)         \u2013 <p>Input X coordinates.</p> </li> <li> Y             (<code>np.ndarray</code>)         \u2013 <p>Input Y coordinates.</p> </li> <li> coords_bytes             (<code>tuple, optional</code>)         \u2013 <p>Byte position of coordinates (default: <code>(73, 77)</code>).</p> </li> <li> coords_units             (<code>int, optional</code>)         \u2013 <p>SEG-Y specific coordinate unit identifier (default: <code>1</code>).</p> </li> <li> scale_factor             (<code>int, optional</code>)         \u2013 <p>Coordinate scaler for conversion from int to actual format (default: <code>-100</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray(s)</code>         \u2013 <p>Unscaled X and Y coordinate arrays.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def unscale_coordinates(\nX, Y,\ncoords_bytes: tuple = (73, 77),\ncoords_units: int = 1,\nscale_factor: int = -100\n):\n\"\"\"\n    Convert X/Y coordinates to int32 using scale factor for SEG-Y writing.\n    Returns arrays of 32-bit integer X and Y coordinates.\n    Parameters\n    ----------\n    X : np.ndarray\n        Input X coordinates.\n    Y : np.ndarray\n        Input Y coordinates.\n    coords_bytes : tuple, optional\n        Byte position of coordinates (default: `(73, 77)`).\n    coords_units : int, optional\n        SEG-Y specific coordinate unit identifier (default: `1`).\n    scale_factor : int, optional\n        Coordinate scaler for conversion from int to actual format (default: `-100`).\n    Returns\n    -------\n    np.ndarray(s)\n        Unscaled X and Y coordinate arrays.\n    \"\"\"\nxcoords_byte, ycoords_byte = coords_bytes  # unpack source coordinates byte IDs\nx, y = np.asarray(X), np.asarray(Y)  # convert lists to arrays\nif coords_units == 1:  # length (meter or feet)\n# print('SourceGroupScalar: ', SourceGroupScalar)\nif scale_factor &lt; 0:\nx = x * np.abs(scale_factor)\ny = y * np.abs(scale_factor)\nelif scale_factor &gt; 0:\nx = x / np.abs(scale_factor)\ny = y / np.abs(scale_factor)\nelif scale_factor == 0:\npass  # no scaling applied\nelif coords_units == 2:  # seconds of arc\nx = x * 3600000\ny = y * 3600000\nelif coords_units == 3:  # decimal degrees\nraise NotImplementedError('Functionality to convert DD data is not implemented.')\nelif coords_units == 4:  # degrees, minutes, seconds (DMS)\nraise NotImplementedError('Functionality to convert DMS data is not implemented.')\nreturn np.around(x, 0).astype('int'), np.around(y, 0).astype('int')\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.set_coordinates","title":"<code>set_coordinates(segyfile, X, Y, crs_dst, dst_coords_bytes=(73, 77), coordinate_units=1, scaler=-100)</code>","text":"<p>Set X and Y coordinates using given input arrays.</p> <p>Parameters:</p> <ul> <li> segyfile             (<code>segyio.SegyFile</code>)         \u2013 <p>Loaded SEG-Y file object.</p> </li> <li> X             (<code>numpy.array</code>)         \u2013 <p>Transformed X coordinates.</p> </li> <li> Y             (<code>numpy.array</code>)         \u2013 <p>Transformed Y coordinates.</p> </li> <li> crs_dst             (<code>pyproj.crs.CRS</code>)         \u2013 <p>Output CRS (of given coordinate arrays).</p> </li> <li> dst_coords_bytes             (<code>tuple, optional</code>)         \u2013 <p>Tuple of starting byte of X and Y coordinates in seismic trace header (default: <code>(73,77)</code>).</p> </li> <li> coordinate_units         \u2013 <p>Integer code for coordinate units (default: <code>1</code> i.e. meter).</p> </li> <li> scaler             (<code>int, optional</code>)         \u2013 <p>Coordinate scaler for conversion from int to actual format (default: <code>-100</code>).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def set_coordinates(\nsegyfile, X, Y, crs_dst, dst_coords_bytes=(73, 77), coordinate_units=1, scaler=-100\n):\n\"\"\"\n    Set X and Y coordinates using given input arrays.\n    Parameters\n    ----------\n    segyfile : segyio.SegyFile\n        Loaded SEG-Y file object.\n    X : numpy.array\n        Transformed X coordinates.\n    Y : numpy.array\n        Transformed Y coordinates.\n    crs_dst : pyproj.crs.CRS\n        Output CRS (of given coordinate arrays).\n    dst_coords_bytes : tuple, optional\n        Tuple of starting byte of X and Y coordinates in seismic trace header (default: `(73,77)`).\n    coordinate_units: int,\n        Integer code for coordinate units (default: `1` i.e. meter).\n    scaler : int, optional\n        Coordinate scaler for conversion from int to actual format (default: `-100`).\n    \"\"\"\nif crs_dst.is_geographic:\nraise NotImplementedError(\n'Functionality to convert to geographic output CRS is not yet implemented.'\n)\nelif crs_dst.is_projected:\n# unpack source coordinates byte IDs\nxcoords_byte, ycoords_byte = dst_coords_bytes\n# unscale coordinates\nX_unscale, Y_unscale = unscale_coordinates(\nX, Y, dst_coords_bytes, coordinate_units, scaler\n)\n# set (transformed) coordinates\nfor i, h in enumerate(segyfile.header[:]):\nh.update(\n{\nxcoords_byte: X_unscale[i],  # 73\nycoords_byte: Y_unscale[i],  # 77\nsegyio.TraceField.CoordinateUnits: coordinate_units,  # 89\nsegyio.TraceField.SourceGroupScalar: scaler,\n}\n)  # 71\nelse:\nraise AttributeError('Issues with output CRS. Please check!')\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.check_coordinate_scalar","title":"<code>check_coordinate_scalar(coord_scalar, xcoords=None, ycoords=None)</code>","text":"<p>Check input coordinate scalar. Return coordinate scalar (<code>coord_scalar</code>) and its mulitplier (<code>coord_scalar_mult</code>).</p> <p>Parameters:</p> <ul> <li> coord_scalar             (<code>int</code>)         \u2013 <p>Input coordinate scalar (-1000, -100, -10, 0, 10, 100, 1000, or 'auto').</p> </li> </ul> <p>Returns:</p> <ul> <li> coord_scalar(            <code>int</code> )        \u2013 <p>Coordinate scalar to apply.</p> </li> <li> coord_scalar_mult(            <code>float</code> )        \u2013 <p>Factor to scale coordinates.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def check_coordinate_scalar(\ncoord_scalar: int,\nxcoords: np.ndarray = None,\nycoords: np.ndarray = None,\n):\n\"\"\"\n    Check input coordinate scalar.\n    Return coordinate scalar (`coord_scalar`) and its mulitplier (`coord_scalar_mult`).\n    Parameters\n    ----------\n    coord_scalar : int\n        Input coordinate scalar (-1000, -100, -10, 0, 10, 100, 1000, or 'auto').\n    Returns\n    -------\n    coord_scalar : int\n        Coordinate scalar to apply.\n    coord_scalar_mult : float\n        Factor to scale coordinates.\n    \"\"\"\n# coord_scalar = args.scalar_coords\nif coord_scalar is None:\ncoord_scalar = 0\nif coord_scalar == 'auto':\nmax_digits = 10  # for 4 byte field --&gt; 2,147,483,647 (int32 )\nn_digits_x = str(xcoords.flat[0]).find('.')\nn_digits_y = str(ycoords.flat[0]).find('.')\nn_digits = max(n_digits_x, n_digits_y)\ncoord_scalar_mult = 10 ** (max_digits - n_digits)\ncoord_scalar = -coord_scalar_mult if coord_scalar_mult &gt; 1 else int(1 / coord_scalar_mult)\nelif coord_scalar &gt; 0:\ncoord_scalar_mult = 1 / abs(coord_scalar)\nelif coord_scalar &lt; 0:\ncoord_scalar_mult = abs(coord_scalar)\nelse:\ncoord_scalar_mult = 1\nreturn coord_scalar, coord_scalar_mult\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.wrap_text","title":"<code>wrap_text(txt, width=80)</code>","text":"<p>Format textual header for pretty printing.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def wrap_text(txt, width=80):\n\"\"\"Format textual header for pretty printing.\"\"\"\nreturn '\\n'.join([txt[i : i + width] for i in range(0, len(txt), width)])\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.whitespac_indices","title":"<code>whitespac_indices(s)</code>","text":"<p>Return list of whitespace indices in given string.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def whitespac_indices(s):\n\"\"\"Return list of whitespace indices in given string.\"\"\"\nreturn [i for i, c in enumerate(s) if c == ' ']\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.find_empty_line","title":"<code>find_empty_line(txt, splitter='\\n')</code>","text":"<p>Return individual lines of textual header and index of first empty line.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def find_empty_line(txt, splitter='\\n'):\n\"\"\"Return individual lines of textual header and index of first empty line.\"\"\"\nif not isinstance(txt, list):\nlines = txt.split(splitter)\nelse:\nlines = txt\ncnts = [r[3:].count(' ') for r in lines]\ntry:\nline_idx = cnts.index(77)\nexcept ValueError:\nline_idx = None\nreturn lines, line_idx\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.find_line_by_str","title":"<code>find_line_by_str(lines, search_str='PROCESSING')</code>","text":"<p>Return indices for lines starting with search string.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def find_line_by_str(lines, search_str='PROCESSING'):\n\"\"\"Return indices for lines starting with search string.\"\"\"\nif not isinstance(lines, list):\nlines = lines.split('\\n')\nif search_str is None:\nreturn []\nreturn [i for i in range(len(lines)) if lines[i][4:].startswith(search_str)]\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.add_processing_info_header","title":"<code>add_processing_info_header(txt, info_str, prefix=None, header=True, header_line=25, overwrite=True, newline=False)</code>","text":"<p>Add processing information annotation to textual header string. The info will be added to a line starting with <code>prefix</code> if provided and line is not filled.</p> <p>Parameters:</p> <ul> <li> txt             (<code>str</code>)         \u2013 <p>Textual header string read from SEG-Y file.</p> </li> <li> info_str             (<code>str</code>)         \u2013 <p>String to add to header.</p> </li> <li> prefix             (<code>str, optional</code>)         \u2013 <p>Line prefix where to insert <code>info</code> (default: <code>None</code>).</p> </li> <li> header             (<code>bool, str, optional</code>)         \u2013 <p>If <code>True</code>: update header string with default header line If <code>str</code>:  use provided string for header line</p> </li> <li> header_line         \u2013 <p>Line in textual header where to insert header line (default: <code>25</code>).</p> </li> <li> overwrite         \u2013 <p>Overwrite existing text in specified header line and following lines (default: <code>True</code>).</p> </li> <li> newline         \u2013 <p>Force text to be added to newline instead of appending to existing one (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013 <p>Updated textual header string.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def add_processing_info_header(\ntxt, info_str, prefix=None, header=True, header_line=25, overwrite=True, newline=False\n):\n\"\"\"\n    Add processing information annotation to textual header string.\n    The info will be added to a line starting with `prefix` if provided\n    and line is not filled.\n    Parameters\n    ----------\n    txt : str\n        Textual header string read from SEG-Y file.\n    info_str : str\n        String to add to header.\n    prefix : str, optional\n        Line prefix where to insert `info` (default: `None`).\n    header : bool, str, optional\n        If `True`: update header string with default header line\n        If `str`:  use provided string for header line\n    header_line: int, optional\n        Line in textual header where to insert header line (default: `25`).\n    overwrite: bool, optional\n        Overwrite existing text in specified header line and following lines (default: `True`).\n    newline: bool, optional\n        Force text to be added to newline instead of appending to existing one (default: `False`).\n    Returns\n    -------\n    str\n        Updated textual header string.\n    \"\"\"\nTOTAL_LENGTH = 3200\nLINE_LENGTH = 80\nLINE_PREFIX_LEN = 3\nidx_header = None\n# set prefix to today's date if keyoword provided\nif isinstance(prefix, str) and prefix.upper() in ['_TODAY_', '_DATE_']:\nprefix = datetime.date.today().strftime('%Y-%m-%d')\n# set header line to DEFAULT_HEADER\nif header is True:\ntxt, line_header = set_header_line(txt, header=header, line=header_line, overwrite=True)\nidx_header = line_header - 1\n# set custom header line\nelif isinstance(header, str):\n# check if header alrady exists\nidx_header = find_header_line(txt, header)\nif idx_header is None:\n# create new header line\ntxt, line_header = set_header_line(\ntxt, header=header, line=header_line, overwrite=overwrite\n)\nidx_header = line_header - 1\nelse:\nraise ValueError(f'Parameter &lt; {header} &gt; is not permitted as input for `header`')\n# split textual header into list of lines &amp; get indices of empty lines\nlines, idx_line = find_empty_line(txt)\nif idx_line is None:\nraise IndexError(\n'SEG-Y textual header is already full. Adding more information is not possible.'\n)\n# check for already existing lines with prefix\nidx_prefix = find_line_by_str(lines, search_str=prefix)\n# filter lines starting with prefix to only occur AFTER header line (if header set)\nif idx_header is not None:\nidx_prefix = [i for i in idx_prefix if i &gt; idx_header]\n_inserted = False\nif prefix is not None and (newline is False):\n# get number of characters in string to paste\nlen_info = len(info_str)\nfor idx in idx_prefix:\n# print('-------------')\n# print(idx, _inserted)\nline = lines[idx]\nidx_last_char = len(line.rstrip())\n# enough space to add info to already existing line?\nif len_info &lt; (LINE_LENGTH - idx_last_char):\n# print('Enough space? ', len_info &lt; (LINE_LENGTH - idx_last_char), LINE_LENGTH - idx_last_char)\nlines[idx] = (\nline[: idx_last_char + 2] + f'{info_str}' + line[idx_last_char + len_info + 2 :]\n)\n_inserted = True\nbreak\n# if (a) no line with prefix, (b) info not inserted, or (c) no prefix provided\nif any([(len(idx_prefix) == 0), (not _inserted), (prefix is None)]):\n# print('[INFO] No prefix provided or found or already full')\nto_add = f' {prefix}: {info_str}' if prefix is not None else f'{info_str}'\n# if header line exists BUT index of empty line is before header -&gt; find empty line after header\nif idx_header is not None and (idx_line &lt; idx_header):\n_, idx_line = find_empty_line(lines[idx_header:])\nidx_line += idx_header\n# construct and set line with info\nlines[idx_line] = (\nlines[idx_line][:LINE_PREFIX_LEN]\n+ to_add\n+ ' ' * (LINE_LENGTH - LINE_PREFIX_LEN - len(to_add))\n)  # lines[idx_line][LINE_PREFIX_LEN + len(to_add):]\ntest = len(''.join(lines))\nassert (\ntest == TOTAL_LENGTH\n), f'Length of updated textual header ({test}) is not correct ({TOTAL_LENGTH} characters)'\nreturn '\\n'.join(lines)\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.find_header_line","title":"<code>find_header_line(lines, header)</code>","text":"<p>Return index of header in list of lines.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def find_header_line(lines, header):\n\"\"\"Return index of header in list of lines.\"\"\"\nif isinstance(lines, str):\nlines = lines.split('\\n')\ncheck = [True if line.find(header) &gt; -1 else False for line in lines]\nif any(check):\nreturn check.index(True)\nelse:\nreturn None\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.set_header_line","title":"<code>set_header_line(lines, header, line=25, overwrite=True)</code>","text":"<p>Set custom header line to textual header line 'C25'.</p> <p>Parameters:</p> <ul> <li> lines             (<code>str</code>)         \u2013 <p>String of textual header lines.</p> </li> <li> header             (<code>str or bool</code>)         \u2013 <p>Header string to set to line.</p> </li> <li> line             (<code>int, optional</code>)         \u2013 <p>Header line selcetion (default: <code>25</code>).</p> </li> <li> overwrite             (<code>bool, optional</code>)         \u2013 <p>Overwrite existing text in specified header line and following lines (default: <code>True</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013 <p>Updated textual header.</p> </li> <li> <code>int</code>         \u2013 <p>Line number of header.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def set_header_line(lines, header, line=25, overwrite=True):\n\"\"\"\n    Set custom header line to textual header line 'C25'.\n    Parameters\n    ----------\n    lines : str\n        String of textual header lines.\n    header : str or bool\n        Header string to set to line.\n    line : int, optional\n        Header line selcetion (default: `25`).\n    overwrite : bool, optional\n        Overwrite existing text in specified header line and following lines (default: `True`).\n    Returns\n    -------\n    str\n        Updated textual header.\n    int\n        Line number of header.\n    \"\"\"\nDEFAULT_HEADER = '***** PROCESSING WORKFLOW *****'\nif header is True:\nheader = DEFAULT_HEADER\nif isinstance(lines, str):\nlines = lines.split('\\n')\nelif isinstance(lines, list):\npass\nelse:\nraise ValueError(f'Not supported textual header type: {type(lines)}')\n# check for already existing header in lines\nidx_header = find_header_line(lines, header)\nif idx_header is not None:\nwarn('Specified header line already exists.', UserWarning)\nreturn '\\n'.join(lines), idx_header + 1\nempty_header_line = lines[line - 1].count(' ') &gt;= 77\nif not empty_header_line and not overwrite:\nraise Exception(\nf'Selected header line ({line}) is already in use and overwrite is set False.'\n)\nelif not empty_header_line and overwrite:\nwarn('Selected header line is already in use and will be overwritten!', UserWarning)\nlen_header = len(header)\npad = 77 - len_header\npad_left = pad // 2\npad_right = pad_left if pad % 2 == 0 else pad_left + 1\n# set header to specified line\nlines[line - 1] = lines[line - 1][:3] + ' ' * pad_left + header + ' ' * pad_right\nfor i in range(39 - line):  # exclude last line\nlines[line + i] = lines[line + i][:3] + ' ' * 77\nreturn '\\n'.join(lines), line\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.get_textual_header","title":"<code>get_textual_header(path)</code>","text":"<p>Read SEG-Y file and return textual header as string.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def get_textual_header(path):\n\"\"\"Read SEG-Y file and return textual header as string.\"\"\"\nwith open(path, 'rb') as file:\nfile.seek(0, 0)  # find byte position zero relative to start of file (0)\ntext_byte = file.read(3200)\nif _isascii(text_byte):\ntext = text_byte.decode(\"ascii\")\ntext = wrap_text(text)\nelse:\ntext = text_byte.decode(\"cp500\")  # EBCDIC encoding\ntext = wrap_text(text)\nreturn text\n</code></pre>"},{"location":"api/functions/api_header/#pseudo_3D_interpolation.functions.header.write_textual_header","title":"<code>write_textual_header(path, txt, **kwargs_segy)</code>","text":"<p>Write textual header string to SEG-Y file.</p> Source code in <code>pseudo_3D_interpolation\\functions\\header.py</code> <pre><code>def write_textual_header(path, txt: str, **kwargs_segy):\n\"\"\"Write textual header string to SEG-Y file.\"\"\"\nif isinstance(txt, str):\ntxt = ''.join([t[:80] for t in txt.split('\\n')])\nelif isinstance(txt, list):\ntxt = ''.join([t[:80] for t in txt])  # silent truncating of each line if too long!\nelse:\nraise ValueError(f'Not supported textual header type: {type(txt)}')\nheader = bytes(txt, 'utf-8')\nassert len(header) == 3200, 'Binary string is too long, something went wrong...'\nkwargs_segy['ignore_geometry'] = True\nwith segyio.open(path, 'r+', **kwargs_segy) as f:\nf.text[0] = header\n</code></pre>"},{"location":"api/functions/api_plot/","title":"<code>plot.py</code>","text":"<p>Utility functions for plotting.</p>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.trim_axes","title":"<code>trim_axes(axes, N)</code>","text":"<p>Trim unused axes from figure.</p> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def trim_axes(axes, N):\n\"\"\"Trim unused axes from figure.\"\"\"\naxes = axes.ravel()\nfor ax in axes[N:]:\nax.remove()\nreturn axes[:N]\n</code></pre>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.plot_seismic_image","title":"<code>plot_seismic_image(data, dt=None, twt=None, traces=None, cmap='Greys', show_colormap=True, show_xaxis_labels=True, gain=1, norm=False, title=None, env=False, reverse=False, units='ms', label_kwargs=None, plot_kwargs=None)</code>","text":"<p>Plot seismic traces of SEG-Y file as image using specified colormap and gain.</p> <p>Parameters:</p> <ul> <li> data             (<code>numpy.array</code>)         \u2013 <p>2D array of SEG-Y trace data..</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in specified units (default: <code>seconds</code>). The default is None.</p> </li> <li> twt             (<code>np.array, optional</code>)         \u2013 <p>1D array of two-way traveltimes (TWT, default: <code>seconds</code>). The default is None.</p> </li> <li> traces             (<code>np.array, optional</code>)         \u2013 <p>1D array of trace indices (default: <code>None</code>).</p> </li> <li> cmap             (<code>str, optional</code>)         \u2013 <p>Matplotlib-compatible string of colormap (default: <code>Greys</code>).</p> </li> <li> gain             (<code>int, optional</code>)         \u2013 <p>Custom gain parameter (for visualization only) (default: <code>1</code>).</p> </li> <li> norm         \u2013 <p>Normalize amplitude of trace(s) using <code>rms</code> or <code>peak</code> amplitude.</p> </li> <li> title             (<code>str</code>)         \u2013 <p>Figure title (e.g. filename) (default: <code>None</code>).</p> </li> <li> env             (<code>bool, optional</code>)         \u2013 <p>Envelope as input data type (default: <code>False</code>, i.e. expecting <code>amplitude</code> date).</p> </li> <li> reverse             (<code>bool, optional</code>)         \u2013 <p>Reverse profile orientation for plotting (default: <code>False</code>).</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Time units (y-axis) (default: <code>ms</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> fig(            <code>matplotlib.figure.Figure</code> )        \u2013 <p>Figure handle.</p> </li> <li> ax(            <code>matplotlib.axes.Axes</code> )        \u2013 <p>Axes handle.</p> </li> <li> colormap(            <code>matplotlib.colorbar.Colorbar</code> )        \u2013 <p>Colormap handle.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def plot_seismic_image(\ndata,\ndt=None,\ntwt=None,\ntraces=None,\ncmap='Greys',\nshow_colormap=True,\nshow_xaxis_labels=True,\ngain=1,\nnorm=False,\ntitle=None,\nenv=False,\nreverse=False,\nunits='ms',\nlabel_kwargs=None,\nplot_kwargs=None,\n):\n\"\"\"\n    Plot seismic traces of SEG-Y file as image using specified colormap and gain.\n    Parameters\n    ----------\n    data : numpy.array\n        2D array of SEG-Y trace data..\n    dt : float, optional\n        Sampling interval in specified units (default: `seconds`).\n        The default is None.\n    twt : np.array, optional\n        1D array of two-way traveltimes (TWT, default: `seconds`).\n        The default is None.\n    traces : np.array, optional\n        1D array of trace indices (default: `None`).\n    cmap : str, optional\n        Matplotlib-compatible string of colormap (default: `Greys`).\n    gain : int, optional\n        Custom gain parameter (for visualization only) (default: `1`).\n    norm :\n        Normalize amplitude of trace(s) using `rms` or `peak` amplitude.\n    title : str\n        Figure title (e.g. filename) (default: `None`).\n    env : bool, optional\n        Envelope as input data type (default: `False`, i.e. expecting `amplitude` date).\n    reverse : bool, optional\n        Reverse profile orientation for plotting (default: `False`).\n    units : str, optional\n        Time units (y-axis) (default: `ms`).\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure handle.\n    ax : matplotlib.axes.Axes\n        Axes handle.\n    colormap : matplotlib.colorbar.Colorbar\n        Colormap handle.\n    \"\"\"\n# get samples and traces from data\nnsamples, ntraces = data.shape\n# create time axis (convert dt fro ms to s)\nif dt is None and twt is None:\nraise ValueError('Either dt or twt required')\nelif dt is not None:\ntwt = np.linspace(0, dt * nsamples, nsamples)\nelif twt is not None:\ndt = np.mean(np.diff(twt))\n# normalize\nif norm is True or isinstance(norm, str) and norm.lower() == 'rms':\ndata = rms_normalization(data, axis=0)\nelif isinstance(norm, str) and norm.lower() in ['max', 'peak']:\ndata /= np.max(np.abs(data))\n# set plotting extent [xmin, xmax, ymin, ymax]\n_offset = 0.5 * dt\nextent = [-_offset, ntraces + _offset, twt[-1] + _offset, twt[0] - _offset]\n# clip amplitude data for plotting\nif gain is not None:\nclip_percentile = ((1 - gain) * 2) + 97.5  # empirically tested\nvm = np.percentile(data, clip_percentile)  # clipping\nelse:\nvm = np.max(np.abs(data))\n# adjust parameter for colormap\nvmax = vm\nif env:\nvmin = 0\ndata_label = 'envelope'\nelse:\nvmin = -vm\ndata_label = 'amplitude'\nx_label = 'trace #' if traces is None else 'field record number'\ny_label = f'time ({units})'\nif label_kwargs is None:\nlabel_kwargs = dict(labels_size=12, ticklabels_size=10, title_size=12)\n# create figure and axes\nif plot_kwargs is None:\nplot_kwargs = dict(figsize=(16, 8))\nfig, ax = plt.subplots(1, 1, **plot_kwargs)\n# plot data\nprofile = ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto', extent=extent)\n# create colormap\nif show_colormap:\ncolormap = fig.colorbar(\nprofile,\nax=ax,\npad=0.025,\nfraction=0.05,  # pad=0.01\nlocation='right',\norientation='vertical',\nformat='%.3f',\n)\ncolormap.ax.set_ylabel(\ndata_label, labelpad=25, rotation=270, fontsize=label_kwargs['labels_size']\n)\ncolormap.ax.tick_params(axis='y', labelsize=label_kwargs['ticklabels_size'])\n# set x-axis\n## ticks\nif traces is not None:\nif ntraces &lt; 25:\nxticks = np.arange(0, ntraces, 1)\nxticklabels = [str(t) for t in traces]\nelse:  # too many labels to plot for every trace\nxticks = np.arange(\n0, ntraces + 1, np.around(ntraces // 10, 1 - len(str(ntraces // 10)))\n)\nxticks = np.append(xticks, np.atleast_1d(ntraces - 1), axis=0)\nxticks = xticks[xticks &lt; ntraces]\nxticklabels = [str(t) for t in traces[xticks]]\nax.set_xticks(xticks)\nax.set_xticklabels(\nxticklabels, rotation=45, ha='left', fontsize=label_kwargs['ticklabels_size']\n)\nax.xaxis.tick_top()\n## labels\nif show_xaxis_labels:\nax.set_xlabel(x_label, fontweight='semibold', fontsize=label_kwargs['labels_size'])\nax.xaxis.set_label_position('top')\nelse:\nax.set_xticklabels([])\n# set y-axis\n## ticks\n# ax.set_ylim([twt.max(), twt.min()])\nax.tick_params(\naxis='y', which='minor', direction='out', bottom=False, top=False, left=True, right=False\n)\nax.yaxis.set_minor_locator(AutoMinorLocator(11))\n## labels\nax.set_ylabel(y_label, fontweight='semibold', fontsize=label_kwargs['labels_size'])\nax.tick_params(axis='both', labelsize=label_kwargs['ticklabels_size'])\n# set subplot title\nif title is not None:\nax.set_title(title, fontweight='semibold', fontsize=label_kwargs['title_size'])\n# reverse profile plot if needed\nif reverse:\nax.invert_xaxis()\nfig.tight_layout(pad=1.1)\nif show_colormap:\nreturn fig, ax, colormap\nelse:\nreturn fig, ax\n</code></pre>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.plot_seismic_image_diff","title":"<code>plot_seismic_image_diff(data_org, data_edit, dt=None, twt=None, traces=None, cmap='Greys', show_colormap=True, gain=1, env=False, norm=False, reverse=False, titles=None, units='ms', plot_kwargs=None)</code>","text":"<p>Plot seismic traces of SEG-Y file as image using specified colormap and gain.</p> <p>Parameters:</p> <ul> <li> data_org             (<code>numpy.ndarray</code>)         \u2013 <p>2D arrays of SEG-Y trace data (original and edited).</p> </li> <li> data_edit             (<code>numpy.ndarray</code>)         \u2013 <p>2D arrays of SEG-Y trace data (original and edited).</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in specified units (default: <code>seconds</code>). The default is None.</p> </li> <li> twt             (<code>np.array, optional</code>)         \u2013 <p>1D array of two-way traveltimes (TWT, default: <code>seconds</code>). The default is None.</p> </li> <li> traces             (<code>np.array, optional</code>)         \u2013 <p>1D array of trace indices (default: <code>None</code>).</p> </li> <li> cmap             (<code>str, optional</code>)         \u2013 <p>Matplotlib-compatible string of colormap (default: <code>Greys</code>).</p> </li> <li> gain             (<code>int, optional</code>)         \u2013 <p>Custom gain parameter (for visualization only) (default: <code>1</code>).</p> </li> <li> norm         \u2013 <p>Normalize amplitude of trace(s) using <code>rms</code> or <code>peak</code> amplitude.</p> </li> <li> env             (<code>bool, optional</code>)         \u2013 <p>Envelope as input data type. The default is False (amplitude).</p> </li> <li> reverse             (<code>bool, optional</code>)         \u2013 <p>Reverse profile orientation for plotting (default: <code>False</code>).</p> </li> <li> titles             (<code>list, optional</code>)         \u2013 <p>List of plot titles (as strings). Should be exactly 3 elements.</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Time units (y-axis) (default: <code>ms</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> fig(            <code>matplotlib.figure.Figure</code> )        \u2013 <p>Figure handle.</p> </li> <li> ax(            <code>matplotlib.axes.Axes</code> )        \u2013 <p>Axes handle.</p> </li> <li> colormap(            <code>matplotlib.colorbar.Colorbar</code> )        \u2013 <p>Colormap handle.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def plot_seismic_image_diff(\ndata_org,\ndata_edit,\ndt=None,\ntwt=None,\ntraces=None,\ncmap='Greys',\nshow_colormap=True,\ngain=1,\nenv=False,\nnorm=False,\nreverse=False,\ntitles=None,\nunits='ms',\nplot_kwargs=None,\n):\n\"\"\"\n    Plot seismic traces of SEG-Y file as image using specified colormap and gain.\n    Parameters\n    ----------\n    data_org, data_edit : numpy.ndarray\n        2D arrays of SEG-Y trace data (original and edited).\n    dt : float, optional\n        Sampling interval in specified units (default: `seconds`).\n        The default is None.\n    twt : np.array, optional\n        1D array of two-way traveltimes (TWT, default: `seconds`).\n        The default is None.\n    traces : np.array, optional\n        1D array of trace indices (default: `None`).\n    cmap : str, optional\n        Matplotlib-compatible string of colormap (default: `Greys`).\n    gain : int, optional\n        Custom gain parameter (for visualization only) (default: `1`).\n    norm :\n        Normalize amplitude of trace(s) using `rms` or `peak` amplitude.\n    env : bool, optional\n        Envelope as input data type. The default is False (amplitude).\n    reverse : bool, optional\n        Reverse profile orientation for plotting (default: `False`).\n    titles : list, optional\n        List of plot titles (as strings). Should be exactly 3 elements.\n    units : str, optional\n        Time units (y-axis) (default: `ms`).\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure handle.\n    ax : matplotlib.axes.Axes\n        Axes handle.\n    colormap : matplotlib.colorbar.Colorbar\n        Colormap handle.\n    \"\"\"\n# get samples and traces from data\nnsamples, ntraces = data_org.shape\n# create time axis (convert dt fro ms to s)\nif dt is None and twt is None:\nraise ValueError('Either dt or twt required')\nelif dt is not None:\ntwt = np.linspace(0, dt * nsamples, nsamples)\nelif twt is not None:\ndt = np.mean(np.diff(twt))\n# normalize\nif norm is True or isinstance(norm, str) and norm.lower() == 'rms':\ndata_org = rms_normalization(data_org, axis=0)\ndata_edit = rms_normalization(data_edit, axis=0)\nelif isinstance(norm, str) and norm.lower() in ['max', 'peak']:\ndata_org /= np.max(np.abs(data_org))\ndata_edit /= np.max(np.abs(data_edit))\n# create difference array\nif data_org.shape == data_edit.shape:\ndata_diff = data_org - data_edit\nelse:\ndata_diff = np.zeros_like(data_org)\n# set plotting extent [xmin, xmax, ymin, ymax]\n_offset = 0.5 * dt\nextent = [-_offset, ntraces + _offset, twt[-1] + _offset, twt[0] - _offset]\n# clip amplitude data for plotting\nif gain is not None:\nclip_percentile = ((1 - gain) * 2) + 97.5  # empirically tested\nvm = np.percentile(data_org, clip_percentile)  # clipping\nelse:\nvm = np.max(np.abs(data_org))\n# adjust parameter for colormap\nvmax = vm\nvmax_diff = np.max(np.abs(data_diff))\nif env:\nvmin = 0\nvmin_diff = 0\ndata_label = 'envelope'\nelse:\nvmin = -vm\nvmin_diff = -vmax_diff\ndata_label = 'amplitude'\ntitles = ['original', 'edited', 'difference'] if titles is None else titles\nif len(titles) != 3:\nraise ValueError('Number of elements in `titles` must be 3 but is {len(titles)}')\nx_label = 'trace #' if traces is None else 'field record number'\ny_label = f'time [{units}]'\ncmaps = [cmap, cmap, 'seismic'] if not env else [cmap, cmap, 'Reds']\nvmins = [vmin, vmin, vmin_diff]\nvmaxs = [vmax, vmax, vmax_diff]\n# create figure and axes\nif plot_kwargs is None:\nplot_kwargs = dict(figsize=(16, 8))\nfig, ax = plt.subplots(1, 3, sharex=True, sharey=True, **plot_kwargs)\nfor i, data in enumerate([data_org, data_edit, data_diff]):\n# plot data\nprofile = ax[i].imshow(\ndata, cmap=cmaps[i], vmin=vmins[i], vmax=vmaxs[i], aspect='auto', extent=extent\n)\n# create colormap\nif show_colormap:\ncolormap = fig.colorbar(\nprofile,\nax=ax[i],\npad=0.02,\nfraction=0.05,\nshrink=0.9,\nlocation='bottom',\norientation='horizontal',\nformat='%.3f',\n)\ncolormap.ax.set_xlabel(data_label, labelpad=5, fontsize=10)\n# set x-axis\n## ticks\nif traces is not None:\nif ntraces &lt; 25:\nxticks = np.arange(0, ntraces, 1)\nxticklabels = [str(t) for t in traces]\nelse:  # too many labels to plot for every trace\nxticks = np.arange(\n0, ntraces + 1, np.around(ntraces // 10, 1 - len(str(ntraces // 10)))\n)\nxticks = np.append(xticks, np.atleast_1d(ntraces - 1), axis=0)\nxticklabels = [str(t) for t in traces[xticks]]\nax[i].set_xticks(xticks)\nax[i].set_xticklabels(xticklabels, rotation=45, ha='left', fontsize=10)\nax[i].xaxis.tick_top()\n## labels\nax[i].set_xlabel(x_label, fontweight='semibold')\nax[i].xaxis.set_label_position('top')\n# set y-axis\n## ticks\n# ax[i].set_ylim([twt.max(), twt.min()])\nax[i].tick_params(\naxis='y',\nwhich='minor',\ndirection='out',\nbottom=False,\ntop=False,\nleft=True,\nright=False,\n)\nax[i].yaxis.set_minor_locator(AutoMinorLocator(11))\n# set subplot title\nax[i].set_title(titles[i], fontweight='semibold', fontsize=12)\n# reverse profile plot if needed\nif reverse:\nax[i].invert_xaxis()\n## labels\nax[0].set_ylabel(y_label, fontweight='semibold')\nfig.tight_layout()\nfig.subplots_adjust(wspace=0.05)\nif show_colormap:\nreturn fig, ax, colormap\nelse:\nreturn fig, ax\n</code></pre>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.plot_seismic_wiggle","title":"<code>plot_seismic_wiggle(data, dt=None, twt=None, traces=None, add_info=None, title=None, gain=1.0, norm=False, tr_step=1, color='k', units='ms', plot_kwargs=None)</code>","text":"<p>Plot seismic section using wiggle traces.</p> <p>Parameters:</p> <ul> <li> data             (<code>np.array</code>)         \u2013 <p>Seismic data (samples x traces).</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in specified units (default: <code>seconds</code>). The default is None.</p> </li> <li> twt             (<code>np.array, optional</code>)         \u2013 <p>1D array of two-way traveltimes (TWT, default: <code>seconds</code>). The default is None.</p> </li> <li> traces             (<code>np.array, optional</code>)         \u2013 <p>1D array of trace indices (default: <code>None</code>).</p> </li> <li> add_info             (<code>list of strings, optional</code>)         \u2013 <p>Additional information (e.g. delay time) to annotate trace labels with. The default is None.</p> </li> <li> title             (<code>str, optional</code>)         \u2013 <p>Plot title string (default: <code>None</code>).</p> </li> <li> gain             (<code>float, optional</code>)         \u2013 <p>Gain value (default: <code>1.0</code>).</p> </li> <li> norm         \u2013 <p>Normalize amplitude of trace(s) using <code>rms</code> or <code>peak</code> amplitude.</p> </li> <li> tr_step             (<code>int, optional</code>)         \u2013 <p>Plot every {tr_step} trace in data (default: <code>1</code>).</p> </li> <li> color             (<code>str, optional</code>)         \u2013 <p>Fill color for positive wiggle (default: <code>k</code>).</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Time units (y-axis) (default: <code>ms</code>).</p> </li> <li> plot_kwargs             (<code>dict, optional</code>)         \u2013 <p>Keyword arguments for plt.subplots call</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def plot_seismic_wiggle(\ndata,\ndt=None,\ntwt=None,\ntraces=None,\nadd_info=None,\ntitle=None,\ngain=1.0,\nnorm=False,\ntr_step=1,\ncolor='k',\nunits='ms',\nplot_kwargs=None,\n):\n\"\"\"\n    Plot seismic section using wiggle traces.\n    Parameters\n    ----------\n    data : np.array\n        Seismic data (samples x traces).\n    dt : float, optional\n        Sampling interval in specified units (default: `seconds`).\n        The default is None.\n    twt : np.array, optional\n        1D array of two-way traveltimes (TWT, default: `seconds`).\n        The default is None.\n    traces : np.array, optional\n        1D array of trace indices (default: `None`).\n    add_info : list of strings, optional\n        Additional information (e.g. delay time) to annotate trace labels with.\n        The default is None.\n    title : str, optional\n        Plot title string (default: `None`).\n    gain : float, optional\n        Gain value (default: `1.0`).\n    norm :\n        Normalize amplitude of trace(s) using `rms` or `peak` amplitude.\n    tr_step : int, optional\n        Plot every {tr_step} trace in data (default: `1`).\n    color : str, optional\n        Fill color for positive wiggle (default: `k`).\n    units : str, optional\n        Time units (y-axis) (default: `ms`).\n    plot_kwargs : dict, optional\n        Keyword arguments for plt.subplots call\n    \"\"\"\nif traces is not None and add_info is not None:\nassert (\ntraces.size == add_info.size\n), f'Additional annotations must be list of same length as traces ({traces.size})'\n# initialise plot\nif plot_kwargs is None:\nplot_kwargs = dict(figsize=(8, 8))\nfig, ax = plt.subplots(1, 1, **plot_kwargs)\n# select subsets using {tr_step}\ndata = data[:, ::tr_step]\ntraces = traces[::tr_step] if traces is not None else None\nadd_info = add_info[::tr_step] if add_info is not None else None\n# get samples and traces from data\nnsamples, ntraces = data.shape\n# create time axis (convert dt fro ms to s)\nif dt is None and twt is None:\nraise ValueError('Either dt or twt required')\nelif dt is not None:\nt = np.linspace(0, dt * nsamples, nsamples)\nelif twt is not None:\nt = twt\n# normalize\nif norm is True or isinstance(norm, str) and norm.lower() == 'rms':\ndata = rms_normalization(data, axis=0)\nelif isinstance(norm, str) and norm.lower() in ['max', 'peak']:\ndata /= np.max(np.abs(data))\n# get start and end traces\nif traces is None:\nx_start, x_end = 1, ntraces + 1\nelif isinstance(traces, tuple):\nx_start, x_end = traces\nelse:\nx_start, x_end = traces[0], traces[-1] + 1\n# get horizontal increment\ndx = np.around((x_end - x_start) / ntraces, 0)\n# create axes labels\nx_label = 'trace #' if traces is None else 'field record number'\ny_label = f'time [{units}]'\n# set x-axis\n## ticks\nif traces is not None:\nif ntraces &lt; 25:\nxticks = np.arange(x_start, x_end, tr_step)\nxticklabels = [str(t) for t in traces]\nelse:  # too many labels to plot for every trace\nxticks = np.arange(\nx_start, x_end, np.around(ntraces // 10, 1 - len(str(ntraces // 10)))\n)\nxticklabels = [str(t) for t in traces[xticks - x_start]]\n# add additional text annotations (per trace)\nif add_info is not None:\nprint(xticklabels)\nadd_info = add_info[np.isin(traces, [int(t) for t in xticklabels]).nonzero()[0]]\nxticklabels = [f'{s}:{info}' for s, info in zip(xticklabels, add_info)]\nax.set_xticks(xticks)\nax.set_xticklabels(xticklabels, rotation=45, ha='left')\nax.xaxis.tick_top()\nax.set_xlim(x_start - 1, x_end)\n## labels\nax.set_xlabel(x_label, fontweight='semibold')\nax.xaxis.set_label_position('top')\n# set y-axis\n## ticks\nax.set_ylim([t.max(), t.min()])\nax.tick_params(\naxis='y', which='minor', direction='out', bottom=False, top=False, left=True, right=False\n)\nax.yaxis.set_minor_locator(AutoMinorLocator(11))\n## labels\nax.set_ylabel(y_label, fontweight='semibold')\n# set title\nif title is not None:\nax.set_title(title)\nfor i, trace in enumerate(data.T):  # single trace per row with sample as col\ntr = trace * gain * dx  # scale trace and add offset\nx = x_start + i * dx  # calc x position for trace\nax.plot(x + tr, t, 'k', lw=0.5)\nax.fill_betweenx(t, x + tr, x, where=(tr &gt;= 0), color=color)\nfig.tight_layout()\nreturn fig, ax\n</code></pre>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.plot_seismic_wiggle_diff","title":"<code>plot_seismic_wiggle_diff(data_org, data_edit, dt=None, twt=None, traces=None, add_info=None, gain=1.0, norm=False, tr_step=1, color='k', titles=None, units='ms', plot_kwargs=None)</code>","text":"<p>Plot seismic section using wiggle traces.</p> <p>Parameters:</p> <ul> <li> data_org             (<code>np.array</code>)         \u2013 <p>Seismic data arrays (samples x traces).</p> </li> <li> data_edit             (<code>np.array</code>)         \u2013 <p>Seismic data arrays (samples x traces).</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in specified units (default: <code>seconds</code>). The default is None.</p> </li> <li> twt             (<code>np.array, optional</code>)         \u2013 <p>1D array of two-way traveltimes (TWT, default: <code>seconds</code>). The default is None.</p> </li> <li> traces             (<code>np.array, optional</code>)         \u2013 <p>1D array of trace indices (default: <code>None</code>).</p> </li> <li> add_info             (<code>list of strings, optional</code>)         \u2013 <p>Additional information (e.g. delay time) to annotate trace labels with. The default is None.</p> </li> <li> gain             (<code>float, optional</code>)         \u2013 <p>Gain value (default: <code>1.0</code>).</p> </li> <li> norm         \u2013 <p>Normalize amplitude of trace(s) using <code>rms</code> or <code>peak</code> amplitude.</p> </li> <li> tr_step             (<code>int, optional</code>)         \u2013 <p>Plot every {tr_step} trace in data (default: <code>1</code>).</p> </li> <li> color             (<code>str, optional</code>)         \u2013 <p>Fill color for positive wiggle (default: <code>k</code>).</p> </li> <li> titles             (<code>list, optional</code>)         \u2013 <p>List of plot titles (as strings). Should be exactly 3 elements.</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Time units (y-axis) (default: <code>ms</code>).</p> </li> <li> plot_kwargs             (<code>dict, optional</code>)         \u2013 <p>Keyword arguments for plt.subplots call</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def plot_seismic_wiggle_diff(\ndata_org,\ndata_edit,\ndt=None,\ntwt=None,\ntraces=None,\nadd_info=None,\ngain=1.0,\nnorm=False,\ntr_step=1,\ncolor='k',\ntitles=None,\nunits='ms',\nplot_kwargs=None,\n):\n\"\"\"\n    Plot seismic section using wiggle traces.\n    Parameters\n    ----------\n    data_org, data_edit : np.array\n        Seismic data arrays (samples x traces).\n    dt : float, optional\n        Sampling interval in specified units (default: `seconds`).\n        The default is None.\n    twt : np.array, optional\n        1D array of two-way traveltimes (TWT, default: `seconds`).\n        The default is None.\n    traces : np.array, optional\n        1D array of trace indices (default: `None`).\n    add_info : list of strings, optional\n        Additional information (e.g. delay time) to annotate trace labels with.\n        The default is None.\n    gain : float, optional\n        Gain value (default: `1.0`).\n    norm :\n        Normalize amplitude of trace(s) using `rms` or `peak` amplitude.\n    tr_step : int, optional\n        Plot every {tr_step} trace in data (default: `1`).\n    color : str, optional\n        Fill color for positive wiggle (default: `k`).\n    titles : list, optional\n        List of plot titles (as strings). Should be exactly 3 elements.\n    units : str, optional\n        Time units (y-axis) (default: `ms`).\n    plot_kwargs : dict, optional\n        Keyword arguments for plt.subplots call\n    \"\"\"\nassert (\ndata_org.shape == data_edit.shape\n), f'Original array {data_org.shape} and edited array {data_edit.shape} must have identical shapes!'\nif traces is not None and add_info is not None:\nassert (\ntraces.size == add_info.size\n), f'Additional annotations must be list of same length as traces ({traces.size})'\n# select subsets using {tr_step}\ndata = data_org[:, ::tr_step]\ntraces = traces[::tr_step] if traces is not None else None\nadd_info = add_info[::tr_step] if add_info is not None else None\n# get samples and traces from data\nnsamples, ntraces = data_org.shape\n# create time axis (convert dt fro ms to s)\nif dt is None and twt is None:\nraise ValueError('Either dt or twt required')\nelif dt is not None:\nt = np.linspace(0, dt * nsamples, nsamples)\nelif twt is not None:\nt = twt\n# get start and end traces\nif traces is None:\nx_start, x_end = 1, ntraces + 1\nelif isinstance(traces, tuple):\nx_start, x_end = traces\nelse:\nx_start, x_end = traces[0], traces[-1] + 1\n# get horizontal increment\ndx = np.around((x_end - x_start) / ntraces, 0)\n# create axes labels\nx_label = 'trace #' if traces is None else 'field record number'\ny_label = f'time [{units}]'\n# normalize\nif norm is True or isinstance(norm, str) and norm.lower() == 'rms':\ndata_org = rms_normalization(data_org, axis=0)\ndata_edit = rms_normalization(data_edit, axis=0)\nelif isinstance(norm, str) and norm.lower() in ['max', 'peak']:\ndata_org /= np.max(np.abs(data_org))\ndata_edit /= np.max(np.abs(data_edit))\n# create difference array\nif data_org.shape == data_edit.shape:\ndata_diff = data_org - data_edit\nelse:\ndata_diff = np.zeros_like(data_org)\n# initialise plot\nif plot_kwargs is None:\nplot_kwargs = dict(figsize=(16, 8))\nfig, axes = plt.subplots(1, 3, sharex=True, sharey=True, **plot_kwargs)\ntitles = ['original', 'edited', 'difference'] if titles is None else titles\nif len(titles) != 3:\nraise ValueError('Number of elements in `titles` must be 3 but is {len(titles)}')\nfor data, ax, title in zip([data_org, data_edit, data_diff], axes, titles):\n# set x-axis\n## ticks\nif traces is not None:\nif ntraces &lt; 25:\nxticks = np.arange(x_start, x_end, tr_step)\nxticklabels = [str(t) for t in traces]\nelse:  # too many labels to plot for every trace\nxticks = np.arange(\nx_start, x_end, np.around(ntraces // 10, 1 - len(str(ntraces // 10)))\n)\nxticklabels = [str(t) for t in traces[xticks - x_start]]\n# add additional text annotations (per trace)\nif add_info is not None:\nxticklabels = [f'{s}:{info}' for s, info in zip(xticklabels, add_info)]\nax.set_xticks(xticks)\nax.set_xticklabels(xticklabels, rotation=45, ha='left')\nax.xaxis.tick_top()\nax.set_xlim(x_start - 1, x_end)\n## labels\nax.set_xlabel(x_label, fontweight='semibold')\nax.xaxis.set_label_position('top')\n# set y-axis\n## ticks\nax.set_ylim([t.max(), t.min()])\nax.tick_params(\naxis='y',\nwhich='minor',\ndirection='out',\nbottom=False,\ntop=False,\nleft=True,\nright=False,\n)\nax.yaxis.set_minor_locator(AutoMinorLocator(11))\n# set subplot title\nax.set_title(title, fontweight='semibold', fontsize=12)\nfor i, trace in enumerate(data.T):  # single trace per row with sample as col\ntr = trace * gain * dx  # scale trace and add offset\nx = x_start + i * dx  # calc x position for trace\nax.plot(x + tr, t, 'k', lw=0.5)\nax.fill_betweenx(t, x + tr, x, where=(tr &gt;= 0), color=color)\n## labels\naxes[0].set_ylabel(y_label, fontweight='semibold')\nfig.tight_layout()\nfig.subplots_adjust(wspace=0.1)\nreturn fig, axes\n</code></pre>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.plot_trace_freq_spectrum","title":"<code>plot_trace_freq_spectrum(data, dt=None, Fs=None, trace_labels=None, units='ms', plot_mvg_avg=True, plot_combined=True, fig_kwargs=None)</code>","text":"<p>Plot frequency spectrum of input trace(s).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Trace data (samples x traces).</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in milliseconds [ms]. Either <code>dt</code> or <code>Fs</code> needed.</p> </li> <li> Fs             (<code>int, optional</code>)         \u2013 <p>Sampling rate [Hz]. Either <code>dt</code> or <code>Fs</code> needed.</p> </li> <li> trace_labels             (<code>np.ndarray, optional</code>)         \u2013 <p>Array of trace labels (e.g. field record numbers) (default: <code>None</code>).</p> </li> <li> units             (<code>str, optional</code>)         \u2013 <p>Unit of <code>dt</code> as ['s', 'ms', 'ns'] (default: <code>ms</code>).</p> </li> <li> plot_mvg_avg             (<code>bool, optional</code>)         \u2013 <p>Plot moving average of spectrum(s) (default: <code>True</code>).</p> </li> <li> plot_combined             (<code>bool, optional</code>)         \u2013 <p>Plot average spectrum of all traces (default: <code>True</code>).</p> </li> <li> fig_kwargs             (<code>dict, optional</code>)         \u2013 <p>Optional keyword argument for figure creation (default: <code>None</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> fig(            <code>figure.Figure</code> )        \u2013 <p>Matplotlib igure object.</p> </li> <li> ax(            <code>axes.Subplots</code> )        \u2013 <p>Matplotlib subplot axes.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def plot_trace_freq_spectrum(\ndata,\ndt=None,\nFs=None,\ntrace_labels=None,\nunits='ms',\nplot_mvg_avg=True,\nplot_combined=True,\nfig_kwargs=None,\n):\n\"\"\"\n    Plot frequency spectrum of input trace(s).\n    Parameters\n    ----------\n    data : np.ndarray\n        Trace data (samples x traces).\n    dt : float, optional\n        Sampling interval in milliseconds [ms]. Either `dt` or `Fs` needed.\n    Fs : int, optional\n        Sampling rate [Hz]. Either `dt` or `Fs` needed.\n    trace_labels : np.ndarray, optional\n        Array of trace labels (e.g. field record numbers) (default: `None`).\n    units : str, optional\n        Unit of `dt` as ['s', 'ms', 'ns'] (default: `ms`).\n    plot_mvg_avg : bool, optional\n        Plot moving average of spectrum(s) (default: `True`).\n    plot_combined : bool, optional\n        Plot average spectrum of all traces (default: `True`).\n    fig_kwargs : dict, optional\n        Optional keyword argument for figure creation (default: `None`).\n    Returns\n    -------\n    fig : figure.Figure\n        Matplotlib igure object.\n    ax : axes.Subplots\n        Matplotlib subplot axes.\n    \"\"\"\n# get number of samples and traces\nif data.ndim == 1:\nnsamples, ntraces = data.size, 1\nncols, nrows = 1, 1\ndata = np.atleast_2d(data).T\nelse:\nnsamples, ntraces = data.shape\nprint('data.shape:', data.shape)\nntr = ntraces + 1 if plot_combined and ntraces &gt; 1 else ntraces\nprint('ntr:', ntr)\nncols = int(np.ceil(np.sqrt(ntr)))\nprint('ncols:', ncols)\nnrows = 1 if ncols == ntr else ncols - 1\nprint('nrows:', nrows)\nnrows = nrows + 1 if ntr &gt; ncols * nrows else nrows\nprint('nrows:', nrows)\nif dt is None and Fs is None:\nraise ValueError('Either `dt` or `Fs` is required.')\nif dt is not None and Fs is None:\nif units == 's':\npass\nelif units == 'ms':\ndt = dt / 1000\nelif units == 'ns':\ndt = dt / 1e-6\nelse:\nraise ValueError(f'Unit \"{units}\" is not supported for `dt`.')\nFs = 1 / dt  # in Hz (samples/s)\nif plot_mvg_avg:\navg_win_size = int(nsamples * 0.01)  # 1% of nsamples\navg_win_size = 2 if avg_win_size &lt; 2 else avg_win_size\nprint('avg_win_size:', avg_win_size)\nif fig_kwargs is None:\nfig_kwargs = dict(figsize=(ncols * 4, nrows * 3))\namps, fpeaks, fmins, fmaxs = [], [], [], []\n# initialize figure\nfig, axes = plt.subplots(nrows, ncols, sharex=True, sharey=True, **fig_kwargs)\n# set labels\nif ntraces &gt; 1:\nfor xax in axes[-1, :]:\nxax.set_xlabel('frequency [Hz]', fontsize=11)\nfor yax in axes[:, 0]:\nyax.set_ylabel('amplitude [-]', fontsize=11)\nelse:\naxes.set_xlabel('frequency [Hz]', fontsize=11)\naxes.set_ylabel('amplitude [-]', fontsize=11)\n# prepare subplot axes (remove unused subplots, account for ntraces ==1)\naxes = trim_axes(axes, ntr) if ntraces &gt; 1 else axes\naxes_iter = axes[:-1] if plot_combined and ntraces &gt; 1 else axes\naxes_iter = [axes_iter] if ntraces == 1 else axes_iter\n# loop over every trace in input array\nfor i, ax in enumerate(axes_iter):\n# compute frequency spectrum\nf, amp, f_min, f_max = freq_spectrum(data[:, i], Fs, return_minmax=True)\n# get frequency of peak amplitude\nf_peak = f[np.argmax(amp)]\n# plot frequency spectrum\nax.plot(f, amp, c='b', lw=0.5)\n# plot moving window average\nif plot_mvg_avg:\n# amp_win = moving_average(amp, avg_win_size, pad=True)\nhalf_window = (avg_win_size - 1) // 2\namp_win = pad_array(amp, half_window, zeros=True)\namp_win = moving_average(amp_win, win=avg_win_size)\nfx = f if f.size == amp_win.size else f[:-1]\nax.plot(fx, amp_win, c='k', lw=1)\n# set title\ntitle = f'trace #{trace_labels[i]}' if trace_labels is not None else f'trace #{i}'\nax.set_title(title, fontsize=12, fontweight='semibold')\n# annotate plot\nif f_min &gt; 1000:\nstats = f'\\nMin: {f_min/1000:.1f} kHz\\nMax: {f_max/1000:.1f} kHz\\nPeak: {f_peak/1000:.1f} kHz'\nelse:\nstats = f'\\nMin: {f_min:.1f} Hz\\nMax: {f_max:.1f} Hz\\nPeak: {f_peak:.1f} Hz'\nax.text(\n0.98,\n0.95,\n'AMPLITUDE SPECTRUM',\nhorizontalalignment='right',\nverticalalignment='top',\nfontweight='normal',\ncolor='b',\ntransform=ax.transAxes,\nfontsize=11,\n)\nax.text(\n0.98,\n0.95,\nstats,\nhorizontalalignment='right',\nverticalalignment='top',\ntransform=ax.transAxes,\nfontsize=9,\n)\n# append trace spectrum parameter to list\namps.append(amp)\nfpeaks.append(f_peak)\nfmins.append(f_min)\nfmaxs.append(f_max)\n# set overall x-axis limits\n# ax.set_xlim([np.min(fmins), np.max(fmaxs)])\nif plot_combined and ntraces &gt; 1:\n# calc averages\namp_mean = np.nanmean(np.array(amps), axis=0)\nfpeaks_mean = np.mean(fpeaks)\nfmins_mean = np.mean(fmins)\nfmaxs_mean = np.mean(fmaxs)\nf_nyquist = Fs // 2\n# plot average frequency spectrum\naxes[-1].fill_between(f, 0, amp_mean, color='grey')\n# plot moving window average\n# amp_mean_win = moving_average(amp_mean, avg_win_size, pad=True)\nhalf_window = (avg_win_size - 1) // 2\namp_mean_win = pad_array(amp_mean, half_window, zeros=True)\namp_mean_win = moving_average(amp_mean_win, win=avg_win_size)\nfx = f if f.size == amp_mean_win.size else f[:-1]\naxes[-1].plot(fx, amp_mean_win, 'k', lw=1)\n# annotate plot\naxes[-1].set_title('average spectrum', fontsize=12, fontweight='semibold')\nif f_min &gt; 1000:\nstats = f'\\nMin: {fmins_mean/1000:.1f} kHz\\nMax: {fmaxs_mean/1000:.1f} kHz\\nPeak: {fpeaks_mean/1000:.1f} kHz\\nNyquist: {f_nyquist/1000:.1f} kHz'\nelse:\nstats = f'\\nMin: {fmins_mean:.1f} Hz\\nMax: {fmaxs_mean:.1f} Hz\\nPeak: {fpeaks_mean:.1f} Hz\\nNyquist: {f_nyquist:.1f} Hz'\naxes[-1].text(\n0.98,\n0.95,\n'AMPLITUDE SPECTRUM',\nhorizontalalignment='right',\nverticalalignment='top',\nfontweight='bold',\ncolor='k',\ntransform=axes[-1].transAxes,\n)\naxes[-1].text(\n0.98,\n0.95,\nstats,\nhorizontalalignment='right',\nverticalalignment='top',\ntransform=axes[-1].transAxes,\nfontsize=9,\n)\nreturn fig, ax\n</code></pre>"},{"location":"api/functions/api_plot/#pseudo_3D_interpolation.functions.plot.plot_average_freq_spectrum","title":"<code>plot_average_freq_spectrum(data, dt=None, Fs=None, trace_labels=None, plot_mvg_avg=True, fig_kwargs=None)</code>","text":"<p>Plot frequency spectrum of input trace(s).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Trace data (samples x traces).</p> </li> <li> dt             (<code>float, optional</code>)         \u2013 <p>Sampling interval in milliseconds [ms]. Either <code>dt</code> or <code>Fs</code> needed.</p> </li> <li> Fs             (<code>int, optional</code>)         \u2013 <p>Sampling rate [Hz]. Either <code>dt</code> or <code>Fs</code> needed.</p> </li> <li> trace_labels             (<code>np.ndarray, optional</code>)         \u2013 <p>Array of trace labels (e.g. field record numbers) (default: <code>None</code>).</p> </li> <li> plot_mvg_avg             (<code>bool, optional</code>)         \u2013 <p>Plot moving average of spectrum(s) (default: <code>True</code>).</p> </li> <li> fig_kwargs             (<code>dict, optional</code>)         \u2013 <p>Optional keyword argument for figure creation (default: <code>None</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> fig(            <code>figure.Figure</code> )        \u2013 <p>Matplotlib igure object.</p> </li> <li> ax(            <code>axes.Subplots</code> )        \u2013 <p>Matplotlib subplot axes.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\plot.py</code> <pre><code>def plot_average_freq_spectrum(\ndata, dt=None, Fs=None, trace_labels=None, plot_mvg_avg=True, fig_kwargs=None\n):\n\"\"\"\n    Plot frequency spectrum of input trace(s).\n    Parameters\n    ----------\n    data : np.ndarray\n        Trace data (samples x traces).\n    dt : float, optional\n        Sampling interval in milliseconds [ms]. Either `dt` or `Fs` needed.\n    Fs : int, optional\n        Sampling rate [Hz]. Either `dt` or `Fs` needed.\n    trace_labels : np.ndarray, optional\n        Array of trace labels (e.g. field record numbers) (default: `None`).\n    plot_mvg_avg : bool, optional\n        Plot moving average of spectrum(s) (default: `True`).\n    fig_kwargs : dict, optional\n        Optional keyword argument for figure creation (default: `None`).\n    Returns\n    -------\n    fig : figure.Figure\n        Matplotlib igure object.\n    ax : axes.Subplots\n        Matplotlib subplot axes.\n    \"\"\"\n# get number of samples and traces\nif data.ndim == 1:\nnsamples, ntraces = data.size, 1\ndata = np.atleast_2d(data).T\nelse:\nnsamples, ntraces = data.shape\nif dt is None and Fs is None:\nraise ValueError('Either `dt` or `Fs` is required.')\nif dt is not None and Fs is None:\ndt = dt / 1000  # convert ms to s\nFs = 1 / dt  # in Hz (samples/s)\nif plot_mvg_avg:\navg_win_size = int(nsamples * 0.01)  # 1% of nsamples\navg_win_size = 2 if avg_win_size &lt; 2 else avg_win_size\nprint('avg_win_size:', avg_win_size)\nif fig_kwargs is None:\nfig_kwargs = dict(figsize=(12, 8))\namps, fpeaks, fmins, fmaxs = [], [], [], []\n# initialize figure\nfig, ax = plt.subplots(nrows=1, ncols=1, **fig_kwargs)\n# loop over every trace in input array\nfor i in range(ntraces):\n# compute frequency spectrum\nf, amp, f_min, f_max = freq_spectrum(data[:, i], Fs, return_minmax=True)\n# get frequency of peak amplitude\nf_peak = f[np.argmax(amp)]\n# append trace spectrum parameter to list\namps.append(amp)\nfpeaks.append(f_peak)\nfmins.append(f_min)\nfmaxs.append(f_max)\n# set overall x-axis limits\n# ax.set_xlim([np.min(fmins), np.max(fmaxs)])\nprint(np.min(fmins), np.max(fmaxs))\n# set labels\nax.set_xlabel('frequency [Hz]', fontsize=11)\nax.set_ylabel('amplitude [-]', fontsize=11)\n# calc averages\namp_mean = np.nanmean(np.array(amps), axis=0)\nfpeaks_mean = np.mean(fpeaks)\nfmins_mean = np.mean(fmins)\nfmaxs_mean = np.mean(fmaxs)\nf_nyquist = Fs // 2\n# plot average frequency spectrum\nax.fill_between(f, 0, amp_mean, color='grey')\n# plot moving window average\n# amp_mean_win = moving_average(amp_mean, avg_win_size, pad=True)\nhalf_window = (avg_win_size - 1) // 2\namp_mean_win = pad_array(amp_mean, half_window, zeros=True)\namp_mean_win = moving_average(amp_mean_win, win=avg_win_size)\nfx = f if f.size == amp_mean_win.size else f[:-1]\nax.plot(fx, amp_mean_win, 'k', lw=1)\n# annotate plot\nax.set_title('average spectrum', fontsize=12, fontweight='semibold')\nif f_min &gt; 1000:\nstats = f'\\nMin: {fmins_mean/1000:.1f} kHz\\nMax: {fmaxs_mean/1000:.1f} kHz\\nPeak: {fpeaks_mean/1000:.1f} kHz\\nNyquist: {f_nyquist/1000:.1f} kHz'\nelse:\nstats = f'\\nMin: {fmins_mean:.1f} Hz\\nMax: {fmaxs_mean:.1f} Hz\\nPeak: {fpeaks_mean:.1f} Hz\\nNyquist: {f_nyquist:.1f} Hz'\nax.text(\n0.98,\n0.95,\n'AMPLITUDE SPECTRUM',\nhorizontalalignment='right',\nverticalalignment='top',\nfontweight='bold',\ncolor='k',\ntransform=ax.transAxes,\n)\nax.text(\n0.98,\n0.95,\nstats,\nhorizontalalignment='right',\nverticalalignment='top',\ntransform=ax.transAxes,\nfontsize=9,\n)\nreturn fig, ax\n</code></pre>"},{"location":"api/functions/api_signal/","title":"<code>signal.py</code>","text":"<p>Utility functions for acoustic signals.</p>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.SNR","title":"<code>SNR(x, noise)</code>","text":"<p>Signal-to-noise ratio (SNR).</p> <p>Parameters:</p> <ul> <li> x             (<code>np.ndarray</code>)         \u2013 <p>Original signal.</p> </li> <li> noise             (<code>np.ndarray</code>)         \u2013 <p>Noisy signal.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013 <p>Signal-to-noise ratio between arrays (in dB).</p> </li> <li> <code>References</code>         \u2013        </li> <li> <code>----------</code>         \u2013        </li> <li> <code>[</code>         \u2013        </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def SNR(x, noise):\n\"\"\"\n    Signal-to-noise ratio (SNR).\n    Parameters\n    ----------\n    x : np.ndarray\n        Original signal.\n    noise : np.ndarray\n        Noisy signal.\n    Returns\n    -------\n    float\n        Signal-to-noise ratio between arrays (in dB).\n    References\n    ----------\n    [^1]: Yang et al. (2012) Curvelet-based POCS interpolation of nonuniformly sampled seismic records\n    \"\"\"\nif np.linalg.norm(x - noise) == 0:\nreturn np.Inf\nelse:\nreturn 10 * np.log10(np.sum(np.power(x, 2)) / np.sum(np.power(x - noise, 2)))\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.PSNR","title":"<code>PSNR(x, noise, max_pixel=1.0)</code>","text":"<p>Peak signal-to-noise ratio (SNR).</p> <p>Parameters:</p> <ul> <li> x             (<code>np.ndarray</code>)         \u2013 <p>Original signal.</p> </li> <li> noise             (<code>np.ndarray</code>)         \u2013 <p>Noisy signal.</p> </li> <li> max_pixel             (<code>float, optional</code>)         \u2013 <p>Maximum fluctuation in input image type. For <code>float</code>: 1, <code>uint8</code>: 255. If None, compute and use max(x).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013 <p>Peak signal-to-noise ratio between arrays (in dB).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def PSNR(x, noise, max_pixel=1.0):\n\"\"\"\n    Peak signal-to-noise ratio (SNR).\n    Parameters\n    ----------\n    x : np.ndarray\n        Original signal.\n    noise : np.ndarray\n        Noisy signal.\n    max_pixel : float, optional\n        Maximum fluctuation in input image type. For `float`: 1, `uint8`: 255.\n        If None, compute and use max(x).\n    Returns\n    -------\n    float\n        Peak signal-to-noise ratio between arrays (in dB).\n    \"\"\"\nMSE = np.mean((x - noise) ** 2)\nif MSE == 0:\nreturn np.inf\nif max_pixel is None:\nmax_pixel = np.max(x)\nreturn 10 * np.log10(max_pixel / np.sqrt(MSE))\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.estimate_noise_level","title":"<code>estimate_noise_level(img)</code>","text":"<p>Estimate image noise level based on Immerk\u00e6r (1996) \"Fast Noise Variance Estimation\".</p> <p>Parameters:</p> <ul> <li> img             (<code>np.ndarray</code>)         \u2013 <p>Input image.</p> </li> </ul> <p>Returns:</p> <ul> <li> sigma(            <code>float</code> )        \u2013 <p>Noise level factor.</p> </li> </ul>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.estimate_noise_level--references","title":"References","text":"<ol> <li> <p>https://stackoverflow.com/a/25436112\u00a0\u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def estimate_noise_level(img):\n\"\"\"\n    Estimate image noise level based on Immerk\u00e6r (1996) \"Fast Noise Variance Estimation\".\n    Parameters\n    ----------\n    img : np.ndarray\n        Input image.\n    Returns\n    -------\n    sigma : float\n        Noise level factor.\n    References\n    ----------\n    [^1]: https://stackoverflow.com/a/25436112\n    \"\"\"\nnrows, ncols = img.shape\nM = [[1, -2, 1], [-2, 4, -2], [1, -2, 1]]\nsigma = np.sum(np.sum(np.absolute(convolve2d(rescale(img, 0, 255), M))))\nsigma = sigma * np.sqrt(0.5 * np.pi) / (6 * (ncols - 2) * (nrows - 2))\nreturn sigma\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.gain","title":"<code>gain(data, twt, tpow=0.0, epow=0.0, etpow=1.0, ebase=None, gpow=0.0, clip=None, pclip=None, nclip=None, qclip=None, bias=None, scale=1.0, norm=False, norm_rms=False, copy=True, axis=0)</code>","text":"<p>Apply various different types of gain for either single trace (1D array) or seismic section (2D array).</p> <p>Copyright</p> <p>This function is a Python implementation of the Seismic Unix <code>sugain</code> module. Please refer to the license file <code>LICENSE_SeismicUnix</code>!</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Seismic trace (nsamples,) or section (nsamples, ntraces).</p> </li> <li> twt             (<code>np.ndarray</code>)         \u2013 <p>Array of samples (in seconds TWT) appropriate spacing of sample rate (<code>dt</code>).</p> </li> <li> tpow             (<code>float, optional</code>)         \u2013 <p>Multiply data by t^tpow (default: <code>0.0</code>).</p> </li> <li> epow             (<code>float, optional</code>)         \u2013 <p>Multiply data by exp(epow*t) (default: <code>0.0</code>).</p> </li> <li> etpow             (<code>float, optional</code>)         \u2013 <p>Multiply data by exp(epow*t^etpow) (default: <code>1.0</code>).</p> </li> <li> ebase             (<code>float, optional</code>)         \u2013 <p>Base of exponential function (default: <code>e</code>).</p> </li> <li> gpow             (<code>float, optional</code>)         \u2013 <p>Take signed gpowth power of scaled data (default: <code>0.0</code>).</p> </li> <li> clip             (<code>float, optional</code>)         \u2013 <p>Clip any value whose magnitude exceeds clipval (default: <code>None</code>).</p> </li> <li> pclip             (<code>float, optional</code>)         \u2013 <p>Clip any value greater than clipval (default: <code>None</code>).</p> </li> <li> nclip             (<code>float, optional</code>)         \u2013 <p>Clip any value less than clipval (default: <code>None</code>).</p> </li> <li> qclip             (<code>float, optional</code>)         \u2013 <p>Clip by quantile on absolute values on trace (default: <code>None</code>).</p> </li> <li> bias             (<code>float, optional</code>)         \u2013 <p>Bias data by adding an overall bias value (default: <code>None</code>).</p> </li> <li> scale             (<code>float, optional</code>)         \u2013 <p>Multiply data by overall scale factor (default: <code>1.0</code>).</p> </li> <li> norm             (<code>bool, optional</code>)         \u2013 <p>Divide data by overall scale factor (default: <code>False</code>).</p> </li> <li> norm_rms             (<code>bool, optional</code>)         \u2013 <p>Normalize using RMS amplitude (default: <code>False</code>).</p> </li> <li> copy             (<code>bool, optional</code>)         \u2013 <p>Copy input data (no change of input data) (default: <code>True</code>).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>Axis along which to gain (default: <code>0</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> data(            <code>np.ndarray</code> )        \u2013 <p>Input data with applied gain function(s) along <code>axis</code>.</p> </li> </ul>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.gain--notes","title":"Notes","text":"<p>By default, the input array will be copied (<code>copy=True</code>) to avoid updating of the input data in place.</p>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.gain--references","title":"References","text":"<ol> <li> <p><code>sugain</code> module help, http://sepwww.stanford.edu/oldsep/cliner/files/suhelp/sugain.txt \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def gain(\ndata,\ntwt,\ntpow=0.0,  # multiply data by t^tpow\nepow=0.0,  # multiply data by exp(epow*t)\netpow=1.0,  # multiply data by exp(epow*t^etpow)\nebase=None,  # use as base for exp function (default: e)\ngpow=0.0,  # take signed gpowth power of scaled data\nclip=None,  # clip any value whose magnitude exceeds clipval\npclip=None,  # clip any value greater than clipval\nnclip=None,  # clip any value less than clipval\nqclip=None,  # clip by quantile on absolute values on trace\nbias=None,  # bias data by adding an overall bias value\nscale=1.0,  # multiply data by overall scale factor\nnorm: bool = False,  # divide data by overall scale factor\nnorm_rms: bool = False,  # normalize using RMS amplitude\ncopy: bool = True,\naxis=0,\n):\n\"\"\"\n    Apply various different types of gain for either single trace (1D array) or seismic section (2D array).\n    !!! warning \"Copyright\"\n        This function is a Python implementation of the Seismic Unix `sugain` module.\n        Please refer to the license file `LICENSE_SeismicUnix`!\n    Parameters\n    ----------\n    data : np.ndarray\n        Seismic trace (nsamples,) or section (nsamples, ntraces).\n    twt : np.ndarray\n        Array of samples (in seconds TWT) appropriate spacing of sample rate (`dt`).\n    tpow : float, optional\n        Multiply data by t^tpow (default: `0.0`).\n    epow : float, optional\n        Multiply data by exp(epow*t) (default: `0.0`).\n    etpow : float, optional\n        Multiply data by exp(epow*t^etpow) (default: `1.0`).\n    ebase : float, optional\n        Base of exponential function (default: `e`).\n    gpow : float, optional\n        Take signed gpowth power of scaled data (default: `0.0`).\n    clip : float, optional\n        Clip any value whose magnitude exceeds clipval (default: `None`).\n    pclip : float, optional\n        Clip any value greater than clipval (default: `None`).\n    nclip : float, optional\n        Clip any value less than clipval (default: `None`).\n    qclip : float, optional\n        Clip by quantile on absolute values on trace (default: `None`).\n    bias : float, optional\n        Bias data by adding an overall bias value (default: `None`).\n    scale : float, optional\n        Multiply data by overall scale factor (default: `1.0`).\n    norm : bool, optional\n        Divide data by overall scale factor (default: `False`).\n    norm_rms : bool, optional\n        Normalize using RMS amplitude (default: `False`).\n    copy : bool, optional\n        Copy input data (no change of input data) (default: `True`).\n    axis : int, optional\n        Axis along which to gain (default: `0`).\n    Returns\n    -------\n    data : np.ndarray\n        Input data with applied gain function(s) along `axis`.\n    Notes\n    -----\n    By default, the input array will be copied (`copy=True`) to avoid updating of the input data in place.\n    References\n    ----------\n    [^1]: `sugain` module help, [http://sepwww.stanford.edu/oldsep/cliner/files/suhelp/sugain.txt](http://sepwww.stanford.edu/oldsep/cliner/files/suhelp/sugain.txt)\n    \"\"\"\nif copy:\ndata = data.copy()\nif data.ndim == 1:\nnsamples, ntraces, ndim = data.size, None, 1\nelif data.ndim == 2:\nif axis == 0:\nnsamples, ntraces = data.shape\nelse:\nntraces, nsamples = data.shape\nndim = 2\nelse:\nif axis == 0:\nnsamples, nil, nxl = data.shape\nelif axis == 2 or axis == -1:\nnil, nxl, nsamples = data.shape\nelse:\nraise ValueError('For 3D datasets the time axis must be either first or last.')\nndim = 2\nfor param, name in zip(\n[tpow, epow, etpow, gpow, clip, pclip, nclip, qclip, bias, scale],\n['tpow', 'epow', 'etpow', 'gpow', 'clip', 'pclip', 'nclip', 'qclip', 'bias', 'scale'],\n):\nif (param is not None) and not isinstance(param, (int, float)):\nraise ValueError(f'`{name}` must be either int or float')\n# bias\nif (bias is not None) and (bias != 0.0):\ndata += bias\n# tpow\nif (tpow is not None) and (tpow != 0.0):\ntpow_fact = np.power(twt, tpow)\ntpow_fact[0] = 0.0 if twt[0] == 0.0 else np.power(twt[0], tpow)\nif ndim == 1:\ndata *= tpow_fact\nelse:\ndata *= tpow_fact[:, None]\n# epow &amp; etpow (&amp; ebase)\nif epow is not None and epow != 0.0:\n# etpow\netpow_fact = np.power(twt, etpow)\n# epow\nif ebase is None:\nepow_fact = np.exp(epow * etpow_fact)\nelse:\nepow_fact = np.power(ebase, epow * etpow_fact)\nif ndim == 1:\ndata *= epow_fact\nelse:\ndata *= epow_fact[:, None]\n# gpow (take signed gpowth power of scaled data)\nif (gpow is not None) and (gpow != 0.0):\n# workaround to prevent numpy from complaining about negative numbers\ndata = np.sign(data) * np.abs(data) ** gpow\n# clip\nif clip is not None:\ndata = np.where(data &gt; clip, clip, data)\ndata = np.where(data &lt; -clip, -clip, data)\n# pclip\nif pclip is not None:\ndata = np.where(data &gt; pclip, pclip, data)\n# nclip\nif nclip is not None:\ndata = np.where(data &lt; nclip, nclip, data)\n# qclip\nif qclip is not None:\nqclip_per_trace = np.quantile(np.abs(data), q=qclip, axis=axis)\ndata = np.where(data &gt; qclip_per_trace, qclip_per_trace, data)\n# norm_rms\nif norm_rms:\ndata = rms_normalization(data, axis=axis)\n# scale\nif (scale is not None) and (scale != 1.0):\ndata = data * scale if not norm else data * 1 / scale\nreturn data\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.rms","title":"<code>rms(array, axis=None)</code>","text":"<p>Calculate the RMS amplitude(s) of a given array.</p> <p>Parameters:</p> <ul> <li> array             (<code>np.ndarray</code>)         \u2013 <p>Amplitude array.</p> </li> <li> axis             (<code>int, tuple, list(optional)</code>)         \u2013 <p>Axis for RMS amplitude calculation (default: <code>None</code>, i.e. single value for whole array).</p> </li> </ul> <p>Returns:</p> <ul> <li> rms(            <code>np.ndarray</code> )        \u2013 <p>Root mean square (RMS) amplitude(s).</p> </li> </ul> \\[ rms = \\sqrt{\\frac{\\sum{a^2}}{N}} \\] Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def rms(array, axis=None):\nr\"\"\"\n    Calculate the RMS amplitude(s) of a given array.\n    Parameters\n    ----------\n    array : np.ndarray\n        Amplitude array.\n    axis : int, tuple, list (optional)\n        Axis for RMS amplitude calculation (default: `None`, i.e. single value for whole array).\n    Returns\n    -------\n    rms : np.ndarray\n        Root mean square (RMS) amplitude(s).\n    $$\n    rms = \\sqrt{\\frac{\\sum{a^2}}{N}}\n    $$\n    \"\"\"\nif axis is None:\nN = array.size\nelif isinstance(axis, int):\nN = array.shape[axis]\nelif isinstance(axis, (tuple, list)):\nN = np.prod([array.shape[ax] for ax in axis])\nreturn np.sqrt(np.sum(array**2, axis=axis) / N)\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.rms_normalization","title":"<code>rms_normalization(signal, axis=None)</code>","text":"<p>Normalize signal using RMS amplitude of input array.</p> <p>Parameters:</p> <ul> <li> signal             (<code>np.ndarray</code>)         \u2013 <p>Input trace(s).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>Axis used for RMS amplitude calculation (default: <code>None</code>, i.e. whole array).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Normalized signal using RMS amplitude.</p> </li> </ul>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.rms_normalization--references","title":"References","text":"<ol> <li> <p>https://superkogito.github.io/blog/2020/04/30/rms_normalization.html \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def rms_normalization(signal, axis=None):\n\"\"\"\n    Normalize signal using RMS amplitude of input array.\n    Parameters\n    ----------\n    signal : np.ndarray\n        Input trace(s).\n    axis : int, optional\n        Axis used for RMS amplitude calculation (default: `None`, i.e. whole array).\n    Returns\n    -------\n    np.ndarray\n        Normalized signal using RMS amplitude.\n    References\n    ----------\n    [^1]: [https://superkogito.github.io/blog/2020/04/30/rms_normalization.html](https://superkogito.github.io/blog/2020/04/30/rms_normalization.html)\n    \"\"\"\nsignal = np.asarray(signal)\n_rms = rms(signal, axis=axis)\n_rms[_rms == 0.0] = 1.0\nreturn signal / _rms\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.balance_traces","title":"<code>balance_traces(traces, scale='rms', n_traces=None, axis_samples=None)</code>","text":"<p>Balance (i.e. scale) adjacent seismic traces. This function uses one of the following reference amplitude(s) per trace(s):</p> <ul> <li><code>rms</code> (default)</li> <li><code>peak</code> (absolute value)</li> <li><code>mean</code> (absolute value)</li> <li><code>median</code> (absolute value)</li> </ul> <p>The reference amplitude is computed</p> <ul> <li>for the whole dataset (<code>axis_samples = None</code>),</li> <li>for each individual trace (<code>axis_samples &gt;= 0</code>), or</li> <li>in moving windows of <code>n_traces</code> length.</li> </ul> <p>Parameters:</p> <ul> <li> traces             (<code>np.ndarray</code>)         \u2013 <p>Input traces, e.g. with shape: (nsamples x ntraces).</p> </li> <li> scale             (<code>str, optional</code>)         \u2013 <p>Amplitude scaling (balancing) mode (default: <code>rms</code>).</p> </li> <li> n_traces             (<code>int, optional</code>)         \u2013 <p>Number of traces used for windowed balacning (default: <code>None</code>).</p> </li> <li> axis_samples             (<code>int, optional</code>)         \u2013 <p>Axis for balancing computation (default: <code>0</code>, for nsamples x ntraces).</p> </li> </ul> <p>Returns:</p> <ul> <li> traces_eq(            <code>np.ndarray</code> )        \u2013 <p>Equalized (i.e. balanced) traces.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def balance_traces(\ntraces,\nscale: str = 'rms',\nn_traces: int = None,\naxis_samples: int = None,\n) -&gt; np.ndarray:\n\"\"\"\n    Balance (i.e. scale) adjacent seismic traces.\n    This function uses one of the following reference amplitude(s) per trace(s):\n      - `rms` (**default**)\n      - `peak` (absolute value)\n      - `mean` (absolute value)\n      - `median` (absolute value)\n    The reference amplitude is computed\n      - for the whole dataset (`axis_samples = None`),\n      - for each individual trace (`axis_samples &gt;= 0`), or\n      - in moving windows of `n_traces` length.\n    Parameters\n    ----------\n    traces : np.ndarray\n        Input traces, e.g. with shape: (nsamples x ntraces).\n    scale : str, optional\n        Amplitude scaling (balancing) mode (default: `rms`).\n    n_traces : int, optional\n        Number of traces used for windowed balacning (default: `None`).\n    axis_samples : int, optional\n        Axis for balancing computation (default: `0`, for nsamples x ntraces).\n    Returns\n    -------\n    traces_eq : np.ndarray\n        Equalized (i.e. balanced) traces.\n    \"\"\"\nif axis_samples is None:\naxis_samples = 0  # shape: (nsamples x ntraces)\nif scale.lower() not in ['rms', 'max', 'peak', 'mean', 'median']:\nraise ValueError(\n'Unknown equalizing method. Choose either \"rms\", \"peak\", \"mean\", or \"median\".'\n)\nelse:\nscale = scale.lower()\ntraces = np.asanyarray(traces)\n# (1) trace-by-trace balancing\nif n_traces is None or n_traces == 1:\nif scale == 'rms':\namp_ref = rms(traces, axis=axis_samples)\nelif scale in ['peak', 'max']:\namp_ref = np.max(np.abs(traces), axis=axis_samples)\nelif scale == 'mean':\namp_ref = np.mean(np.abs(traces), axis=axis_samples)\nelif scale == 'median':\namp_ref = np.median(np.abs(traces), axis=axis_samples)\n# (2) windowed trace balancing\nelif n_traces &gt; 1:\nif traces.ndim != 2:\nraise ValueError('Input array must be 2D array!')\nn_traces = n_traces + 1 if n_traces % 2 == 0 else n_traces\nif axis_samples == 0:  # (nsamples x ntraces)\nwin = (traces.shape[axis_samples], n_traces)\naxis = 1\nelif axis_samples == 1:  # (ntraces, nsamples)\nwin = (n_traces, traces.shape[axis_samples])\naxis = 0\nelse:\nraise ValueError('False value for ``axis_samples`` (either 0 or 1)!')\nnpad = (n_traces - 1) // 2\ntraces_pad = pad_along_axis(traces, n=npad, axis=axis)\ntraces_win = moving_window_2D(traces_pad, w=win, dx=1, dy=1).squeeze()\naxis = (-2, -1)\nif scale == 'rms':\namp_ref = rms(traces_win, axis=axis)\nelif scale in ['peak', 'max']:\namp_ref = np.max(np.abs(traces_win), axis=axis)\nelif scale == 'mean':\namp_ref = np.mean(np.abs(traces_win), axis=axis)\nelif scale == 'median':\namp_ref = np.median(np.abs(traces_win), axis=axis)\nif traces.ndim == 1:\namp_ref = 1.0 if amp_ref == 0 else amp_ref\nelse:\namp_ref[amp_ref == 0.0] = 1.0\ntraces_balanced = traces / amp_ref\n# traces_balanced = rescale(traces, vmin=-amp_ref, vmax=amp_ref)\nassert traces.shape == traces_balanced.shape, 'Something went wrong here...'\nreturn traces_balanced\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.calc_reference_amplitude","title":"<code>calc_reference_amplitude(traces, axis=None, scale='rms')</code>","text":"<p>Calculate reference amplitude per trace using user-defined scaling (<code>rms</code> or <code>max</code>).</p> <p>Parameters:</p> <ul> <li> traces             (<code>np.ndarray</code>)         \u2013 <p>Input traces.</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>Axis along that reference amplitudes will be calculated (default: <code>None</code>).</p> </li> <li> scale             (<code>str, optional</code>)         \u2013 <p>Scale using either <code>rms</code> (default) or <code>max</code> amplitudes.</p> </li> </ul> <p>Returns:</p> <ul> <li> amp_ref(            <code>np.ndarray</code> )        \u2013 <p>Reference amplitude array.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def calc_reference_amplitude(traces, axis: int = None, scale: str = 'rms'):\n\"\"\"\n    Calculate reference amplitude per trace using user-defined scaling (`rms` or `max`).\n    Parameters\n    ----------\n    traces : np.ndarray\n        Input traces.\n    axis : int, optional\n        Axis along that reference amplitudes will be calculated (default: `None`).\n    scale : str, optional\n        Scale using either `rms` (default) or `max` amplitudes.\n    Returns\n    -------\n    amp_ref : np.ndarray\n        Reference amplitude array.\n    \"\"\"\nif scale == 'rms':\namp_ref = rms(traces, axis=axis)\nelif scale in ['peak', 'max']:\namp_ref = np.max(np.abs(traces), axis=axis)\n# amp_ref[amp_ref == 0.0] = 1.0\namp_ref = np.where(amp_ref == 0.0, 1.0, amp_ref)\nreturn amp_ref\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.envelope","title":"<code>envelope(signal, axis=-1)</code>","text":"<p>Compute envelope of a seismic trace (1D), section (2D) or cube (3D) using the Hilbert transform.</p> <p>Parameters:</p> <ul> <li> signal             (<code>np.ndarray</code>)         \u2013 <p>Seismic trace (1D) or section (2D).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>Axis along which to do the transformation (default: <code>-1</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Amplitude envelope of input array along <code>axis</code>.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def envelope(signal, axis=-1):\n\"\"\"\n    Compute envelope of a seismic trace (1D), section (2D) or cube (3D) using the Hilbert transform.\n    Parameters\n    ----------\n    signal : np.ndarray\n        Seismic trace (1D) or section (2D).\n    axis : int, optional\n        Axis along which to do the transformation (default: `-1`).\n    Returns\n    -------\n    np.ndarray\n        Amplitude envelope of input array along `axis`.\n    \"\"\"\nsignal_analytic = hilbert(signal, axis=axis)\nreturn np.abs(signal_analytic).astype(signal.dtype)\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.get_resampled_twt","title":"<code>get_resampled_twt(twt, n_resamples, n_samples)</code>","text":"<p>Return resampled TWT array.</p> <p>Parameters:</p> <ul> <li> twt             (<code>np.ndarray</code>)         \u2013 <p>Orignial TWT array.</p> </li> <li> n_resamples             (<code>int</code>)         \u2013 <p>Number of resampled trace samples.</p> </li> <li> n_samples             (<code>int</code>)         \u2013 <p>Number of original trace samples.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Resampled twt.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def get_resampled_twt(twt, n_resamples, n_samples):\n\"\"\"\n    Return resampled TWT array.\n    Parameters\n    ----------\n    twt : np.ndarray\n        Orignial TWT array.\n    n_resamples : int\n        Number of resampled trace samples.\n    n_samples : int\n        Number of original trace samples.\n    Returns\n    -------\n    np.ndarray\n        Resampled twt.\n    \"\"\"\nreturn np.arange(0, n_resamples) * (twt[1] - twt[0]) * n_samples / float(n_resamples) + twt[0]\n</code></pre>"},{"location":"api/functions/api_signal/#pseudo_3D_interpolation.functions.signal.freq_spectrum","title":"<code>freq_spectrum(signal, Fs, n=None, taper=True, return_minmax=False)</code>","text":"<p>Compute frequency spectrum of input signal given a sampling rate (<code>Fs</code>).</p> <p>Parameters:</p> <ul> <li> signal             (<code>np.ndarray</code>)         \u2013 <p>1D signal array.</p> </li> <li> Fs             (<code>int</code>)         \u2013 <p>Sampling rate/frequency (Hz).</p> </li> <li> n             (<code>int, optional</code>)         \u2013 <p>Length of FFT, i.e. number of points (default: len(signal)).</p> </li> <li> taper             (<code>TYPE, optional</code>)         \u2013 <p>Window function applied to time signal to improve frequency domain properties (default: <code>True</code>)</p> </li> </ul> <p>Returns:</p> <ul> <li> f(            <code>np.ndarray</code> )        \u2013 <p>Array of signal frequencies.</p> </li> <li> a_norm(            <code>np.ndarray</code> )        \u2013 <p>Magnitude of amplitudes per frequency.</p> </li> <li> f_min(            <code>float</code> )        \u2013 <p>Minimum frequency with actual signal content.</p> </li> <li> f_max(            <code>float</code> )        \u2013 <p>Maximum frequency with actual signal content.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\signal.py</code> <pre><code>def freq_spectrum(signal, Fs, n=None, taper=True, return_minmax=False):\n\"\"\"\n    Compute frequency spectrum of input signal given a sampling rate (`Fs`).\n    Parameters\n    ----------\n    signal : np.ndarray\n        1D signal array.\n    Fs : int\n        Sampling rate/frequency (Hz).\n    n : int, optional\n        Length of FFT, i.e. number of points (default: len(signal)).\n    taper : TYPE, optional\n        Window function applied to time signal to improve frequency domain properties (default: `True`)\n    Returns\n    -------\n    f : np.ndarray\n        Array of signal frequencies.\n    a_norm : np.ndarray\n        Magnitude of amplitudes per frequency.\n    f_min : float\n        Minimum frequency with actual signal content.\n    f_max : float\n        Maximum frequency with actual signal content.\n    \"\"\"\n# signal length (samples)\nN = len(signal)\n# select window function\nif taper:\nwin = np.blackman(N)\nelse:\nwin = np.ones((N))\n# apply tapering\ns = signal * win\n# number of points to use for FFT\nif n is None:\nn = N\n# calc real part of FFT\na = np.abs(np.fft.rfft(s, n))\n# calc frequency array\nf = np.fft.rfftfreq(n, 1 / Fs)\n# scale magnitude of FFT by used window and factor of 2 (only half-spectrum)\na_norm = a * 2 / np.sum(win)\nif return_minmax:\n# get frequency limits using calculated amplitude threshold\nslope = np.abs(np.diff(a_norm) / np.diff(f))  # calculate slope\nthreshold = (slope.max() - slope.min()) * 0.001  # threshold amplitude\nf_limits = np.where(a_norm &gt; threshold)[0]  # get frequency limits\nf_min, f_max = f[f_limits[0]], f[f_limits[-1]]  # select min/max frequencies\nf_min, f_max = np.min(f_limits), np.max(f_limits)  # select min/max frequencies\nreturn f, a_norm, f_min, f_max\nelse:\nreturn f, a_norm\n</code></pre>"},{"location":"api/functions/api_threshold_operator/","title":"<code>threshold_operator.py</code>","text":"<p>Thresholding operators for POCS algorithm. These functions are only used when <code>pywavelets</code> package is not installed (fallback option).</p>"},{"location":"api/functions/api_threshold_operator/#pseudo_3D_interpolation.functions.threshold_operator._soft_threshold","title":"<code>_soft_threshold(data, value, substitute=0)</code>","text":"<p>Soft thresholding (from <code>pywavelet</code>).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input coefficients.</p> </li> <li> value             (<code>float</code>)         \u2013 <p>Threshold value.</p> </li> <li> substitute             (<code>float, optional</code>)         \u2013 <p>Value to insert for values below threshold (default: <code>0</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Coefficients with applied threshold.</p> </li> </ul>"},{"location":"api/functions/api_threshold_operator/#pseudo_3D_interpolation.functions.threshold_operator._soft_threshold--references","title":"References","text":"<ol> <li> <p>PyWavelets, https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\threshold_operator.py</code> <pre><code>def _soft_threshold(data, value, substitute=0):\n\"\"\"\n    Soft thresholding (from `pywavelet`).\n    Parameters\n    ----------\n    data : np.ndarray\n        Input coefficients.\n    value : float\n        Threshold value.\n    substitute : float, optional\n        Value to insert for values below threshold (default: `0`).\n    Returns\n    -------\n    np.ndarray\n        Coefficients with applied threshold.\n    References\n    ----------\n    [^1]: PyWavelets, [https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py](https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py)\n    \"\"\"\ndata = np.asarray(data)\nmagnitude = np.absolute(data)\nwith np.errstate(divide='ignore'):\n# divide by zero okay as np.inf values get clipped, so ignore warning.\nthresholded = 1 - value / magnitude\nthresholded.clip(min=0, max=None, out=thresholded)\nthresholded = data * thresholded\nif substitute == 0:\nreturn thresholded\nelse:\ncond = np.less(magnitude, value)\nreturn np.where(cond, substitute, thresholded)\n</code></pre>"},{"location":"api/functions/api_threshold_operator/#pseudo_3D_interpolation.functions.threshold_operator._nn_garrote","title":"<code>_nn_garrote(data, value, substitute=0)</code>","text":"<p>Non-negative Garrote thresholding (from <code>pywavelet</code>).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input coefficients.</p> </li> <li> value             (<code>float</code>)         \u2013 <p>Threshold value.</p> </li> <li> substitute             (<code>float, optional</code>)         \u2013 <p>Value to insert for values below threshold (default: <code>0</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Coefficients with applied threshold.</p> </li> </ul>"},{"location":"api/functions/api_threshold_operator/#pseudo_3D_interpolation.functions.threshold_operator._nn_garrote--references","title":"References","text":"<ol> <li> <p>PyWavelets, https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\threshold_operator.py</code> <pre><code>def _nn_garrote(data, value, substitute=0):\n\"\"\"\n    Non-negative Garrote thresholding (from `pywavelet`).\n    Parameters\n    ----------\n    data : np.ndarray\n        Input coefficients.\n    value : float\n        Threshold value.\n    substitute : float, optional\n        Value to insert for values below threshold (default: `0`).\n    Returns\n    -------\n    np.ndarray\n        Coefficients with applied threshold.\n    References\n    ----------\n    [^1]: PyWavelets, [https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py](https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py)\n    \"\"\"\ndata = np.asarray(data)\nmagnitude = np.absolute(data)\nwith np.errstate(divide='ignore'):\n# divide by zero okay as np.inf values get clipped, so ignore warning.\nthresholded = 1 - value**2 / magnitude**2\nthresholded.clip(min=0, max=None, out=thresholded)\nthresholded = data * thresholded\nif substitute == 0:\nreturn thresholded\nelse:\ncond = np.less(magnitude, value)\nreturn np.where(cond, substitute, thresholded)\n</code></pre>"},{"location":"api/functions/api_threshold_operator/#pseudo_3D_interpolation.functions.threshold_operator._hard_threshold","title":"<code>_hard_threshold(data, value, substitute=0)</code>","text":"<p>Hard thresholding (from <code>pywavelet</code>).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Input coefficients.</p> </li> <li> value             (<code>float</code>)         \u2013 <p>Threshold value.</p> </li> <li> substitute             (<code>float, optional</code>)         \u2013 <p>Value to insert for values below threshold (default: <code>0</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Coefficients with applied threshold.</p> </li> </ul>"},{"location":"api/functions/api_threshold_operator/#pseudo_3D_interpolation.functions.threshold_operator._hard_threshold--references","title":"References","text":"<ol> <li> <p>PyWavelets, https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\threshold_operator.py</code> <pre><code>def _hard_threshold(data, value, substitute=0):\n\"\"\"\n    Hard thresholding (from `pywavelet`).\n    Parameters\n    ----------\n    data : np.ndarray\n        Input coefficients.\n    value : float\n        Threshold value.\n    substitute : float, optional\n        Value to insert for values below threshold (default: `0`).\n    Returns\n    -------\n    np.ndarray\n        Coefficients with applied threshold.\n    References\n    ----------\n    [^1]: PyWavelets, [https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py](https://github.com/PyWavelets/pywt/blob/master/pywt/_thresholding.py)\n    \"\"\"\ndata = np.asarray(data)\ncond = np.less(np.absolute(data), value)\nreturn np.where(cond, substitute, data)\n</code></pre>"},{"location":"api/functions/api_transform/","title":"<code>transform.py</code>","text":"<p>Custom Affine transformation class.</p>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine","title":"<code>Affine</code>","text":"<p>Affine transformation.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>class Affine:\n\"\"\"Affine transformation.\"\"\"\ndef __init__(self, scaling=1, translation=0, rotation=0, shear=0, matrix=None):\n\"\"\"\n        Initialize Affine transform.\n        Using either user-specified parameters or `numpy.ndarray` of shape (3, 3)\n        holding affine transform (`matrix` parameter).\n        References\n        ----------\n        [^1]: [https://stackoverflow.com/a/53691628](https://stackoverflow.com/a/53691628)\n        \"\"\"\nif np.isscalar(scaling):\nsx = sy = scaling\nelse:\nsx, sy = scaling\nif np.isscalar(translation):\ntx = ty = translation\nelse:\ntx, ty = translation\nrotation = np.deg2rad(rotation)\nif np.isscalar(shear):\ncx = cy = shear\nelse:\ncx, cy = shear\ncx, cy = np.deg2rad(cx), np.deg2rad(cy)\nif matrix is None:\nself.matrix = np.array(\n[\n[sx * np.cos(rotation), -np.sin(rotation) + cx, tx],\n[np.sin(rotation) + cy, sy * np.cos(rotation), ty],\n[0, 0, 1],\n]\n)\nelse:\nif isinstance(matrix, np.ndarray) and matrix.shape == (3, 3):\nself.matrix = matrix\nelse:\nraise AttributeError('Matrix must be numpy array of shape (3,3)!')\ndef __repr__(self):  # noqa\nreturn f'Affine({self.matrix})'\n@staticmethod\ndef get_scalar(param):\n\"\"\"Return (unpacked) scalars from input tuple or duplicated input scalar.\"\"\"\nif np.isscalar(param):\npx = py = param\nelse:\npx, py = param\nreturn px, py\n@staticmethod\ndef fix_floating_point_error(value):\n\"\"\"Fix floating point error.\"\"\"\ndef _func(x):\nreturn float(f'{x:g}')\nfunc = np.vectorize(_func)\nreturn func(value)\ndef _transform(self, points):\n\"\"\"Return input points (2D array) and corresponding transformation matrix.\"\"\"\np = np.atleast_2d(points)\nnrows, ncols = p.shape\nif ncols == 2:\np = np.hstack((p, np.ones((nrows, 1), dtype=p.dtype)))\nreturn p, self.matrix\n@classmethod\ndef identity(cls):\n\"\"\"Return identity matrix (transform to original image).\"\"\"\nreturn cls()\ndef scaling(self, scale, overwrite=False):\n\"\"\"Return `self` with updated matrix using given scaling.\"\"\"\nax = (0, 1)\nsx, sy = self.get_scalar(scale)\nif overwrite:\nself.matrix[ax, ax] = sx, sy\nelse:\nself.matrix = Affine(scaling=(sx, sy)).matrix @ self.matrix\nreturn self\ndef translation(self, offset, overwrite=False):\n\"\"\"Return `self` with updated matrix using given translation.\"\"\"\ntx, ty = self.get_scalar(offset)\nif overwrite:\nself.matrix[2, :2] = tx, ty\nelse:\n# self.matrix = self.matrix @ Affine(translation=(tx, ty)).matrix\nself.matrix = Affine(translation=(tx, ty)).matrix @ self.matrix\nreturn self\ndef rotation(self, angle: float, overwrite=False):\n\"\"\"Return `self` with updated matrix using given rotation.\"\"\"\nangle = np.deg2rad(angle)\n_matrix = np.array(\n[[np.cos(angle), -np.sin(angle), 0], [np.sin(angle), np.cos(angle), 0], [0, 0, 1]]\n)\nif overwrite:\nself.matrix[:2, :2] = _matrix[:2, :2]\nelse:\n# correct order when using chained `.` operators\nself.matrix = _matrix @ self.matrix\nreturn self\ndef rotate_around(self, angle: float, origin: tuple = (0, 0)):\n\"\"\"Return `self` rotated around provided point (default: `(0, 0)`).\"\"\"\nself.translation(tuple(-np.asarray(origin)))\nif angle is not None:\nself.rotation(angle)\nself.translation(origin)\nreturn self\ndef rotate(self, points, angle: float = None, origin: tuple = (0, 0), fix_float: bool = False):\n\"\"\"\n        Return rotated input points (N, 2) or (N, 3).\n        Positive angles indicate a counter-clockwise roation,\n        negative angles a clockwiseroation.\n        Parameters\n        ----------\n        points : np.ndarray\n            Input point coordinates with shape (N, 2) for **2D** or (N, 3) for **3D**.\n        angle : float, optional\n            Rotation angle (will overwrite previously set rotation) (default: `None`).\n        origin : tuple, optional\n            Rotation around given point (default: `(0, 0)`).\n        fix_float : bool, optional\n            Fix floating point precision error (default: `False`).\n        Returns\n        -------\n        np.ndarray\n            Rotated input points.\n        \"\"\"\npoints = np.asarray(points)\nnpts, ndim = points.shape\no = np.atleast_2d(origin)\nif o.shape[1] == 2:\no = np.hstack((o, np.array([[1]])))\nself.translation(tuple(-np.asarray(origin)))\nif angle is not None:\nself.rotation(angle)\nself.translation(origin)\np, A = self._transform(points)\nt = (p @ A.T)[:, :ndim]  # ((p-o) @ A.T + o)[:,:ndim]\nif fix_float:\nreturn self.fix_floating_point_error(t)\nreturn t\ndef skew(self, shear, overwrite=False):\n\"\"\"Return `self` with updated matrix using given shear angle (deg).\"\"\"\ncx, cy = self.get_scalar(shear)\nif overwrite:\nself.matrix[(0, 1), (1, 0)] = np.tan(cx), np.tan(cy)\nelse:\nself.matrix = Affine(shear=(cx, cy)).matrix @ self.matrix\nreturn self\ndef transform(self, points, fix_float: bool = False):\n\"\"\"\n        Return transformed input points (based on parameters set on initiation).\n        Parameters\n        ----------\n        points : np.ndarray\n            2D array of coordinates with shape `(npts, 2)`.\n        fix_float : bool, optional\n            Fix floating point precision error (default: `False`).\n        Returns\n        -------\n        np.ndarray\n            Transformed input points.\n        \"\"\"\npoints = np.atleast_2d(np.asarray(points))\nnpts, ndim = points.shape\np, A = self._transform(points)\nt = (p @ A.T)[:, :ndim]\nif fix_float:\nreturn self.fix_floating_point_error(t)\nreturn t\ndef __matmul__(self, other):\n\"\"\"Matrix multiplication using @ operator (**order-dependent**!).\"\"\"\nif isinstance(other, Affine):\n_matrix = self.matrix @ other.matrix\nelif isinstance(other, np.ndarray):\n_matrix = self.matrix @ other\nelse:\nraise NotImplementedError(\n'Other must be either Affine() or numpy.ndarry of shape (3,3)'\n)\nreturn Affine(matrix=_matrix)\ndef __mul__(self, other):\n\"\"\"Matrix multiplication (**order-dependent**!).\"\"\"\nif isinstance(other, Affine):\n_matrix = self.matrix @ other.matrix\nelif isinstance(other, np.ndarray):\n_matrix = self.matrix @ other\nelse:\nraise NotImplementedError(\n'Other must be either Affine() or numpy.ndarry of shape (3,3)'\n)\nreturn Affine(matrix=_matrix)\ndef __add__(self, other):  # noqa\n\"\"\"\n        Combine Affine transformations so that `C = A + B` equals\n        `C.transform(x) = B.transform(A.transform(x))`.\n        \"\"\"\nif isinstance(other, Affine):\n_matrix = self.matrix @ other.matrix\nelif isinstance(other, np.ndarray):\n_matrix = self.matrix @ other\nelse:\nraise NotImplementedError(\n'Other must be either Affine() or numpy.ndarry of shape (3,3)'\n)\nreturn Affine(matrix=_matrix)\ndef inverse(self, inplace: bool = False):\n\"\"\"\n        Apply inverse transform.\n        Parameters\n        ----------\n        inplace : bool, optional\n            Assigns to `self.matrix` if True (default: `False`).\n        Returns\n        -------\n        Affine\n            Inverse Affine matrix.\n        \"\"\"\nm = self.matrix\n_inv = np.linalg.inv(m[:2, :2])  # only upper left (2,2) sub-matrix\n_t = np.array(\n[\n[-m[0, 2] * _inv[0, 0] - m[1, 2] * _inv[0, 1]],\n[-m[0, 2] * _inv[1, 0] - m[1, 2] * _inv[1, 1]],\n]\n)\n# _t = np.atleast_2d(1 / (np.flip(m[:2,2]) * -1)).T\n_matrix = np.vstack((np.hstack((_inv, _t)), np.array([0, 0, 1])))\nif inplace:\nself.matrix = _matrix\nreturn self\nelse:\nreturn Affine(matrix=_matrix)\ndef copy(self):\n\"\"\"Return copy of `self`.\"\"\"\nreturn Affine(matrix=self.matrix)\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.__init__","title":"<code>__init__(scaling=1, translation=0, rotation=0, shear=0, matrix=None)</code>","text":"<p>Initialize Affine transform. Using either user-specified parameters or <code>numpy.ndarray</code> of shape (3, 3) holding affine transform (<code>matrix</code> parameter).</p>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.__init__--references","title":"References","text":"<ol> <li> <p>https://stackoverflow.com/a/53691628 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def __init__(self, scaling=1, translation=0, rotation=0, shear=0, matrix=None):\n\"\"\"\n    Initialize Affine transform.\n    Using either user-specified parameters or `numpy.ndarray` of shape (3, 3)\n    holding affine transform (`matrix` parameter).\n    References\n    ----------\n    [^1]: [https://stackoverflow.com/a/53691628](https://stackoverflow.com/a/53691628)\n    \"\"\"\nif np.isscalar(scaling):\nsx = sy = scaling\nelse:\nsx, sy = scaling\nif np.isscalar(translation):\ntx = ty = translation\nelse:\ntx, ty = translation\nrotation = np.deg2rad(rotation)\nif np.isscalar(shear):\ncx = cy = shear\nelse:\ncx, cy = shear\ncx, cy = np.deg2rad(cx), np.deg2rad(cy)\nif matrix is None:\nself.matrix = np.array(\n[\n[sx * np.cos(rotation), -np.sin(rotation) + cx, tx],\n[np.sin(rotation) + cy, sy * np.cos(rotation), ty],\n[0, 0, 1],\n]\n)\nelse:\nif isinstance(matrix, np.ndarray) and matrix.shape == (3, 3):\nself.matrix = matrix\nelse:\nraise AttributeError('Matrix must be numpy array of shape (3,3)!')\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.get_scalar","title":"<code>get_scalar(param)</code>  <code>staticmethod</code>","text":"<p>Return (unpacked) scalars from input tuple or duplicated input scalar.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>@staticmethod\ndef get_scalar(param):\n\"\"\"Return (unpacked) scalars from input tuple or duplicated input scalar.\"\"\"\nif np.isscalar(param):\npx = py = param\nelse:\npx, py = param\nreturn px, py\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.fix_floating_point_error","title":"<code>fix_floating_point_error(value)</code>  <code>staticmethod</code>","text":"<p>Fix floating point error.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>@staticmethod\ndef fix_floating_point_error(value):\n\"\"\"Fix floating point error.\"\"\"\ndef _func(x):\nreturn float(f'{x:g}')\nfunc = np.vectorize(_func)\nreturn func(value)\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.identity","title":"<code>identity()</code>  <code>classmethod</code>","text":"<p>Return identity matrix (transform to original image).</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>@classmethod\ndef identity(cls):\n\"\"\"Return identity matrix (transform to original image).\"\"\"\nreturn cls()\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.scaling","title":"<code>scaling(scale, overwrite=False)</code>","text":"<p>Return <code>self</code> with updated matrix using given scaling.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def scaling(self, scale, overwrite=False):\n\"\"\"Return `self` with updated matrix using given scaling.\"\"\"\nax = (0, 1)\nsx, sy = self.get_scalar(scale)\nif overwrite:\nself.matrix[ax, ax] = sx, sy\nelse:\nself.matrix = Affine(scaling=(sx, sy)).matrix @ self.matrix\nreturn self\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.translation","title":"<code>translation(offset, overwrite=False)</code>","text":"<p>Return <code>self</code> with updated matrix using given translation.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def translation(self, offset, overwrite=False):\n\"\"\"Return `self` with updated matrix using given translation.\"\"\"\ntx, ty = self.get_scalar(offset)\nif overwrite:\nself.matrix[2, :2] = tx, ty\nelse:\n# self.matrix = self.matrix @ Affine(translation=(tx, ty)).matrix\nself.matrix = Affine(translation=(tx, ty)).matrix @ self.matrix\nreturn self\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.rotation","title":"<code>rotation(angle, overwrite=False)</code>","text":"<p>Return <code>self</code> with updated matrix using given rotation.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def rotation(self, angle: float, overwrite=False):\n\"\"\"Return `self` with updated matrix using given rotation.\"\"\"\nangle = np.deg2rad(angle)\n_matrix = np.array(\n[[np.cos(angle), -np.sin(angle), 0], [np.sin(angle), np.cos(angle), 0], [0, 0, 1]]\n)\nif overwrite:\nself.matrix[:2, :2] = _matrix[:2, :2]\nelse:\n# correct order when using chained `.` operators\nself.matrix = _matrix @ self.matrix\nreturn self\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.rotate_around","title":"<code>rotate_around(angle, origin=(0, 0))</code>","text":"<p>Return <code>self</code> rotated around provided point (default: <code>(0, 0)</code>).</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def rotate_around(self, angle: float, origin: tuple = (0, 0)):\n\"\"\"Return `self` rotated around provided point (default: `(0, 0)`).\"\"\"\nself.translation(tuple(-np.asarray(origin)))\nif angle is not None:\nself.rotation(angle)\nself.translation(origin)\nreturn self\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.rotate","title":"<code>rotate(points, angle=None, origin=(0, 0), fix_float=False)</code>","text":"<p>Return rotated input points (N, 2) or (N, 3). Positive angles indicate a counter-clockwise roation, negative angles a clockwiseroation.</p> <p>Parameters:</p> <ul> <li> points             (<code>np.ndarray</code>)         \u2013 <p>Input point coordinates with shape (N, 2) for 2D or (N, 3) for 3D.</p> </li> <li> angle             (<code>float, optional</code>)         \u2013 <p>Rotation angle (will overwrite previously set rotation) (default: <code>None</code>).</p> </li> <li> origin             (<code>tuple, optional</code>)         \u2013 <p>Rotation around given point (default: <code>(0, 0)</code>).</p> </li> <li> fix_float             (<code>bool, optional</code>)         \u2013 <p>Fix floating point precision error (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Rotated input points.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def rotate(self, points, angle: float = None, origin: tuple = (0, 0), fix_float: bool = False):\n\"\"\"\n    Return rotated input points (N, 2) or (N, 3).\n    Positive angles indicate a counter-clockwise roation,\n    negative angles a clockwiseroation.\n    Parameters\n    ----------\n    points : np.ndarray\n        Input point coordinates with shape (N, 2) for **2D** or (N, 3) for **3D**.\n    angle : float, optional\n        Rotation angle (will overwrite previously set rotation) (default: `None`).\n    origin : tuple, optional\n        Rotation around given point (default: `(0, 0)`).\n    fix_float : bool, optional\n        Fix floating point precision error (default: `False`).\n    Returns\n    -------\n    np.ndarray\n        Rotated input points.\n    \"\"\"\npoints = np.asarray(points)\nnpts, ndim = points.shape\no = np.atleast_2d(origin)\nif o.shape[1] == 2:\no = np.hstack((o, np.array([[1]])))\nself.translation(tuple(-np.asarray(origin)))\nif angle is not None:\nself.rotation(angle)\nself.translation(origin)\np, A = self._transform(points)\nt = (p @ A.T)[:, :ndim]  # ((p-o) @ A.T + o)[:,:ndim]\nif fix_float:\nreturn self.fix_floating_point_error(t)\nreturn t\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.skew","title":"<code>skew(shear, overwrite=False)</code>","text":"<p>Return <code>self</code> with updated matrix using given shear angle (deg).</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def skew(self, shear, overwrite=False):\n\"\"\"Return `self` with updated matrix using given shear angle (deg).\"\"\"\ncx, cy = self.get_scalar(shear)\nif overwrite:\nself.matrix[(0, 1), (1, 0)] = np.tan(cx), np.tan(cy)\nelse:\nself.matrix = Affine(shear=(cx, cy)).matrix @ self.matrix\nreturn self\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.transform","title":"<code>transform(points, fix_float=False)</code>","text":"<p>Return transformed input points (based on parameters set on initiation).</p> <p>Parameters:</p> <ul> <li> points             (<code>np.ndarray</code>)         \u2013 <p>2D array of coordinates with shape <code>(npts, 2)</code>.</p> </li> <li> fix_float             (<code>bool, optional</code>)         \u2013 <p>Fix floating point precision error (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Transformed input points.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def transform(self, points, fix_float: bool = False):\n\"\"\"\n    Return transformed input points (based on parameters set on initiation).\n    Parameters\n    ----------\n    points : np.ndarray\n        2D array of coordinates with shape `(npts, 2)`.\n    fix_float : bool, optional\n        Fix floating point precision error (default: `False`).\n    Returns\n    -------\n    np.ndarray\n        Transformed input points.\n    \"\"\"\npoints = np.atleast_2d(np.asarray(points))\nnpts, ndim = points.shape\np, A = self._transform(points)\nt = (p @ A.T)[:, :ndim]\nif fix_float:\nreturn self.fix_floating_point_error(t)\nreturn t\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.__matmul__","title":"<code>__matmul__(other)</code>","text":"<p>Matrix multiplication using @ operator (order-dependent!).</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def __matmul__(self, other):\n\"\"\"Matrix multiplication using @ operator (**order-dependent**!).\"\"\"\nif isinstance(other, Affine):\n_matrix = self.matrix @ other.matrix\nelif isinstance(other, np.ndarray):\n_matrix = self.matrix @ other\nelse:\nraise NotImplementedError(\n'Other must be either Affine() or numpy.ndarry of shape (3,3)'\n)\nreturn Affine(matrix=_matrix)\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Matrix multiplication (order-dependent!).</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def __mul__(self, other):\n\"\"\"Matrix multiplication (**order-dependent**!).\"\"\"\nif isinstance(other, Affine):\n_matrix = self.matrix @ other.matrix\nelif isinstance(other, np.ndarray):\n_matrix = self.matrix @ other\nelse:\nraise NotImplementedError(\n'Other must be either Affine() or numpy.ndarry of shape (3,3)'\n)\nreturn Affine(matrix=_matrix)\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.__add__","title":"<code>__add__(other)</code>","text":"<p>Combine Affine transformations so that <code>C = A + B</code> equals <code>C.transform(x) = B.transform(A.transform(x))</code>.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def __add__(self, other):  # noqa\n\"\"\"\n    Combine Affine transformations so that `C = A + B` equals\n    `C.transform(x) = B.transform(A.transform(x))`.\n    \"\"\"\nif isinstance(other, Affine):\n_matrix = self.matrix @ other.matrix\nelif isinstance(other, np.ndarray):\n_matrix = self.matrix @ other\nelse:\nraise NotImplementedError(\n'Other must be either Affine() or numpy.ndarry of shape (3,3)'\n)\nreturn Affine(matrix=_matrix)\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.inverse","title":"<code>inverse(inplace=False)</code>","text":"<p>Apply inverse transform.</p> <p>Parameters:</p> <ul> <li> inplace             (<code>bool, optional</code>)         \u2013 <p>Assigns to <code>self.matrix</code> if True (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Affine</code>         \u2013 <p>Inverse Affine matrix.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def inverse(self, inplace: bool = False):\n\"\"\"\n    Apply inverse transform.\n    Parameters\n    ----------\n    inplace : bool, optional\n        Assigns to `self.matrix` if True (default: `False`).\n    Returns\n    -------\n    Affine\n        Inverse Affine matrix.\n    \"\"\"\nm = self.matrix\n_inv = np.linalg.inv(m[:2, :2])  # only upper left (2,2) sub-matrix\n_t = np.array(\n[\n[-m[0, 2] * _inv[0, 0] - m[1, 2] * _inv[0, 1]],\n[-m[0, 2] * _inv[1, 0] - m[1, 2] * _inv[1, 1]],\n]\n)\n# _t = np.atleast_2d(1 / (np.flip(m[:2,2]) * -1)).T\n_matrix = np.vstack((np.hstack((_inv, _t)), np.array([0, 0, 1])))\nif inplace:\nself.matrix = _matrix\nreturn self\nelse:\nreturn Affine(matrix=_matrix)\n</code></pre>"},{"location":"api/functions/api_transform/#pseudo_3D_interpolation.functions.transform.Affine.copy","title":"<code>copy()</code>","text":"<p>Return copy of <code>self</code>.</p> Source code in <code>pseudo_3D_interpolation\\functions\\transform.py</code> <pre><code>def copy(self):\n\"\"\"Return copy of `self`.\"\"\"\nreturn Affine(matrix=self.matrix)\n</code></pre>"},{"location":"api/functions/api_utils/","title":"<code>utils.py</code>","text":"<p>Miscellaneous utility functions.</p>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.log_message","title":"<code>log_message(msg_lvl, verbosity, *msg_args)</code>","text":"<p>Print log messages depending on level of verbosity.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def log_message(msg_lvl: int, verbosity: int, *msg_args) -&gt; None:\n\"\"\"Print log messages depending on level of verbosity.\"\"\"\nif msg_lvl &lt;= verbosity:\nprint(*msg_args)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.xprint","title":"<code>xprint(*args, kind='info', verbosity=0, **kwargs)</code>","text":"<p>Thin wrapper function for build-in print() to add informative prefix and color-coded print statements.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def xprint(*args, kind: str = 'info', verbosity: int = 0, **kwargs) -&gt; None:\n\"\"\"Thin wrapper function for build-in print() to add informative prefix and color-coded print statements.\"\"\"\nverbosity = 1 if verbosity is True else verbosity\nprefixes = {\n'info': ('\\033[39m', '[INFO]  ', 1),\n'warning': ('\\033[33m\\033[1m', '[WARNING]  ', 0),\n'error': ('\\033[31m\\033[1m', '[ERROR]  ', 0),\n'success': ('\\033[32m', '[SUCCESS]  ', 1),\n'debug': ('\\033[36m', '[DEBUG]  ', 2),\n}\nprefix = prefixes.get(kind, None)\nif prefix is None:\nargs = args\nverbosity_lvl = 1\nelse:\ncolor, prefix_, verbosity_lvl = prefix\nargs = [f'{color}{prefix_}'] + ['{arg}'.format(arg=i) for i in args] + ['\\033[0m']\nif verbosity_lvl &lt;= verbosity:\nprint(*args, **kwargs)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.timeit","title":"<code>timeit(func)</code>","text":"<p>Decorate function to measure its runtime.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def timeit(func):\n\"\"\"Decorate function to measure its runtime.\"\"\"\n@wraps(func)\ndef wrapper(*args, **kwargs):\nstart_time = time.perf_counter()\nresults = func(*args, **kwargs)\nend_time = time.perf_counter() - start_time\nprint(f'[RUNTIME]    {func.__name__}(): {int(end_time//60):d} min {(end_time%60):.4f} sec')\nreturn results\nreturn wrapper\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.profile","title":"<code>profile(output_file=None, sort_by='cumulative', lines_to_print=None, strip_dirs=False)</code>","text":"<p>Profile function.</p> <p>Inspired by and modified the profile decorator of Giampaolo Rodola. Copied code by Ehsan Khodabandeh from <code>towardsdatascience.com</code>.</p> <p>Parameters:</p> <ul> <li> output_file             (<code>str or None</code>)         \u2013 <p>Path of the output file. If only name of the file is given, it's saved in the current directory. If it's None, the name of the decorated function is used (default: <code>None</code>).</p> </li> <li> sort_by             (<code>str or SortKey enum or tuple</code>)         \u2013 <p>Sorting criteria for the Stats object. For a list of valid string and SortKey refer to: https://docs.python.org/3/library/profile.html#pstats.Stats.sort_stats</p> </li> <li> lines_to_print             (<code>int or None</code>)         \u2013 <p>Number of lines to print. Default (None) is for all the lines. This is useful in reducing the size of the printout, especially that sorting by 'cumulative', the time consuming operations are printed toward the top of the file.</p> </li> <li> strip_dirs             (<code>bool</code>)         \u2013 <p>Whether to remove the leading path info from file names. This is also useful in reducing the size of the printout</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>func</code>         \u2013 <p>Profile of the decorated function</p> </li> </ul>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.profile--references","title":"References","text":"<ol> <li> <p>Giampaolo Rodola, http://code.activestate.com/recipes/577817-profile-decorator/ \u21a9</p> </li> <li> <p>Ehsan Khodabandeh, https://towardsdatascience.com/how-to-profile-your-code-in-python-e70c834fad89 \u21a9</p> </li> </ol> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def profile(output_file=None, sort_by='cumulative', lines_to_print=None, strip_dirs=False):\n\"\"\"\n    Profile function.\n    Inspired by and modified the profile decorator of Giampaolo Rodola.\n    Copied code by Ehsan Khodabandeh from `towardsdatascience.com`.\n    Parameters\n    ----------\n    output_file : str or None\n        Path of the output file. If only name of the file is given, it's saved in the current directory.\n        If it's None, the name of the decorated function is used (default: `None`).\n    sort_by : str or SortKey enum or tuple/list of str/SortKey enum\n        Sorting criteria for the Stats object.\n        For a list of valid string and SortKey refer to:\n        https://docs.python.org/3/library/profile.html#pstats.Stats.sort_stats\n    lines_to_print : int or None\n        Number of lines to print. Default (None) is for all the lines.\n        This is useful in reducing the size of the printout, especially\n        that sorting by 'cumulative', the time consuming operations\n        are printed toward the top of the file.\n    strip_dirs : bool\n        Whether to remove the leading path info from file names.\n        This is also useful in reducing the size of the printout\n    Returns\n    -------\n    func\n        Profile of the decorated function\n    References\n    ----------\n    [^1]: Giampaolo Rodola, [http://code.activestate.com/recipes/577817-profile-decorator/](http://code.activestate.com/recipes/577817-profile-decorator/)\n    [^2]: Ehsan Khodabandeh, [https://towardsdatascience.com/how-to-profile-your-code-in-python-e70c834fad89](https://towardsdatascience.com/how-to-profile-your-code-in-python-e70c834fad89)\n    \"\"\"\ndef inner(func):\n@wraps(func)\ndef wrapper(*args, **kwargs):\n_output_file = output_file or func.__name__ + '.prof'\npr = cProfile.Profile()\npr.enable()\nretval = func(*args, **kwargs)\npr.disable()\npr.dump_stats(_output_file)\nwith open(_output_file, 'w') as f:\nps = pstats.Stats(pr, stream=f)\nif strip_dirs:\nps.strip_dirs()\nif isinstance(sort_by, (tuple, list)):\nps.sort_stats(*sort_by)\nelse:\nps.sort_stats(sort_by)\nps.print_stats(lines_to_print)\nreturn retval\nreturn wrapper\nreturn inner\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.debug","title":"<code>debug(func)</code>","text":"<p>Decorate function to print debugging information.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def debug(func):\n\"\"\"Decorate function to print debugging information.\"\"\"\n@wraps(func)\ndef wrapper_debug(*args, **kwargs):\nargs_repr = [repr(a) for a in args]  # 1\nkwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]  # 2\nsignature = \", \".join(args_repr + kwargs_repr)  # 3\nprint(f\"Call {func.__name__}({signature})\")\nvalue = func(*args, **kwargs)\nprint(f\"{func.__name__!r} return {value!r}\")  # 4\nreturn value\nreturn wrapper_debug\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.show_progressbar","title":"<code>show_progressbar(progressbar, verbose=False)</code>","text":"<p>Wrap <code>dask</code> progress bar.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>@contextmanager\ndef show_progressbar(progressbar, verbose=False):\n\"\"\"Wrap `dask` progress bar.\"\"\"\nif verbose:\nwith progressbar:\nyield\nelse:\nyield\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.pad_array","title":"<code>pad_array(a, n, zeros=False)</code>","text":"<p>Pad 1D input array with <code>n</code> elements at start and end (mirror of array).</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>1D input array.</p> </li> <li> n             (<code>int</code>)         \u2013 <p>Number of elements to add at start and end.</p> </li> <li> zeros             (<code>bool, optional</code>)         \u2013 <p>Add zeros instead of mirrored values (default: <code>False</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Padded input array.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def pad_array(a, n: int, zeros=False):\n\"\"\"\n    Pad 1D input array with `n` elements at start and end (mirror of array).\n    Parameters\n    ----------\n    a : np.ndarray\n        1D input array.\n    n : int\n        Number of elements to add at start and end.\n    zeros : bool, optional\n        Add zeros instead of mirrored values (default: `False`).\n    Returns\n    -------\n    np.ndarray\n        Padded input array.\n    \"\"\"\nif zeros:\nreturn np.concatenate((np.zeros(n), a, np.zeros(n)))\nelse:\n# # mirror array\n# pad_start = a[0] + np.abs(a[1:n+1][::-1] - a[0])\n# pad_end = a[-1] + np.abs(a[-n-1:-1][::-1] - a[-1])\n# mirror array AND flip upside down\npad_start = a[0] - np.abs(a[1 : n + 1][::-1] - a[0])\npad_end = a[-1] - np.abs(a[-n - 1 : -1][::-1] - a[-1])\nreturn np.concatenate((pad_start, a, pad_end))\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.pad_along_axis","title":"<code>pad_along_axis(array, n, mode='constant', kwargs=None, axis=-1)</code>","text":"<p>Pad 2D array along given axis (default: <code>-1</code>).</p> <p>Parameters:</p> <ul> <li> array             (<code>np.ndarray</code>)         \u2013 <p>Input data.</p> </li> <li> n             (<code>int</code>)         \u2013 <p>Number of values padded to the edges of specified axis.</p> </li> <li> mode             (<code>str, optional</code>)         \u2013 <p>How to pad array edges (see <code>np.pad</code>, default: <code>constant</code>).</p> </li> <li> kwargs             (<code>dict, optional</code>)         \u2013 <p>OPtional keyword arguments for <code>np.pad</code> (default: <code>None</code>).</p> </li> <li> axis             (<code>int, optional</code>)         \u2013 <p>Axis to pad (default: <code>-1</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Padded input array.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def pad_along_axis(array, n: int, mode: str = 'constant', kwargs: dict = None, axis: int = -1):\n\"\"\"\n    Pad 2D array along given axis (default: `-1`).\n    Parameters\n    ----------\n    array : np.ndarray\n        Input data.\n    n : int\n        Number of values padded to the edges of specified axis.\n    mode : str, optional\n        How to pad array edges (see `np.pad`, default: `constant`).\n    kwargs : dict, optional\n        OPtional keyword arguments for `np.pad` (default: `None`).\n    axis : int, optional\n        Axis to pad (default: `-1`).\n    Returns\n    -------\n    np.ndarray\n        Padded input array.\n    \"\"\"\narray = np.asarray(array)\nif n &lt;= 0:\nreturn array\nif isinstance(n, (int, float)):\nn_before = n_after = int(n)\nelif isinstance(n, (tuple, list)):\nn_before, n_after = n\nif n_before == 0 and n_after == 0:\nreturn array\nnpad = [(0, 0)] * array.ndim\nnpad[axis] = (n_before, n_after)\nprint(npad)\nif kwargs is None:\nkwargs = dict(constant_values=0)\nreturn np.pad(array, pad_width=npad, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.slice_valid_data","title":"<code>slice_valid_data(data, nso)</code>","text":"<p>Account for zero padded input data and return only valid data samples (!= 0).</p> <p>Parameters:</p> <ul> <li> data             (<code>np.ndarray</code>)         \u2013 <p>Seismic section (samples x traces).</p> </li> <li> nso             (<code>int</code>)         \u2013 <p>Original number samples per trace (from binary header).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>\"Unpadded\" seismic section with only valid (non-zero) samples (2D).</p> </li> <li> idx_start_slice(            <code>np.ndarray</code> )        \u2013 <p>Array of starting indices for valid data slice per trace.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def slice_valid_data(data, nso):\n\"\"\"\n    Account for zero padded input data and return only valid data samples (!= 0).\n    Parameters\n    ----------\n    data : np.ndarray\n        Seismic section (samples x traces).\n    nso : int\n        Original number samples per trace (from binary header).\n    Returns\n    -------\n    np.ndarray\n        \"Unpadded\" seismic section with only valid (non-zero) samples (2D).\n    idx_start_slice : np.ndarray\n        Array of starting indices for valid data slice per trace.\n    \"\"\"\n# get indices of first valid sample\n# = 0: trace was padded at bottom\n# &gt; 0: trace was padded at top\nidx_start_slice = (data != 0).argmax(axis=0)\n# create index array\nindexer = np.transpose(np.arange(nso) + idx_start_slice[:, None])\n# return sliced traces and indices\nreturn np.take_along_axis(data, indexer, axis=0), idx_start_slice\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.depth2twt","title":"<code>depth2twt(depth, v=1500)</code>","text":"<p>Convert depth (m) to two-way travel time (TWT in sec).</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def depth2twt(depth, v=1500):\n\"\"\"Convert depth (m) to two-way travel time (TWT in sec).\"\"\"\nreturn depth / (v / 2)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.twt2depth","title":"<code>twt2depth(twt, v=1500, units='s')</code>","text":"<p>Convert two-way travel time (TWT in sec) to depth (m).</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def twt2depth(twt, v=1500, units='s'):\n\"\"\"Convert two-way travel time (TWT in sec) to depth (m).\"\"\"\nif units == 's':\nreturn (v / 2) * twt\nelif units == 'ms':\nreturn (v / 2) * (twt / 1000)\nelif units == 'ns':\nreturn (v / 2) * (twt / 1e-06)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.twt2samples","title":"<code>twt2samples(twt, dt, units='s')</code>","text":"<p>Convert TWT (sec) to samples (#) based on sampling interval <code>dt</code> (sec).</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def twt2samples(twt, dt: float, units='s'):\n\"\"\"Convert TWT (sec) to samples (#) based on sampling interval `dt` (sec).\"\"\"\nif units == 's':\npass\nelif units == 'ms':\ndt = dt / 1000\nelif units == 'ns':\ndt = dt / 1e-6\nreturn twt / dt\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.samples2twt","title":"<code>samples2twt(samples, dt)</code>","text":"<p>Convert samples (#) to TWT (in dt units!) based on sampling interval <code>dt</code>.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def samples2twt(samples, dt: float):\n\"\"\"Convert samples (#) to TWT (in dt units!) based on sampling interval `dt`.\"\"\"\nreturn samples * dt\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.depth2samples","title":"<code>depth2samples(depth, dt, v=1500, units='s')</code>","text":"<p>Convert depth (m) to samples (#) given a sampling interval <code>dt</code> and acoustic velocity.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def depth2samples(depth, dt: float, v=1500, units='s'):\n\"\"\"Convert depth (m) to samples (#) given a sampling interval `dt` and acoustic velocity.\"\"\"\n_twt = depth2twt(depth, v=v)\nif units == 's':\npass\nelif units == 'ms':\ndt = dt / 1000\nelif units == 'ns':\ndt = dt / 1e-6\nreturn twt2samples(_twt, dt=dt)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.samples2depth","title":"<code>samples2depth(samples, dt, v=1500, units='s')</code>","text":"<p>Convert samples (#) to depth (m) given a sampling interval <code>dt</code> and acoustic velocity.</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def samples2depth(samples, dt: float, v=1500, units='s'):\n\"\"\"Convert samples (#) to depth (m) given a sampling interval `dt` and acoustic velocity.\"\"\"\nif units == 's':\npass\nelif units == 'ms':\ndt = dt / 1000\nelif units == 'ns':\ndt = dt / 1e-6\nelse:\nraise ValueError(f'Unit \"{units}\" is not supported for `dt`.')\n_twt = samples2twt(samples, dt=dt)\nreturn twt2depth(_twt, v=v)\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.euclidean_distance","title":"<code>euclidean_distance(coords)</code>","text":"<p>Calculate euclidean distance between consecutive points in array (row-wise).</p> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def euclidean_distance(coords):\n\"\"\"Calculate euclidean distance between consecutive points in array (row-wise).\"\"\"\ndiff = np.diff(coords, axis=0)\ndist = np.sqrt((diff**2).sum(axis=1))\nreturn dist\n</code></pre>"},{"location":"api/functions/api_utils/#pseudo_3D_interpolation.functions.utils.rescale","title":"<code>rescale(a, vmin=0, vmax=1)</code>","text":"<p>Rescale array to given range (default: [0, 1]).</p> <p>Parameters:</p> <ul> <li> a             (<code>np.ndarray</code>)         \u2013 <p>Input array to rescale/normalize.</p> </li> <li> vmin             (<code>float, optional</code>)         \u2013 <p>New minimum value (default: <code>0</code>).</p> </li> <li> vmax             (<code>float, optional</code>)         \u2013 <p>New maximum value (default: <code>1</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>np.ndarray</code>         \u2013 <p>Rescaled input array.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils.py</code> <pre><code>def rescale(a, vmin=0, vmax=1):\n\"\"\"\n    Rescale array to given range (default: [0, 1]).\n    Parameters\n    ----------\n    a : np.ndarray\n        Input array to rescale/normalize.\n    vmin : float, optional\n        New minimum value (default: `0`).\n    vmax : float, optional\n        New maximum value (default: `1`).\n    Returns\n    -------\n    np.ndarray\n        Rescaled input array.\n    \"\"\"\na = np.asarray(a)\n_vmin = np.nanmin(a)\n_vmax = np.nanmax(a)\nvmin = _vmin if vmin is None else vmin\nvmax = _vmax if vmax is None else vmax\nif _vmin == _vmax:\nreturn a\nreturn vmin + (a - _vmin) * ((vmax - vmin) / (_vmax - _vmin))\n</code></pre>"},{"location":"api/functions/api_utils_IO/","title":"<code>utils_IO.py</code>","text":"<p>Miscellaneous utility functions for file I/O.</p>"},{"location":"api/functions/api_utils_IO/#pseudo_3D_interpolation.functions.utils_IO.read_and_merge","title":"<code>read_and_merge(files, splitter='UTM60S', **kwargs_csv)</code>","text":"<p>Read <code>files</code> list into combined <code>pandas.DataFrame</code>.</p> <p>Parameters:</p> <ul> <li> files             (<code>list</code>)         \u2013 <p>List of file paths.</p> </li> <li> splitter             (<code>str, optional</code>)         \u2013 <p>String used to split filename to derive \"original\" filename (default: <code>UTM60S</code>).</p> </li> <li> **kwargs_csv             (<code>dict</code>)         \u2013 <p>Optional parameter for <code>pd.read_csv</code> function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>pd.DataFrame</code>         \u2013 <p>Combined DataFrame of all read files.</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils_IO.py</code> <pre><code>def read_and_merge(files, splitter='UTM60S', **kwargs_csv):\n\"\"\"\n    Read `files` list into combined `pandas.DataFrame`.\n    Parameters\n    ----------\n    files : list\n        List of file paths.\n    splitter : str, optional\n        String used to split filename to derive \"original\" filename (default: `UTM60S`).\n    **kwargs_csv : dict\n        Optional parameter for `pd.read_csv` function.\n    Returns\n    -------\n    pd.DataFrame\n        Combined DataFrame of all read files.\n    \"\"\"\nlist_loaded = []\nfor file in tqdm(\nfiles,\ndesc='Load auxiliary files',\nncols=80,\ntotal=len(files),\nunit_scale=True,\nunit=' files',\n):\nbasepath, filename = os.path.split(file)\nbasename, suffix = os.path.splitext(filename)\ndf = pd.read_csv(file, **kwargs_csv)\nbasename_splits = basename.split('_')\nidx = basename_splits.index(splitter)\ndf['line'] = '_'.join(basename_splits[:idx])\nlist_loaded.append(df)\nreturn pd.concat(list_loaded, ignore_index=True)\n</code></pre>"},{"location":"api/functions/api_utils_IO/#pseudo_3D_interpolation.functions.utils_IO.read_auxiliary_files","title":"<code>read_auxiliary_files(path, fsuffix, prefix=None, suffix=None, index_cols=['line', 'tracl'])</code>","text":"<p>Read auxiliary files (e.g. <code>nav</code>, <code>tide</code>, or <code>mistie</code>) into Pandas DataFrame.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013 <p>Path of (a) directory or (b) datalist with SEG-Y file(s).</p> </li> <li> fsuffix             (<code>str</code>)         \u2013 <p>File suffix.</p> </li> <li> prefix             (<code>str, optional</code>)         \u2013 <p>Filename prefix for filtering (default: <code>None</code>).</p> </li> <li> suffix             (<code>str, optional</code>)         \u2013 <p>Filename suffix for filtering (default: <code>None</code>)</p> </li> <li> index_cols             (<code>list, optional</code>)         \u2013 <p>Index columns of returned DataFrame.</p> </li> </ul> <p>Returns:</p> <ul> <li> df(            <code>pd.DataFrame</code> )        \u2013 <p>Combined DataFrame of all auxiliary files with (filtered using <code>fsuffix</code> and <code>prefix</code>).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils_IO.py</code> <pre><code>def read_auxiliary_files(\npath, fsuffix: str, prefix: str = None, suffix: str = None, index_cols=['line', 'tracl']\n):\n\"\"\"\n    Read auxiliary files (e.g. `nav`, `tide`, or `mistie`) into Pandas DataFrame.\n    Parameters\n    ----------\n    path : str\n        Path of (a) directory or (b) datalist with SEG-Y file(s).\n    fsuffix : str\n        File suffix.\n    prefix : str, optional\n        Filename prefix for filtering (default: `None`).\n    suffix : str, optional\n        Filename suffix for filtering (default: `None`)\n    index_cols : list, optional\n        Index columns of returned DataFrame.\n    Returns\n    -------\n    df : pd.DataFrame\n        Combined DataFrame of all auxiliary files with (filtered using `fsuffix` and `prefix`).\n    \"\"\"\nbasepath, filename = os.path.split(path)\nif fsuffix.find('.') == -1:\nfsuffix = '.' + fsuffix\n# (A) path -&gt; directory\nif os.path.isdir(path):\nfiles = sorted(glob.glob(os.path.join(path, f'*{fsuffix}')))\nif prefix is not None:\nfiles = [\nf for f in files if os.path.split(f)[-1].startswith(prefix)\n]  # filter by prefix\nif suffix is not None:\nfiles = [\nf for f in files if os.path.splitext(os.path.split(f)[-1])[0].endswith(suffix)\n]  # filter by suffix\n# (B) path -&gt; datalist\nelif os.path.isfile(path) and path.endswith('.txt'):\nwith open(path, 'r') as datalist:\nfiles = datalist.readlines()\nfiles = [\nos.path.join(basepath, os.path.splitext(line.rstrip())[0] + fsuffix)\nif os.path.split(line.rstrip()) not in ['', '.']\nelse line.rstrip()\nfor line in files\n]\nelse:\nraise IOError('Invalid input for `path` parameter. Should be either directory or datalist!')\ndf = None\nif len(files) &gt; 0:\nkwargs_csv = dict(sep=',')\ndf = read_and_merge(files, splitter='UTM60S', **kwargs_csv)\nif index_cols is not None:\ndf = df.set_index(index_cols, drop=True)\nreturn df\n</code></pre>"},{"location":"api/functions/api_utils_IO/#pseudo_3D_interpolation.functions.utils_IO.export_coords","title":"<code>export_coords(out_path, xcoords, ycoords, coordinate_units, index_label=None, aux_info=None)</code>","text":"<p>Export function for (reprojected) coordinates.</p> <p>Parameters:</p> <ul> <li> out_path             (<code>str</code>)         \u2013 <p>Output file path.</p> </li> <li> xcoords             (<code>np.ndarray</code>)         \u2013 <p>Array of X coordinates.</p> </li> <li> ycoords             (<code>np.ndarray</code>)         \u2013 <p>Array of Y coordinates.</p> </li> <li> coordinate_units             (<code>int</code>)         \u2013 <p>SEG-Y specific coordinate unit specifier integer.</p> </li> <li> index_label             (<code>str, optional</code>)         \u2013 <p>Label to use for pandas index column (default: None).</p> </li> <li> aux_info             (<code>list, optional</code>)         \u2013 <p>List of auxiliary SEG-Y header information to write (default: None).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils_IO.py</code> <pre><code>def export_coords(\nout_path,\nxcoords: np.ndarray,\nycoords: np.ndarray,\ncoordinate_units: int,\nindex_label: str = None,\naux_info: list = None,\n) -&gt; None:\n\"\"\"\n    Export function for (reprojected) coordinates.\n    Parameters\n    ----------\n    out_path : str\n        Output file path.\n    xcoords : np.ndarray\n        Array of X coordinates.\n    ycoords : np.ndarray\n        Array of Y coordinates.\n    coordinate_units : int\n        SEG-Y specific coordinate unit specifier integer.\n    index_label : str, optional\n        Label to use for pandas index column (default: None).\n    aux_info : list, optional\n        List of auxiliary SEG-Y header information to write (default: None).\n    \"\"\"\nkwargs = dict(sep=',', header=True, index=False, index_label=None, line_terminator='\\n')\nif index_label is not None and index_label is not False:\nkwargs.update({'index': True, 'index_label': index_label})\n# create pandas DataFrame\nif coordinate_units == 1:\nif aux_info is not None:\ncols = ['tracl', 'tracr', 'fldr', 'n_traces', 'n_samples', 'x', 'y']\nvals = aux_info + [xcoords, ycoords]\nelse:\ncols = ['x', 'y']\nvals = [xcoords, ycoords]\ncoords = pd.DataFrame(dict(zip(cols, vals)))\nkwargs['float_format'] = '%.2f'\nelse:\nif aux_info is not None:\ncols = ['tracl', 'tracr', 'fldr', 'n_traces', 'n_samples', 'lat', 'lon']\nvals = aux_info + [ycoords, xcoords]\nelse:\ncols = ['lat', 'lon']\nvals = [xcoords, ycoords]\ncoords = pd.DataFrame(dict(zip(cols, vals)))\nkwargs['float_format'] = '%.6f'\n# save coordinate columns to text\ncoords.to_csv(out_path, **kwargs)\n</code></pre>"},{"location":"api/functions/api_utils_IO/#pseudo_3D_interpolation.functions.utils_IO.extract_navigation_from_segy","title":"<code>extract_navigation_from_segy(path, fsuffix='sgy', prefix=None, suffix=None, splitter='UTM60S', src_coords_bytes=(73, 77), kwargs_segyio=None, write_aux=False)</code>","text":"<p>Extract coordinates from SEG-Y file(s) in given directory or specified in datalist.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013 <p>Path of (a) directory or (b) datalist with SEG-Y file(s).</p> </li> <li> fsuffix             (<code>str, optional</code>)         \u2013 <p>File suffix (default: 'sgy'). Only used if <code>path</code> is a directory.</p> </li> <li> prefix             (<code>str, optional</code>)         \u2013 <p>Filename prefix for filtering (default: None). Only used if <code>path</code> is a directory.</p> </li> <li> suffix             (<code>str, optional</code>)         \u2013 <p>Filename suffix for filtering (default: None). Only used if <code>path</code> is a directory.</p> </li> <li> splitter             (<code>str, optional</code>)         \u2013 <p>String used to split filename to derive \"original\" filename (default: 'UTM60S').</p> </li> <li> src_coords_bytes             (<code>tuple, optional</code>)         \u2013 <p>Byte position of coordinates in SEG-Y trace headers (default: (73, 77)).</p> </li> <li> kwargs_segyio             (<code>dict, optional</code>)         \u2013 <p>Parameter for <code>segyio.open()</code> (default: None).</p> </li> <li> write_aux             (<code>bool, optional</code>)         \u2013 <p>Write extracted navigation to individual auxiliary files (*.nav).</p> </li> </ul> Source code in <code>pseudo_3D_interpolation\\functions\\utils_IO.py</code> <pre><code>def extract_navigation_from_segy(\npath,\nfsuffix: str = 'sgy',\nprefix: str = None,\nsuffix: str = None,\nsplitter: str = 'UTM60S',\nsrc_coords_bytes: tuple = (73, 77),\nkwargs_segyio: dict = None,\nwrite_aux: bool = False,\n) -&gt; None:\n\"\"\"\n    Extract coordinates from SEG-Y file(s) in given directory or specified in datalist.\n    Parameters\n    ----------\n    path : str\n        Path of (a) directory or (b) datalist with SEG-Y file(s).\n    fsuffix : str, optional\n        File suffix (default: 'sgy'). Only used if `path` is a directory.\n    prefix : str, optional\n        Filename prefix for filtering (default: None). Only used if `path` is a directory.\n    suffix : str, optional\n        Filename suffix for filtering (default: None). Only used if `path` is a directory.\n    splitter : str, optional\n        String used to split filename to derive \"original\" filename (default: 'UTM60S').\n    src_coords_bytes : tuple, optional\n        Byte position of coordinates in SEG-Y trace headers (default: (73, 77)).\n    kwargs_segyio : dict, optional\n        Parameter for `segyio.open()` (default: None).\n    write_aux : bool, optional\n        Write extracted navigation to individual auxiliary files (*.nav).\n    \"\"\"\nbasepath, filename = os.path.split(path)\n# (A) path -&gt; directory\nif os.path.isdir(path):\nif fsuffix.find('.') != -1:\nfsuffix = '*' + fsuffix\nelse:\nfsuffix = '*.' + fsuffix\nfiles = sorted(glob.glob(os.path.join(path, fsuffix)))\nif prefix is not None:\nfiles = [\nf for f in files if os.path.split(f)[-1].startswith(prefix)\n]  # filter by prefix\nif suffix is not None:\nfiles = [\nf for f in files if os.path.splitext(os.path.split(f)[-1])[0].endswith(suffix)\n]  # filter by suffix\n# (B) path -&gt; datalist\nelif os.path.isfile(path) and path.endswith('.txt'):\nwith open(path, 'r') as datalist:\nfiles = datalist.readlines()\nfiles = [\nos.path.join(basepath, line.rstrip())\nif os.path.split(line.rstrip()) not in ['', '.']\nelse line.rstrip()\nfor line in files\n]\nelse:\nraise IOError('Invalid input for `path` parameter. Should be either directory or datalist!')\nif kwargs_segyio is None or kwargs_segyio == {}:\nkwargs_segyio = dict(strict=False, ignore_geometry=True)\nif len(files) &gt; 0:\nlist_df = []\nfor file in tqdm(\nfiles, desc='Extract coords', ncols=80, total=len(files), unit_scale=True, unit=' files'\n):\nbasepath, filename = os.path.split(file)\nbasename, suffix = os.path.splitext(filename)\npath_out = os.path.join(basepath, f'{basename}.nav')\nwith segyio.open(file, 'r', **kwargs_segyio) as f:\n# get scaled coordinates\nxcoords, ycoords, coordinate_units = scale_coordinates(f, src_coords_bytes)\n# create pandas DataFrame\ndf = pd.DataFrame(np.column_stack((xcoords, ycoords)), columns=['x', 'y'])\nbasename_splits = basename.split('_')\nidx = basename_splits.index(splitter)\ndf['line'] = '_'.join(basename_splits[:idx])\nif write_aux:\n# export coordinates to file\nexport_coords(\npath_out,\nxcoords,\nycoords,\ncoordinate_units=coordinate_units,\nindex_label=None,\naux_info=None,\n)\nlist_df.append(df)\nreturn pd.concat(list_df, ignore_index=True)\nelse:\nreturn\n</code></pre>"}]}